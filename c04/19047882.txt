Acceptability of an Electronic Self Report Assessment Program for Patients with Cancer Background Eliciting symptom and quality of life information from patients is an important component of medical and nursing care processes. Traditionally, this information has been collected with paper and pencil. However, this approach presents several barriers, including delays in receiving information, difficulty integrating responses with electronic records and the time required to manual score questionnaires for measurement purposes. One solution that addresses many of these barriers is the adoption of computerized screening for symptom and quality of life information. Objective This research explored the acceptability of asking symptom and quality of life questions using the Electronic Self Report Assessment – Cancer program on wireless laptops equipped with touch screen format. Methods Acceptability data was explored with respect to whether any differences may be attributed to demographics, and symptom and quality of life levels such as depression, cognitive and emotional functioning. This evaluation utilized descriptive and univariate statistics to examine data from 342 participants from the ongoing ESRA-C randomized clinical trial. Research participants for the ESRA-C study were recruited from the Seattle Cancer Care Alliance (SCCA), a consortium between the University of Washington Medical Center, Fred Hutchinson Cancer Research Center, and Children’s Hospital and Regional Medical Center in Seattle, Washington. Results The sample consisted of 342 adult participants who completed both baseline and follow-up survey sessions. Medical Oncology represented the largest recruitment group (45.3%), followed by Stem Cell Transplant (34.5%) and Radiation Oncology (20.2%). The primary finding was that patients were generally able to utilize ESRA-C quickly and without difficulty in a real-world clinical setting and that they were overall quite satisfied with the ESRA-C program. Significant differences were found in several acceptability areas with respect to demographics and quality of life measures, such as age, gender, and severe distress. Conclusions This analysis confirms that the ESRA-C application for collecting symptom and quality of life information is easy for patients to use and acceptable across a range of user characteristics. We intend to build on our work by using the survey platform in other modalities while ensuring that the patient’s preferences are considered at all times.  Introduction <p id="P6">Eliciting symptom and quality of life information from patients is an important component of medical and nursing care processes. Commonly, patients are given screening forms related to symptom and quality of life information to complete when they arrive in the reception area. Presumably, when the patient is called for their visit with a clinician, their written responses are appended to the medical chart and utilized as a basis for further assessment. Several problems exist with this paper-based approach ranging from difficulty integrating responses with the electronic medical record to requirements for manual scoring of scaled questionnaires to determine whether critical thresholds have been exceeded.</p> <p id="P7">The routine use of computerized screening for symptom and quality of life information presents many advantages over paper-based approaches. Foundational work by Slack and colleagues in 1964 established the feasibility of computerized data collection, complete with question branching and automatically generated exception reports for physicians.[<xref ref-type="bibr" rid="R1">1</xref>] The advantages of computerized screening are numerous: patient responses can be integrated into the electronic medical record in real time, questionnaires can be drawn from banks of validated instruments, scores can be automatically calculated and presented as graphical summaries with attention drawn to critical values, questionnaires can be customized to particular patient needs providing tailored question stems and skip patterns, and multimedia content can be embedded such that the process is transformed into an adaptive educational tool.</p> <p id="P8">Within the last decade, researchers have studied computerized screening in a number of settings. Newell and colleagues [<xref ref-type="bibr" rid="R2">2</xref>] found that using touch screens for assessing physical side effects, anxiety, depression, and perceived needs was highly acceptable for 229 patients with cancer who were receiving chemotherapy even among computer naive users. Velikova and colleagues [<xref ref-type="bibr" rid="R3">3</xref>] conducted a randomized trial with oncology patients and found that when summaries were immediately provided to clinicians there was a statistically significant increase in the frequency of quality of life issues discussed. Carlson and colleagues [<xref ref-type="bibr" rid="R4">4</xref>] examined the acceptability of administering a computerized quality of life instrument to 46 patients in a cancer center. On pre-test patients had rated computerized data collection as less preferable than face-to-face collection methods and as acceptable as paper and pencil. However at post-test, positive responses on the variables indicated a high level of acceptability as well as significant shifts in attitude with patients rating it as acceptable as face-to-face and preferable to paper and pencil collection.</p> <p id="P9">Our interdisciplinary research team has developed an open-source Distributed Health Assessment and Intervention Research (DHAIR) platform. Designed to be administered to patients using any device capable of supporting a standards compliant web browser, the DHAIR platform includes a diverse set of tools for administering web based surveys including: question branching, tailored content, avatars, text to speech, interfaces for portable devices, and real-time data access for researchers. Technical components of the DHAIR platform have been described elsewhere.[<xref ref-type="bibr" rid="R5">5</xref>] The DHAIR platform has been put to use in a variety of health-related studies including examining treatment decision making in prostate cancer [<xref ref-type="bibr" rid="R6">6</xref>] and is currently being used in studies related to self-management for patients with dyspnea, virtual surrogate readers for health literacy, and assessing sexual risk and antiretroviral medication adherence among an HIV-positive Peruvian population</p> <sec sec-type="methods" id="S7"> <title>Current study Most recently, we have implemented a randomized clinical trial exploring the impact on clinical interactions and outcomes of delivering a real time graphical summary of symptom and quality of life information on treatment and referral patterns by physicians, physician assistants, and advanced practice nurses. This three-year study, Electronic Symptom and Report Assessment Cancer (ESRA-C) is the largest study to utilize the DHAIR framework and is approaching completion. Patients in a multi-specialty oncology clinic complete validated symptom and quality of life measures on wireless touch screen laptop computers before starting treatment (T1) and again about 6–7 weeks later (T2). Half of these patients are assigned to the intervention group at the end of the second survey session with a graphical summary of their survey responses provided to their care team. No graphical summary of survey response is provided if patients are in the control group, however if any responses indicate severe symptom distress, suicidal ideation, depression, or pain – these are communicated to the care team regardless of study group. By audio-taping clinical encounters and conducting medical chart reviews we can determine whether the intervention increases provider-patient dialogue around symptoms and quality of life management, and what impact it has on treatment and referral patterns.  <p id="P6">Eliciting symptom and quality of life information from patients is an important component of medical and nursing care processes. Commonly, patients are given screening forms related to symptom and quality of life information to complete when they arrive in the reception area. Presumably, when the patient is called for their visit with a clinician, their written responses are appended to the medical chart and utilized as a basis for further assessment. Several problems exist with this paper-based approach ranging from difficulty integrating responses with the electronic medical record to requirements for manual scoring of scaled questionnaires to determine whether critical thresholds have been exceeded.</p> <p id="P7">The routine use of computerized screening for symptom and quality of life information presents many advantages over paper-based approaches. Foundational work by Slack and colleagues in 1964 established the feasibility of computerized data collection, complete with question branching and automatically generated exception reports for physicians.[<xref ref-type="bibr" rid="R1">1</xref>] The advantages of computerized screening are numerous: patient responses can be integrated into the electronic medical record in real time, questionnaires can be drawn from banks of validated instruments, scores can be automatically calculated and presented as graphical summaries with attention drawn to critical values, questionnaires can be customized to particular patient needs providing tailored question stems and skip patterns, and multimedia content can be embedded such that the process is transformed into an adaptive educational tool.</p> <p id="P8">Within the last decade, researchers have studied computerized screening in a number of settings. Newell and colleagues [<xref ref-type="bibr" rid="R2">2</xref>] found that using touch screens for assessing physical side effects, anxiety, depression, and perceived needs was highly acceptable for 229 patients with cancer who were receiving chemotherapy even among computer naive users. Velikova and colleagues [<xref ref-type="bibr" rid="R3">3</xref>] conducted a randomized trial with oncology patients and found that when summaries were immediately provided to clinicians there was a statistically significant increase in the frequency of quality of life issues discussed. Carlson and colleagues [<xref ref-type="bibr" rid="R4">4</xref>] examined the acceptability of administering a computerized quality of life instrument to 46 patients in a cancer center. On pre-test patients had rated computerized data collection as less preferable than face-to-face collection methods and as acceptable as paper and pencil. However at post-test, positive responses on the variables indicated a high level of acceptability as well as significant shifts in attitude with patients rating it as acceptable as face-to-face and preferable to paper and pencil collection.</p> <p id="P9">Our interdisciplinary research team has developed an open-source Distributed Health Assessment and Intervention Research (DHAIR) platform. Designed to be administered to patients using any device capable of supporting a standards compliant web browser, the DHAIR platform includes a diverse set of tools for administering web based surveys including: question branching, tailored content, avatars, text to speech, interfaces for portable devices, and real-time data access for researchers. Technical components of the DHAIR platform have been described elsewhere.[<xref ref-type="bibr" rid="R5">5</xref>] The DHAIR platform has been put to use in a variety of health-related studies including examining treatment decision making in prostate cancer [<xref ref-type="bibr" rid="R6">6</xref>] and is currently being used in studies related to self-management for patients with dyspnea, virtual surrogate readers for health literacy, and assessing sexual risk and antiretroviral medication adherence among an HIV-positive Peruvian population</p> <sec sec-type="methods" id="S7"> <title>Current study Most recently, we have implemented a randomized clinical trial exploring the impact on clinical interactions and outcomes of delivering a real time graphical summary of symptom and quality of life information on treatment and referral patterns by physicians, physician assistants, and advanced practice nurses. This three-year study, Electronic Symptom and Report Assessment Cancer (ESRA-C) is the largest study to utilize the DHAIR framework and is approaching completion. Patients in a multi-specialty oncology clinic complete validated symptom and quality of life measures on wireless touch screen laptop computers before starting treatment (T1) and again about 6–7 weeks later (T2). Half of these patients are assigned to the intervention group at the end of the second survey session with a graphical summary of their survey responses provided to their care team. No graphical summary of survey response is provided if patients are in the control group, however if any responses indicate severe symptom distress, suicidal ideation, depression, or pain – these are communicated to the care team regardless of study group. By audio-taping clinical encounters and conducting medical chart reviews we can determine whether the intervention increases provider-patient dialogue around symptoms and quality of life management, and what impact it has on treatment and referral patterns.  Purpose The purpose of this paper is to report on patients’ acceptability of using theESRA-C symptom and quality of life program in a diverse clinical setting and whether any differences in acceptability may be attributed to demographics, and symptom and quality of life levels such as depression, cognitive and emotional functioning.  Methods Acceptability data was explored with respect to whether any differences may be attributed to demographics, and symptom and quality of life levels such as depression, cognitive and emotional functioning. This evaluation utilized descriptive and univariate statistics to examine data from 342 participants from the ongoing ESRA-C randomized clinical trial. Research participants for the ESRA-C study were recruited from the Seattle Cancer Care Alliance (SCCA), a consortium between the University of Washington Medical Center, Fred Hutchinson Cancer Research Center, and Children’s Hospital and Regional Medical Center in Seattle, Washington.  Results The sample consisted of 342 adult participants who completed both baseline and follow-up survey sessions. Medical Oncology represented the largest recruitment group (45.3%), followed by Stem Cell Transplant (34.5%) and Radiation Oncology (20.2%). The primary finding was that patients were generally able to utilize ESRA-C quickly and without difficulty in a real-world clinical setting and that they were overall quite satisfied with the ESRA-C program. Significant differences were found in several acceptability areas with respect to demographics and quality of life measures, such as age, gender, and severe distress.  Conclusions This analysis confirms that the ESRA-C application for collecting symptom and quality of life information is easy for patients to use and acceptable across a range of user characteristics. We intend to build on our work by using the survey platform in other modalities while ensuring that the patient’s preferences are considered at all times.  Methods Sample Research participants for the ESRA-C study were recruited from the Seattle Cancer Care Alliance (SCCA), a consortium between the University of Washington Medical Center, Fred Hutchinson Cancer Research Center (FHCRC), and Children’s Hospital and Regional Medical Center in Seattle, Washington. The SCCA provided care for 3,609 new patients during fiscal year 2006 with the majority (85%) originating from Washington State. (personal communication by Meadearis, D., 8/11/2006) Eligibility criteria for the current study are: patients who were being evaluated for new radiation therapy, medical oncology therapy or hematopoietic stem cell transplantation (HSCT), at least 18 years of age, able to communicate in English, and competent to understand the study information and give informed consent. All procedures and protocols were initially approved by the University of Washington Human Subjects Division (APP00000089) and subsequently approved in years two and three by the Cancer Consortium IRB at the FHCRC (#6210) Between April 2005 and November 2006, 698 eligible patients were invited to participate in the study, with 509 (72.9%) patients providing written consent. To date, 342 of these patients have also completed a follow-up survey (T2). The 342 patients who have provided both a baseline and a follow-up survey represent the research sample within this analysis. Survey Instruments During the first survey session (T1- see Procedures below), participants were presented with an introductory screen explaining the purpose of the study followed by nine demographic questions. Four validated questionnaires were presented during both T1 and T2 (follow-up) survey sessions: the 13-item Symptom Distress Scale (SDS), the 30-item European Organization for Research and Treatment of Cancer (EORTC) QLQ-30 v.3 [ 7 ], a single item Pain Intensity Numerical Scale, and the 9-item Patient Health Questionnaire-depression module (PHQ-9)[ 8 ]. The full PHQ-9 was only triggered if certain items on the PHQ-9, SDS, or QLQ-30 exceeded a pre-determined threshold. During the second survey session (T2) six acceptability items were presented after the PHQ-9 instrument. The acceptability questions were adapted with permission from Carlson’s work. [ 4 ] Materials and Equipment Development and Testing Development time for ESRA-C was approximately 6 months and involved rapid prototyping and extensive testing following the usability engineering lifecycle proposed by Mayhew. [ 9 ] Usability testing was also conducted with a sample of proxy patients at a community center for adults with literacy needs and minor revisions were made based on these results. [ 10 ] System Architecture The DHAIR platform was built on an open source architecture comprising a Linux Operating System, Apache web server, MySQL database system, and the PHP or PERL or Python programming languages (LAMP). An administrative interface provided a survey editing environment for researchers where questions and response options could be entered and immediately deployed. Options for layout, question branching, forced response, and user control were also available within this interface. Setting and Security ESRA-C was presented to participants via laptop computers equipped with wireless network cards in the clinic reception areas or exam rooms prior to a provider visit. When completing the survey in reception areas, participants were seated in areas reserved for research studies containing privacy partitions. If this was not possible care was taken to locate seating a suitable distance from other patients. All connections to the wireless base stations were secured via Machine Address Code, NT network login, and 128 bit wireless G encryption standards. Several brands of laptops were used including both touch screen laptops with Windows XP Professional and ‘tablet’ laptops that required use of a proprietary stylus and Windows XP Tablet. User/survey interface and operation To minimize the need for scrolling and to increase focus on individual questions, participants typically found one survey question per screen with response options arranged in either a vertical or horizontal fashion. Most response options were radio buttons or check boxes; these were redesigned into a larger format from native html form elements to accommodate the size of participant fingers. Participants were able to change their response after making a selection with all intermediate responses recorded by the server. The lower part of each screen was anchored by a graphical progress bar, on either side of this were previous and next buttons. Large font sizes and mid-tone colors are used on each screen with minimal supplementary text to increase usability. [ 11 ] [ 12 ] All survey responses and associated timestamps were sent to a secure web server via an encrypted connection. At the conclusion of the survey patients were shown a list of items they may have purposefully or accidentally skipped and were invited to revisit the missed questions. Patients were also presented with a list of the entire set of previously completed questions and were given the option of revising their answers. Procedures Clinic scheduling staff or registered nurses asked patients if they were willing to meet with a member of the ESRA-C research team during a normally scheduled visit to discuss participating in a study about improving methods for evaluating patient symptoms and quality of life. Patients who met with research staff received a more in-depth explanation per IRB approved informed-consent procedures. This usually occured in a private exam room or in the reception area set aside for research studies. If consent was obtained, the research staff created a participant record in the DHAIR platform before handing the laptop to the patient. Patients were surveyed a second time (T2) approximately 6–7 weeks after beginning treatment. At the conclusion of the second survey, patients were greeted with a “Thank-you” screen along with a message that they give the laptop back to the research staff. The staff member then entered a password to access an administrative screen. This action triggered an automatic randomized script which assigned the patient into the intervention or control group. If the patient was assigned to the intervention group, a two page graphical summary of their responses was printed and placed on top of the chart If the patient is assigned to the control group then no graphical summary was printed. For patients in both groups an audio-recorder was placed in the exam room to record the clinical interaction. Safety review After each patient completed the survey, the survey administrator examined a “Safety Net Review Screen” indicating whether any survey responses for Severe Distress, Suicidal Ideation, Depression, or Pain exceeded threshold values. If any of these thresholds were exceeded the survey administrator communicated this information to the clinical care team and documented respective actions in the system. Data Analysis This acceptability evaluation utilized descriptive and univariate statistics to examine data collected within the ongoing ESRA-C clinical trial. Data was downloaded as an ASCII Comma Separated Value File (CSV) from the administrative interface of the DHAIR platform. The CSV file was then imported into SPSS version 13 with value and variable labels assigned through the use of an automatically generated SPSS syntax file. Data were then examined for irregularities and data quality issues before conducting the analysis. Two-tailed independent groups t-tests with a level of significance of ?=0.05 were computed to examine differences between demographic variables and acceptability items. Demographic items and symptom and quality of life information were dichotomized and used as independent variables. Time to survey completion was also considered as a demographic variable, this variable was dichotomized using a median split; into a fast and slow group. One value fell directly on the median and this was grouped into the fast group.  Sample Research participants for the ESRA-C study were recruited from the Seattle Cancer Care Alliance (SCCA), a consortium between the University of Washington Medical Center, Fred Hutchinson Cancer Research Center (FHCRC), and Children’s Hospital and Regional Medical Center in Seattle, Washington. The SCCA provided care for 3,609 new patients during fiscal year 2006 with the majority (85%) originating from Washington State. (personal communication by Meadearis, D., 8/11/2006) Eligibility criteria for the current study are: patients who were being evaluated for new radiation therapy, medical oncology therapy or hematopoietic stem cell transplantation (HSCT), at least 18 years of age, able to communicate in English, and competent to understand the study information and give informed consent. All procedures and protocols were initially approved by the University of Washington Human Subjects Division (APP00000089) and subsequently approved in years two and three by the Cancer Consortium IRB at the FHCRC (#6210) Between April 2005 and November 2006, 698 eligible patients were invited to participate in the study, with 509 (72.9%) patients providing written consent. To date, 342 of these patients have also completed a follow-up survey (T2). The 342 patients who have provided both a baseline and a follow-up survey represent the research sample within this analysis.  Survey Instruments During the first survey session (T1- see Procedures below), participants were presented with an introductory screen explaining the purpose of the study followed by nine demographic questions. Four validated questionnaires were presented during both T1 and T2 (follow-up) survey sessions: the 13-item Symptom Distress Scale (SDS), the 30-item European Organization for Research and Treatment of Cancer (EORTC) QLQ-30 v.3 [ 7 ], a single item Pain Intensity Numerical Scale, and the 9-item Patient Health Questionnaire-depression module (PHQ-9)[ 8 ]. The full PHQ-9 was only triggered if certain items on the PHQ-9, SDS, or QLQ-30 exceeded a pre-determined threshold. During the second survey session (T2) six acceptability items were presented after the PHQ-9 instrument. The acceptability questions were adapted with permission from Carlson’s work. [ 4 ]  Materials and Equipment Development and Testing Development time for ESRA-C was approximately 6 months and involved rapid prototyping and extensive testing following the usability engineering lifecycle proposed by Mayhew. [ 9 ] Usability testing was also conducted with a sample of proxy patients at a community center for adults with literacy needs and minor revisions were made based on these results. [ 10 ] System Architecture The DHAIR platform was built on an open source architecture comprising a Linux Operating System, Apache web server, MySQL database system, and the PHP or PERL or Python programming languages (LAMP). An administrative interface provided a survey editing environment for researchers where questions and response options could be entered and immediately deployed. Options for layout, question branching, forced response, and user control were also available within this interface. Setting and Security ESRA-C was presented to participants via laptop computers equipped with wireless network cards in the clinic reception areas or exam rooms prior to a provider visit. When completing the survey in reception areas, participants were seated in areas reserved for research studies containing privacy partitions. If this was not possible care was taken to locate seating a suitable distance from other patients. All connections to the wireless base stations were secured via Machine Address Code, NT network login, and 128 bit wireless G encryption standards. Several brands of laptops were used including both touch screen laptops with Windows XP Professional and ‘tablet’ laptops that required use of a proprietary stylus and Windows XP Tablet. User/survey interface and operation To minimize the need for scrolling and to increase focus on individual questions, participants typically found one survey question per screen with response options arranged in either a vertical or horizontal fashion. Most response options were radio buttons or check boxes; these were redesigned into a larger format from native html form elements to accommodate the size of participant fingers. Participants were able to change their response after making a selection with all intermediate responses recorded by the server. The lower part of each screen was anchored by a graphical progress bar, on either side of this were previous and next buttons. Large font sizes and mid-tone colors are used on each screen with minimal supplementary text to increase usability. [ 11 ] [ 12 ] All survey responses and associated timestamps were sent to a secure web server via an encrypted connection. At the conclusion of the survey patients were shown a list of items they may have purposefully or accidentally skipped and were invited to revisit the missed questions. Patients were also presented with a list of the entire set of previously completed questions and were given the option of revising their answers.  Development and Testing Development time for ESRA-C was approximately 6 months and involved rapid prototyping and extensive testing following the usability engineering lifecycle proposed by Mayhew. [ 9 ] Usability testing was also conducted with a sample of proxy patients at a community center for adults with literacy needs and minor revisions were made based on these results. [ 10 ]  System Architecture The DHAIR platform was built on an open source architecture comprising a Linux Operating System, Apache web server, MySQL database system, and the PHP or PERL or Python programming languages (LAMP). An administrative interface provided a survey editing environment for researchers where questions and response options could be entered and immediately deployed. Options for layout, question branching, forced response, and user control were also available within this interface.  Setting and Security ESRA-C was presented to participants via laptop computers equipped with wireless network cards in the clinic reception areas or exam rooms prior to a provider visit. When completing the survey in reception areas, participants were seated in areas reserved for research studies containing privacy partitions. If this was not possible care was taken to locate seating a suitable distance from other patients. All connections to the wireless base stations were secured via Machine Address Code, NT network login, and 128 bit wireless G encryption standards. Several brands of laptops were used including both touch screen laptops with Windows XP Professional and ‘tablet’ laptops that required use of a proprietary stylus and Windows XP Tablet.  User/survey interface and operation To minimize the need for scrolling and to increase focus on individual questions, participants typically found one survey question per screen with response options arranged in either a vertical or horizontal fashion. Most response options were radio buttons or check boxes; these were redesigned into a larger format from native html form elements to accommodate the size of participant fingers. Participants were able to change their response after making a selection with all intermediate responses recorded by the server. The lower part of each screen was anchored by a graphical progress bar, on either side of this were previous and next buttons. Large font sizes and mid-tone colors are used on each screen with minimal supplementary text to increase usability. [ 11 ] [ 12 ] All survey responses and associated timestamps were sent to a secure web server via an encrypted connection. At the conclusion of the survey patients were shown a list of items they may have purposefully or accidentally skipped and were invited to revisit the missed questions. Patients were also presented with a list of the entire set of previously completed questions and were given the option of revising their answers.  Procedures Clinic scheduling staff or registered nurses asked patients if they were willing to meet with a member of the ESRA-C research team during a normally scheduled visit to discuss participating in a study about improving methods for evaluating patient symptoms and quality of life. Patients who met with research staff received a more in-depth explanation per IRB approved informed-consent procedures. This usually occured in a private exam room or in the reception area set aside for research studies. If consent was obtained, the research staff created a participant record in the DHAIR platform before handing the laptop to the patient. Patients were surveyed a second time (T2) approximately 6–7 weeks after beginning treatment. At the conclusion of the second survey, patients were greeted with a “Thank-you” screen along with a message that they give the laptop back to the research staff. The staff member then entered a password to access an administrative screen. This action triggered an automatic randomized script which assigned the patient into the intervention or control group. If the patient was assigned to the intervention group, a two page graphical summary of their responses was printed and placed on top of the chart If the patient is assigned to the control group then no graphical summary was printed. For patients in both groups an audio-recorder was placed in the exam room to record the clinical interaction. Safety review After each patient completed the survey, the survey administrator examined a “Safety Net Review Screen” indicating whether any survey responses for Severe Distress, Suicidal Ideation, Depression, or Pain exceeded threshold values. If any of these thresholds were exceeded the survey administrator communicated this information to the clinical care team and documented respective actions in the system.  Safety review After each patient completed the survey, the survey administrator examined a “Safety Net Review Screen” indicating whether any survey responses for Severe Distress, Suicidal Ideation, Depression, or Pain exceeded threshold values. If any of these thresholds were exceeded the survey administrator communicated this information to the clinical care team and documented respective actions in the system.  Data Analysis This acceptability evaluation utilized descriptive and univariate statistics to examine data collected within the ongoing ESRA-C clinical trial. Data was downloaded as an ASCII Comma Separated Value File (CSV) from the administrative interface of the DHAIR platform. The CSV file was then imported into SPSS version 13 with value and variable labels assigned through the use of an automatically generated SPSS syntax file. Data were then examined for irregularities and data quality issues before conducting the analysis. Two-tailed independent groups t-tests with a level of significance of ?=0.05 were computed to examine differences between demographic variables and acceptability items. Demographic items and symptom and quality of life information were dichotomized and used as independent variables. Time to survey completion was also considered as a demographic variable, this variable was dichotomized using a median split; into a fast and slow group. One value fell directly on the median and this was grouped into the fast group.  Survey Administration Participants completed the computerized survey in an average of 15 minutes and 20 seconds (SD=6.26 minutes). Given that patients could be interrupted by family members and clinical staff, if the total survey time was greater than 20 minutes, the individual time intervals between survey responses were examined, and any intervals greater than 10 minutes were removed. Research staff logged any technical issues encountered during the session in a web based study tracking system. A review of these notes found 5 notes flagged as ‘technical- major’ indicating that technical issues forced the survey session to be abandoned. A content analysis of these notes indicates that all were related to disruptions in the wireless network; each of the survey sessions was rescheduled. An additional 30 notes were entered and flagged as ‘technical- minor,’ indicating that issues were encountered but a survey session was completed. The majority of these issues dealt with momentary interruptions with the wireless network and a poorly calibrated stylus on one of the tablet laptops. These momentary interruptions, when corroborated by study notes, were removed from the calculation average survey time. Nearly 20 percent of the patients answered questions out of sequence, meaning that they navigated backwards through the survey to change answers or to revisit questions presented on a list of ‘skipped questions’ near the end of the survey.  Symptom and Quality of Life Data Patient’s scores on the emotional and cognitive functioning subscales of the QLQ-C30 at T2 ranged from 76 to 78. The possible range for these subscales is 0–100 with higher scores reflecting a higher/healthy level of functioning. For the PHQ-9 scores, if participants triggered the scale and skipped one or two questions, these missing values were replaced with their item average score. However, if more than two questions were skipped, their PHQ-9 score was treated as missing. When valid percents were computed, 9 missing cases resulted in a total sample size of 333. More than a fifth of the sample (n=76, 22.80%) reported PHQ-9 scores of 10 or greater representing at least moderate depression. If either of the two SDS intensity items (nausea and pain) were missing, they were coded as 1, however if more than these two items were missing the sum score was coded as missing. All responses were indexed from a 1, resulting in a possible range of 13 to 65. When valid percents were computed, 19 missing cases yielded a total sample size of 323. Of these valid cases, 52 subjects (16.10%) reported summary SDS scores of at least 33, a suggested cutoff point for severe distress. [ 13 , 14 ] In general, participants reported low levels of pain, with a mean score of 2.2 (SD = 2.02) on the 0–10 Pain Intensity Numerical scale.  Acceptability Scores With response options ranging from 1 (Not at all) to 5 (Very much), the six acceptability questions presented at the end of the T2 survey garnered high acceptability with five items having a mean greater than 4.0. Predictors of Acceptability A number of significant differences were found when exploring the relationships between demographic variables and acceptability items. There was a significant effect for gender, t(332) = -2.47, P = .014, with women reporting higher scores (mean=4.14, SD=0.942) than men (mean=3.88, SD=0.962) in response to “How much did you enjoy using this computer program?” Similarly, women also reported higher levels of overall satisfaction (mean=4.40, SD=0.793) with the computer program, t(330) =-2.243, P = .026, than did men (mean=4.20, SD=0.859) There was a significant effect for age, t(214) = 2.16, p = .032, with participants under 60 reporting higher scores (mean=3.97, SD=0.927) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than those in the older age category (mean=3.72 SD=1.049). However those over 60 reported that the time it took to complete the program was more acceptable (mean=4.44 SD=0.967) than those under 60 (mean=4.16, SD=1.314), t(187) =1.99, P = .048). A number of significant effects were found with respect to speed of completion. People who took longer to finish the survey reported higher scores (mean=4.01, SD=0.954) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than people who finished the survey more quickly (mean=3.75, SD=0.987), t(331) =2.38, p = .018. Time to complete and ease of use were also significant findings, t(243) = 2.99, p = .003; with slow survey takers reporting that it was easier (mean=4.93, SD=0.311) than fast survey takers (mean=4.78, SD=0.608). Similarly, questions were reported as more understandable, t(301)=2.54, P = .012. with slow survey takers reporting higher levels of understanding (mean=4.85, SD=0.432) than fast survey takers (mean=4.71, SD=0.585). And the time it took was more acceptable, t(309) =2.50, P = .013, for slow survey takers (4.49, SD=0.948) than it was for fast survey takers (mean=4.19, SD=1.234) Last, there was a significant effect for distress, t(314) =2.02, P = .044, with non-severely distressed participants reporting higher levels of overall satisfaction (mean=4.34, SD=0.799) than severely distressed patients (mean=4.08, SD=0.954).  Predictors of Acceptability A number of significant differences were found when exploring the relationships between demographic variables and acceptability items. There was a significant effect for gender, t(332) = -2.47, P = .014, with women reporting higher scores (mean=4.14, SD=0.942) than men (mean=3.88, SD=0.962) in response to “How much did you enjoy using this computer program?” Similarly, women also reported higher levels of overall satisfaction (mean=4.40, SD=0.793) with the computer program, t(330) =-2.243, P = .026, than did men (mean=4.20, SD=0.859) There was a significant effect for age, t(214) = 2.16, p = .032, with participants under 60 reporting higher scores (mean=3.97, SD=0.927) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than those in the older age category (mean=3.72 SD=1.049). However those over 60 reported that the time it took to complete the program was more acceptable (mean=4.44 SD=0.967) than those under 60 (mean=4.16, SD=1.314), t(187) =1.99, P = .048). A number of significant effects were found with respect to speed of completion. People who took longer to finish the survey reported higher scores (mean=4.01, SD=0.954) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than people who finished the survey more quickly (mean=3.75, SD=0.987), t(331) =2.38, p = .018. Time to complete and ease of use were also significant findings, t(243) = 2.99, p = .003; with slow survey takers reporting that it was easier (mean=4.93, SD=0.311) than fast survey takers (mean=4.78, SD=0.608). Similarly, questions were reported as more understandable, t(301)=2.54, P = .012. with slow survey takers reporting higher levels of understanding (mean=4.85, SD=0.432) than fast survey takers (mean=4.71, SD=0.585). And the time it took was more acceptable, t(309) =2.50, P = .013, for slow survey takers (4.49, SD=0.948) than it was for fast survey takers (mean=4.19, SD=1.234) Last, there was a significant effect for distress, t(314) =2.02, P = .044, with non-severely distressed participants reporting higher levels of overall satisfaction (mean=4.34, SD=0.799) than severely distressed patients (mean=4.08, SD=0.954).  Results Demographics Demographics of the study participants are shown in Table 1 . The sample was predominantly white and highly educated, with a majority reporting using computers in the home and in the workplace ‘often’ or ‘very often’. Survey Administration Participants completed the computerized survey in an average of 15 minutes and 20 seconds (SD=6.26 minutes). Given that patients could be interrupted by family members and clinical staff, if the total survey time was greater than 20 minutes, the individual time intervals between survey responses were examined, and any intervals greater than 10 minutes were removed. Research staff logged any technical issues encountered during the session in a web based study tracking system. A review of these notes found 5 notes flagged as ‘technical- major’ indicating that technical issues forced the survey session to be abandoned. A content analysis of these notes indicates that all were related to disruptions in the wireless network; each of the survey sessions was rescheduled. An additional 30 notes were entered and flagged as ‘technical- minor,’ indicating that issues were encountered but a survey session was completed. The majority of these issues dealt with momentary interruptions with the wireless network and a poorly calibrated stylus on one of the tablet laptops. These momentary interruptions, when corroborated by study notes, were removed from the calculation average survey time. Nearly 20 percent of the patients answered questions out of sequence, meaning that they navigated backwards through the survey to change answers or to revisit questions presented on a list of ‘skipped questions’ near the end of the survey. Symptom and Quality of Life Data Patient’s scores on the emotional and cognitive functioning subscales of the QLQ-C30 at T2 ranged from 76 to 78. The possible range for these subscales is 0–100 with higher scores reflecting a higher/healthy level of functioning. For the PHQ-9 scores, if participants triggered the scale and skipped one or two questions, these missing values were replaced with their item average score. However, if more than two questions were skipped, their PHQ-9 score was treated as missing. When valid percents were computed, 9 missing cases resulted in a total sample size of 333. More than a fifth of the sample (n=76, 22.80%) reported PHQ-9 scores of 10 or greater representing at least moderate depression. If either of the two SDS intensity items (nausea and pain) were missing, they were coded as 1, however if more than these two items were missing the sum score was coded as missing. All responses were indexed from a 1, resulting in a possible range of 13 to 65. When valid percents were computed, 19 missing cases yielded a total sample size of 323. Of these valid cases, 52 subjects (16.10%) reported summary SDS scores of at least 33, a suggested cutoff point for severe distress. [ 13 , 14 ] In general, participants reported low levels of pain, with a mean score of 2.2 (SD = 2.02) on the 0–10 Pain Intensity Numerical scale. Acceptability Scores With response options ranging from 1 (Not at all) to 5 (Very much), the six acceptability questions presented at the end of the T2 survey garnered high acceptability with five items having a mean greater than 4.0. Predictors of Acceptability A number of significant differences were found when exploring the relationships between demographic variables and acceptability items. There was a significant effect for gender, t(332) = -2.47, P = .014, with women reporting higher scores (mean=4.14, SD=0.942) than men (mean=3.88, SD=0.962) in response to “How much did you enjoy using this computer program?” Similarly, women also reported higher levels of overall satisfaction (mean=4.40, SD=0.793) with the computer program, t(330) =-2.243, P = .026, than did men (mean=4.20, SD=0.859) There was a significant effect for age, t(214) = 2.16, p = .032, with participants under 60 reporting higher scores (mean=3.97, SD=0.927) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than those in the older age category (mean=3.72 SD=1.049). However those over 60 reported that the time it took to complete the program was more acceptable (mean=4.44 SD=0.967) than those under 60 (mean=4.16, SD=1.314), t(187) =1.99, P = .048). A number of significant effects were found with respect to speed of completion. People who took longer to finish the survey reported higher scores (mean=4.01, SD=0.954) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than people who finished the survey more quickly (mean=3.75, SD=0.987), t(331) =2.38, p = .018. Time to complete and ease of use were also significant findings, t(243) = 2.99, p = .003; with slow survey takers reporting that it was easier (mean=4.93, SD=0.311) than fast survey takers (mean=4.78, SD=0.608). Similarly, questions were reported as more understandable, t(301)=2.54, P = .012. with slow survey takers reporting higher levels of understanding (mean=4.85, SD=0.432) than fast survey takers (mean=4.71, SD=0.585). And the time it took was more acceptable, t(309) =2.50, P = .013, for slow survey takers (4.49, SD=0.948) than it was for fast survey takers (mean=4.19, SD=1.234) Last, there was a significant effect for distress, t(314) =2.02, P = .044, with non-severely distressed participants reporting higher levels of overall satisfaction (mean=4.34, SD=0.799) than severely distressed patients (mean=4.08, SD=0.954).  Results Demographics Demographics of the study participants are shown in Table 1 . The sample was predominantly white and highly educated, with a majority reporting using computers in the home and in the workplace ‘often’ or ‘very often’. Survey Administration Participants completed the computerized survey in an average of 15 minutes and 20 seconds (SD=6.26 minutes). Given that patients could be interrupted by family members and clinical staff, if the total survey time was greater than 20 minutes, the individual time intervals between survey responses were examined, and any intervals greater than 10 minutes were removed. Research staff logged any technical issues encountered during the session in a web based study tracking system. A review of these notes found 5 notes flagged as ‘technical- major’ indicating that technical issues forced the survey session to be abandoned. A content analysis of these notes indicates that all were related to disruptions in the wireless network; each of the survey sessions was rescheduled. An additional 30 notes were entered and flagged as ‘technical- minor,’ indicating that issues were encountered but a survey session was completed. The majority of these issues dealt with momentary interruptions with the wireless network and a poorly calibrated stylus on one of the tablet laptops. These momentary interruptions, when corroborated by study notes, were removed from the calculation average survey time. Nearly 20 percent of the patients answered questions out of sequence, meaning that they navigated backwards through the survey to change answers or to revisit questions presented on a list of ‘skipped questions’ near the end of the survey. Symptom and Quality of Life Data Patient’s scores on the emotional and cognitive functioning subscales of the QLQ-C30 at T2 ranged from 76 to 78. The possible range for these subscales is 0–100 with higher scores reflecting a higher/healthy level of functioning. For the PHQ-9 scores, if participants triggered the scale and skipped one or two questions, these missing values were replaced with their item average score. However, if more than two questions were skipped, their PHQ-9 score was treated as missing. When valid percents were computed, 9 missing cases resulted in a total sample size of 333. More than a fifth of the sample (n=76, 22.80%) reported PHQ-9 scores of 10 or greater representing at least moderate depression. If either of the two SDS intensity items (nausea and pain) were missing, they were coded as 1, however if more than these two items were missing the sum score was coded as missing. All responses were indexed from a 1, resulting in a possible range of 13 to 65. When valid percents were computed, 19 missing cases yielded a total sample size of 323. Of these valid cases, 52 subjects (16.10%) reported summary SDS scores of at least 33, a suggested cutoff point for severe distress. [ 13 , 14 ] In general, participants reported low levels of pain, with a mean score of 2.2 (SD = 2.02) on the 0–10 Pain Intensity Numerical scale. Acceptability Scores With response options ranging from 1 (Not at all) to 5 (Very much), the six acceptability questions presented at the end of the T2 survey garnered high acceptability with five items having a mean greater than 4.0. Predictors of Acceptability A number of significant differences were found when exploring the relationships between demographic variables and acceptability items. There was a significant effect for gender, t(332) = -2.47, P = .014, with women reporting higher scores (mean=4.14, SD=0.942) than men (mean=3.88, SD=0.962) in response to “How much did you enjoy using this computer program?” Similarly, women also reported higher levels of overall satisfaction (mean=4.40, SD=0.793) with the computer program, t(330) =-2.243, P = .026, than did men (mean=4.20, SD=0.859) There was a significant effect for age, t(214) = 2.16, p = .032, with participants under 60 reporting higher scores (mean=3.97, SD=0.927) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than those in the older age category (mean=3.72 SD=1.049). However those over 60 reported that the time it took to complete the program was more acceptable (mean=4.44 SD=0.967) than those under 60 (mean=4.16, SD=1.314), t(187) =1.99, P = .048). A number of significant effects were found with respect to speed of completion. People who took longer to finish the survey reported higher scores (mean=4.01, SD=0.954) in response to “How helpful to you was this computer program in describing your symptoms and quality of life?” than people who finished the survey more quickly (mean=3.75, SD=0.987), t(331) =2.38, p = .018. Time to complete and ease of use were also significant findings, t(243) = 2.99, p = .003; with slow survey takers reporting that it was easier (mean=4.93, SD=0.311) than fast survey takers (mean=4.78, SD=0.608). Similarly, questions were reported as more understandable, t(301)=2.54, P = .012. with slow survey takers reporting higher levels of understanding (mean=4.85, SD=0.432) than fast survey takers (mean=4.71, SD=0.585). And the time it took was more acceptable, t(309) =2.50, P = .013, for slow survey takers (4.49, SD=0.948) than it was for fast survey takers (mean=4.19, SD=1.234) Last, there was a significant effect for distress, t(314) =2.02, P = .044, with non-severely distressed participants reporting higher levels of overall satisfaction (mean=4.34, SD=0.799) than severely distressed patients (mean=4.08, SD=0.954).  Demographics Demographics of the study participants are shown in Table 1 . The sample was predominantly white and highly educated, with a majority reporting using computers in the home and in the workplace ‘often’ or ‘very often’.  Demographics Demographics of the study participants are shown in Table 1 . The sample was predominantly white and highly educated, with a majority reporting using computers in the home and in the workplace ‘often’ or ‘very often’.  Principal Results This acceptability analysis yielded several interesting findings within a diverse oncology patient sample. The primary finding was that participants were able to utilize ESRA-C quickly and without difficulty in a real-world clinical setting and that they were quite satisfied with the ESRA-C platform. The fact that nearly 20% answered questions out of sequence points to the need for designing flexible navigation systems, notably, providing mechanisms for returning to prior questions to re-evaluate responses. The mean survey administration time of 15 minutes, 20 seconds is feasible within a busy clinical setting. The symptom data collected in this study are consistent with previously reported data from prior work. [ 15 ] While we found several significant differences in acceptability measures when examined by demographic characteristics and quality of life measures, it is difficult to determine whether these differences are clinically meaningful given that, regardless of demographic category, acceptability levels were relatively high on all demographic characteristics. Those under 60 found the program more helpful; however across all of the acceptability items, helpfulness was the lowest scored item. It could be that the overall low score for this item may be due to the fact that participants recognize that there is only a 50% chance that their clinician will receive a summary of their responses, nor will they (the patient) receive their own copy. However, those over 60 reported that the time it took was more acceptable this could be due to having more time available given the proximity to retirement age. When evaluating differences between those who finished quickly and those who took longer, a number of interesting differences were found. Notably, those who took longer to finish found the program more helpful and easy to use, the questions more understandable, and the time it took to complete the survey more acceptable. The take-home message may be that time should be specially set aside in the clinic for electronic symptom and quality of life data collection efforts and that it is important not to rush patients through the process.  Principal Results This acceptability analysis yielded several interesting findings within a diverse oncology patient sample. The primary finding was that participants were able to utilize ESRA-C quickly and without difficulty in a real-world clinical setting and that they were quite satisfied with the ESRA-C platform. The fact that nearly 20% answered questions out of sequence points to the need for designing flexible navigation systems, notably, providing mechanisms for returning to prior questions to re-evaluate responses. The mean survey administration time of 15 minutes, 20 seconds is feasible within a busy clinical setting. The symptom data collected in this study are consistent with previously reported data from prior work. [ 15 ] While we found several significant differences in acceptability measures when examined by demographic characteristics and quality of life measures, it is difficult to determine whether these differences are clinically meaningful given that, regardless of demographic category, acceptability levels were relatively high on all demographic characteristics. Those under 60 found the program more helpful; however across all of the acceptability items, helpfulness was the lowest scored item. It could be that the overall low score for this item may be due to the fact that participants recognize that there is only a 50% chance that their clinician will receive a summary of their responses, nor will they (the patient) receive their own copy. However, those over 60 reported that the time it took was more acceptable this could be due to having more time available given the proximity to retirement age. When evaluating differences between those who finished quickly and those who took longer, a number of interesting differences were found. Notably, those who took longer to finish found the program more helpful and easy to use, the questions more understandable, and the time it took to complete the survey more acceptable. The take-home message may be that time should be specially set aside in the clinic for electronic symptom and quality of life data collection efforts and that it is important not to rush patients through the process.  Comparison with Prior Work Our findings are consistent with earlier work in smaller samples by Carlson [ 4 ] and Newell [ 2 ] in which high levels of acceptability were reported after using a computerized program for symptom reporting in a cancer clinic. Our findings were also consistent with acceptability of touch screen symptom reporting by patients in Velikova and colleagues research. Our research adds to the field by evaluating the impact of demographic variables and distress levels on acceptability. [ 3 ]  Comparison with Prior Work Our findings are consistent with earlier work in smaller samples by Carlson [ 4 ] and Newell [ 2 ] in which high levels of acceptability were reported after using a computerized program for symptom reporting in a cancer clinic. Our findings were also consistent with acceptability of touch screen symptom reporting by patients in Velikova and colleagues research. Our research adds to the field by evaluating the impact of demographic variables and distress levels on acceptability. [ 3 ]  Limitations One of the limitations of this study is that the available sample was comprised mostly of well-educated participants with generally high levels of computer experience derived largely from the Pacific Northwest. Furthermore, acceptability questions were only asked on the second survey session which could reflect a biased sample. Perhaps participants, who became averse to the DHAIR platform during the first session elected not to participate in the second survey. However, among the 509 patients who consented, 57 (11%) did not return for a second survey, only 2 of these were voluntary withdrawals and 17 were deceased. Although our results indicate relationships between these variables, we cannot specify causation; there may be other factors at play, such as the waiting room environment and symptom status influencing acceptability.  Limitations One of the limitations of this study is that the available sample was comprised mostly of well-educated participants with generally high levels of computer experience derived largely from the Pacific Northwest. Furthermore, acceptability questions were only asked on the second survey session which could reflect a biased sample. Perhaps participants, who became averse to the DHAIR platform during the first session elected not to participate in the second survey. However, among the 509 patients who consented, 57 (11%) did not return for a second survey, only 2 of these were voluntary withdrawals and 17 were deceased. Although our results indicate relationships between these variables, we cannot specify causation; there may be other factors at play, such as the waiting room environment and symptom status influencing acceptability.  Discussion Principal Results This acceptability analysis yielded several interesting findings within a diverse oncology patient sample. The primary finding was that participants were able to utilize ESRA-C quickly and without difficulty in a real-world clinical setting and that they were quite satisfied with the ESRA-C platform. The fact that nearly 20% answered questions out of sequence points to the need for designing flexible navigation systems, notably, providing mechanisms for returning to prior questions to re-evaluate responses. The mean survey administration time of 15 minutes, 20 seconds is feasible within a busy clinical setting. The symptom data collected in this study are consistent with previously reported data from prior work. [ 15 ] While we found several significant differences in acceptability measures when examined by demographic characteristics and quality of life measures, it is difficult to determine whether these differences are clinically meaningful given that, regardless of demographic category, acceptability levels were relatively high on all demographic characteristics. Those under 60 found the program more helpful; however across all of the acceptability items, helpfulness was the lowest scored item. It could be that the overall low score for this item may be due to the fact that participants recognize that there is only a 50% chance that their clinician will receive a summary of their responses, nor will they (the patient) receive their own copy. However, those over 60 reported that the time it took was more acceptable this could be due to having more time available given the proximity to retirement age. When evaluating differences between those who finished quickly and those who took longer, a number of interesting differences were found. Notably, those who took longer to finish found the program more helpful and easy to use, the questions more understandable, and the time it took to complete the survey more acceptable. The take-home message may be that time should be specially set aside in the clinic for electronic symptom and quality of life data collection efforts and that it is important not to rush patients through the process. Comparison with Prior Work Our findings are consistent with earlier work in smaller samples by Carlson [ 4 ] and Newell [ 2 ] in which high levels of acceptability were reported after using a computerized program for symptom reporting in a cancer clinic. Our findings were also consistent with acceptability of touch screen symptom reporting by patients in Velikova and colleagues research. Our research adds to the field by evaluating the impact of demographic variables and distress levels on acceptability. [ 3 ] Limitations One of the limitations of this study is that the available sample was comprised mostly of well-educated participants with generally high levels of computer experience derived largely from the Pacific Northwest. Furthermore, acceptability questions were only asked on the second survey session which could reflect a biased sample. Perhaps participants, who became averse to the DHAIR platform during the first session elected not to participate in the second survey. However, among the 509 patients who consented, 57 (11%) did not return for a second survey, only 2 of these were voluntary withdrawals and 17 were deceased. Although our results indicate relationships between these variables, we cannot specify causation; there may be other factors at play, such as the waiting room environment and symptom status influencing acceptability. Conclusions This analysis has confirmed that we have created an application for collecting symptom and quality of life information that is easy for patients to use and acceptable across a range of user characteristics including age, gender, and service line. We intend to build on our work by using the DHAIR platform in other modalities toward systems that are not only more efficient, but that also ensure that the patient’s symptoms are documented and needs are identified at times and locations convenient to the patient.  Conclusions This analysis has confirmed that we have created an application for collecting symptom and quality of life information that is easy for patients to use and acceptable across a range of user characteristics including age, gender, and service line. We intend to build on our work by using the DHAIR platform in other modalities toward systems that are not only more efficient, but that also ensure that the patient’s symptoms are documented and needs are identified at times and locations convenient to the patient.  Tables Table 1 Demographics by Service Line Medical Oncology n=155 (45.3%) Radiation Oncology n=69 (20.2%) HSCT n=118 (34.5%) Total N=342 (100%) Female, n (%) 75 (48.4%) 36 (52.2%) 46 (39.0%) 157 (45.9%) Mean Age, × (SD) 58.59 (12.48) 53.70 (14.67) 49.96 (12.97) 54.28 (13.76) 60 or Over, n (%) 75 (48.4%) 22 (31.9%) 23 (19.5%) 120 (35.1%) White/Caucasian, n (%) 140 (90.3%) 63 (91.3%) 111 (94.1%) 314 (91.8%) Working full time, n (%) 39 (25.2%) 26 (37.7%) 12 (10.2%) 77 (22.5%) Post secondary Education, n (%) 106 (69.7%) 47 (68.1%) 88 (75.2%) 241 (71.3%) Household Income > 55k, n (%) 80 (56.7%) 44 (69.8%) 56 (51.9%) 180 (57.7%) Computer Use at Home (often or very often) n (%) 99 (64.3%) 50 (72.5%) 85 (72.6%) 234 (68.8%) Table 3 Symptom and QOL Subscales n (%) or mean (SD) QLQ-C30: Emotional Functioning Subscale, × (SD) 76.44 (18.36) QLQ-C30: Cognitive Functioning Subscale, × (SD) 78.07 (21.82) PHQ-9 Score 10 or greater: at least moderately depressed, n (%) 76 (22.80%) SDS Score 33 or greater: Severe Distress, n (%) 52 (16.10%) Table 4 Acceptability Mean (SD) How easy was this computer program for you to use? 4.86 (0.487) How understandable were questions? 4.78 (0.517) How much did you enjoy using this computer program? 4.00 (0.960) How helpful to you was this computer program in describing your symptoms and quality of life? 3.88 (0.977) Was the amount of time it took to complete this computer program acceptable? 4.34 (1.108) How would you rate your overall satisfaction with this computer program? 4.29 (0.835) 