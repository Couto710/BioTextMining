Predicting radiotherapy outcomes using statistical learning techniques * Radiotherapy outcomes are determined by complex interactions between treatment, anatomical and patient-related variables. A common obstacle to building maximally predictive outcome models for clinical practice is the failure to capture potential complexity of heterogeneous variable interactions and applicability beyond institutional data. We describe a statistical learning methodology that can automatically screen for nonlinear relations among prognostic variables and generalize to unseen data before. In this work, several types of linear and nonlinear kernels to generate interaction terms and approximate the treatment-response function are evaluated. Examples of institutional datasets of esophagitis, pneumonitis and xerostomia endpoints were used. Furthermore, an independent RTOG dataset was used for ‘generalizabilty’ validation. We formulated the discrimination between risk groups as a supervised learning problem. The distribution of patient groups was initially analyzed using principle components analysis (PCA) to uncover potential nonlinear behavior. The performance of the different methods was evaluated using bivariate correlations and actuarial analysis. Over-fitting was controlled via cross-validation resampling. Our results suggest that a modified support vector machine (SVM) kernel method provided superior performance on leave-one-out testing compared to logistic regression and neural networks in cases where the data exhibited nonlinear behavior on PCA. For instance, in prediction of esophagitis and pneumonitis endpoints, which exhibited nonlinear behavior on PCA, the method provided 21% and 60% improvements, respectively. Furthermore, evaluation on the independent pneumonitis RTOG dataset demonstrated good generalizabilty beyond institutional data in contrast with other models. This indicates that the prediction of treatment response can be improved by utilizing nonlinear kernel methods for discovering important nonlinear interactions among model variables. These models have the capacity to predict on unseen data.  1. Introduction Advances in 3D treatment planning could potentially pave the way for individualized and patient-specific treatment planning decisions based on estimates of tumor local control probability or complication risk to surrounding normal tissues ( Webb 1997 ). Accurate prediction of treatment outcomes would provide clinicians with better tools for informed decision making about patients expected benefits versus anticipated risk. Recently, there has been a burgeoning interest in using radiobiological models to rank patients’ treatment plans in order to identify the ‘optimal’ or at least personalize the patient's plan ( Brahme 1999 , Deasy et al 2002 ). However, instead of using dose metrics only, other parameters associated with the radiobiological response can be incorporated into the treatment design process. For instance, these models could be used as a guide for treatment options ( Armstrong et al 2005 , Weinstein et al 2001 ). Alternatively, once a decision has been reached, these models could be included in an objective function, and the optimization problem driving the actual patient's treatment plan can be formulated in terms relevant to complication risk and tumor eradication ( Moiseenko et al 2004 , Brahme 1999 ). Recent years have witnessed an extraordinary increase in patient-specific biological and clinical information due to the progress in genetics and imaging technology ( Elshaikh et al 2006 ). Therefore, recent approaches have focused more on data-driven models, in which dosimetric metrics are mixed with other patient- or disease-based prognostic factors ( Deasy and El Naqa 2007 ). This approach is motivated by recent reports of image-specific outcomes findings ( de Crevoisier et al 2005 , Hope et al 2005 ). In ( de Crevoisier et al 2005 ) it was reported that rectal distension on the planning computed tomography (CT) scan is associated with an increased risk of biochemical and local failure in patients of prostate cancer when treated without daily image-guided localization of the prostate. Similarly, in ( Hope et al 2005 ), it was found that tumor distance to the spinal cord was a significant predictor of failure in irradiated lung cancer patients. Moreover, biological markers were found to be predictive of biochemical failure in prostate cancer or lung injury in non-small cell lung cancer (NSCLC) post-radiotherapy treatment (post-RT) (Alan Pollack 2003, Chen et al 2005 ). In standard modeling methods, model parameters could be chosen using traditional statistical techniques to define the abscissa of a logistic regression function, for instance ( Blanco et al 2005 , Bradley et al 2004 , Levegrun et al 2001 , Marks 2002 , El Naqa et al 2006 , Hope et al 2006 ). These methods, though useful, are incapable of handling potentially complex interactions, manifested as important nonlinear relationships between combinations of variables and resulting outcomes. Thus, limiting their predictive power and applicability in clinical practice. Handling such nonlinearities requires the active development of more sophisticated methods utilizing datamining approaches and advances in statistical learning theory ( Hastie et al 2001 ). Artificial intelligence techniques (e.g., neural networks and decision trees), which are able to emulate human intelligence by learning the surrounding environment from the given input data, have also been utilized because of their ability to detect nonlinear patterns in the data. In particular, neural networks were extensively investigated to model post-radiation treatment outcomes for cases of lung injury ( Munley et al 1999 , Su et al 2005 ) and biochemical failure and rectal bleeding in prostate cancer ( Gulliford et al 2004 ) However, these studies have mainly focused on using a single class of neural networks, namely feed-forward neural networks (FFNN) ( Haykin 1999 ) with different types of activation functions. In our previous work ( El Naqa et al 2005 ), we have shown that a different neural network architecture, referred to as generalized regression neural network (GRNN) ( Specht 1991 ), outperforms classical neural networks. The major drawback using neural network methods is that they are based on greedy heuristic algorithms with no guarantee of global optimality or robustness, in addition to the extensive computational burden associated with them. In this study, we focus on a branch of machine learning called kernel-based methods, and in particular an extension relying on support vector machine (SVM) methodology. This class of learning methods possesses the ability to adapt artificial intelligence with the avoidance of excessively fitting data incorrectly while maintaining the computational efficiency of the classical statistical methods ( Vapnik 1998 , Shawe-Taylor and Cristianini 2004 ). Kernel-based methods are able to incorporate prior knowledge, while handling missing information and uncertainty in the observed data. The basic philosophy is that with the aid of a certain projection or similarity measure (called the kernel) the data are implicitly embedded in a high-dimensional feature space, which allows computationally efficient and well-understood linear methods to be utilized, as illustrated in figure 1 . Kernel methods have been applied successfully in many diverse applications such as pattern recognition ( Shawe-Taylor and Cristianini 2004 , Vapnik 1998 ) and in computer-aided diagnosis in medical imaging, as in our previous work ( El-Naqa et al 2002 , 2004 ). In most, if not all of these applications the kernel-based methods outperformed the state-of-art technology or provided a competitive performance. We therefore conjecture that kernel-based machine learning methods if properly utilized can help the outcome analyst gain a more insightful understanding of complex variable interactions that affect outcome and wider model applicability. This is partially due to the kernel's natural ability to identify patterns, variable interactions and higher-order relationships without the required guesswork of an analyst. In this paper, we test the hypothesis that kernel-based machine learning methods may improve upon outcomes models using institutional data and resampling methods. Furthermore, we evaluated its clinical applicability using an independent test data.  2. Background 2.1. Multi-metric modeling of radiotherapy outcomes The approach we adopted for modeling outcomes follows an exploratory datamining-based approach. In the context of data-driven outcomes modeling, the observed treatment outcome (e.g., normal tissue complication probability (NTCP) or tumor control probability (TCP)) is considered as the result of functional mapping of multiple dosimetric, clinical or biological input variables. Mathematically, this could be expressed as f ( x; w *) : X ? Y , where x i ? R d are the input explanatory variables (dose–volume metrics, patient disease?specific prognostic factors or biological markers) of length d and y i ? Y are the corresponding observed treatment outcome (TCP or NTCP), and w * includes the optimal parameters of outcome model f (·) obtained by optimizing a certain objective criteria. In our previous work ( Deasy and El Naqa 2007 , El Naqa et al 2006 ), a logit transformation was used: (1) f ( x i ) = e g ( x i ) 1 + e g ( x i ) , i = 1 , … , n , where n is the number of cases (patients), x i is a vector of the input variable values used to predict f ( x i ) for outcome y i of the i th patient. The ‘ x -axis’ summation g( x i ) is given by (2) g ( x i ) = ? o + ? j = 1 d ? j x i j , i = 1 , … , n , j = 1 , … , d , where d is the number of model variables, and the ? 's are the set of model coefficients determined by maximizing the probability that the data gave rise to the observations. However, a major weakness in using this formulation is that the model's capacity to learn out-of-sample data is limited as noted in our recent analysis of the 93–11 RTOG data ( Bradley et al 2007 ). In addition, equation (2) requires the user's feedback to determine whether interaction terms or higher-order terms should be included in the model, making it a trial-and-error process. A solution to ameliorate these problems is offered by applying machine-learning methods as discussed in the next section. 2.2. Kernel-based methods Kernel-based methods and its most prominent member, SVMs, are universal constructive learning procedures based on the statistical learning theory ( Vapnik 1998 ). 2.2.1. Statistical learning Learning is defined in this context as estimating dependences from data ( Hastie et al 2001 ). There are two common types of learning: supervised and unsupervised. In this study, we will focus mainly on supervised learning where the endpoints of the treatments such as tumor control or toxicity grade are provided by experienced oncologists following RTOG or NCI criteria. Nevertheless, we will use unsupervised methods such as PCA to aid visualization of multivariate data and selection of kernel type as described later. For discrimination between patients who are at low risk versus patients who are at high risk of radiation therapy, the main idea of the kernel-based technique would be to separate these two classes with ‘hyper-planes’ that maximize the margin between them in the nonlinear feature space defined by implicit kernel mapping. The optimization problem is formulated as minimizing the following cost function, (3) L ( x , ? ) = 1 2 w T w + C ? i = 1 n ? i , subject to the constraint y i ( w T ? ( x i ) + b ) ? 1 ? ? i i = 1 , 2 , … , n , ? i ? 0 for all i where w is a weighting vector, and ?(·) is a nonlinear mapping function. The ? i represents the tolerance error allowed for each sample being on the wrong side of the margin. Note that minimization of the first term in equation (3) increases the separation (improves generalizabilty) between the two classes, whereas, minimization of the second term (penalty term) improves fitting accuracy. The trade-off between complexity and fitting error is controlled by the regularization parameter C . It stands to reason that such nonlinear formulation would suffer from the curse of dimensionality (i.e., the dimension of the problem becomes too large to solve) ( Haykin 1999 , Hastie et al 2001 ). However, computational efficiency is achieved from solving the dual optimization problem instead of equation (3) , which is convex with a complexity that is dependent only on the number of samples ( Vapnik 1998 ). The prediction function in this case is characterized only by a subset of the training data known as support vectors s i : (4) f ( x ) = ? i = 1 n s ? i y i K ( s i , x ) + ? 0 , where n s is the number of support vectors, ? i are the dual coefficients determined by quadratic programming, and K (·,·) is the kernel function as discussed next. 2.2.2. Kernel construction Kernels are the basic ingredient shared by all kernel-based methods. An admissible kernel should satisfy Mercer's positivity conditions since by definition they represent inner product functions ( Schèolkopf and Smola 2002 ): (5) K ( x , x ? ) = ? ( x ) T ? ( x ? ) , where the mapping ? is implicit and need not to be defined. 2.3. Model variable selection Any multivariate analysis often involves a large number of variables or features ( Guyon and Elissee 2003 ). The main features that characterize the observations are usually unknown. Therefore, dimensionality reduction or subset selection aims to find the ‘significant’ set of features. Finding the best subset of features is definitely challenging, especially in the case of nonlinear models. The objective is to reduce the model complexity, decrease the computational burden and improve the generalizability on unseen data. The straightforward approach is to make an educated guess based on experience and domain knowledge; then, apply feature transformation (e.g., principle component analysis (PCA)) ( Dawson et al 2005 , Kennedy et al 1998 ), or sensitivity analysis by using organized search such as sequential forward selection or sequential backward selection or combination of both ( Kennedy et al 1998 ). A recursive feature elimination (RFE) technique that is based on machine learning has also been suggested ( Guyon et al 2002 ), in which the data set is initialized to contain the whole set, train the predictor (e.g., SVM classifier) on the data, rank the features according tocertain criteria (e.g., ? w ?) and keep iterating by eliminating the lowest ranked one. In our previous work ( El Naqa et al 2006 ), we used model order determination based on information theory and resampling techniques to select the significant variables.  2.1. Multi-metric modeling of radiotherapy outcomes The approach we adopted for modeling outcomes follows an exploratory datamining-based approach. In the context of data-driven outcomes modeling, the observed treatment outcome (e.g., normal tissue complication probability (NTCP) or tumor control probability (TCP)) is considered as the result of functional mapping of multiple dosimetric, clinical or biological input variables. Mathematically, this could be expressed as f ( x; w *) : X ? Y , where x i ? R d are the input explanatory variables (dose–volume metrics, patient disease?specific prognostic factors or biological markers) of length d and y i ? Y are the corresponding observed treatment outcome (TCP or NTCP), and w * includes the optimal parameters of outcome model f (·) obtained by optimizing a certain objective criteria. In our previous work ( Deasy and El Naqa 2007 , El Naqa et al 2006 ), a logit transformation was used: (1) f ( x i ) = e g ( x i ) 1 + e g ( x i ) , i = 1 , … , n , where n is the number of cases (patients), x i is a vector of the input variable values used to predict f ( x i ) for outcome y i of the i th patient. The ‘ x -axis’ summation g( x i ) is given by (2) g ( x i ) = ? o + ? j = 1 d ? j x i j , i = 1 , … , n , j = 1 , … , d , where d is the number of model variables, and the ? 's are the set of model coefficients determined by maximizing the probability that the data gave rise to the observations. However, a major weakness in using this formulation is that the model's capacity to learn out-of-sample data is limited as noted in our recent analysis of the 93–11 RTOG data ( Bradley et al 2007 ). In addition, equation (2) requires the user's feedback to determine whether interaction terms or higher-order terms should be included in the model, making it a trial-and-error process. A solution to ameliorate these problems is offered by applying machine-learning methods as discussed in the next section.  2.2. Kernel-based methods Kernel-based methods and its most prominent member, SVMs, are universal constructive learning procedures based on the statistical learning theory ( Vapnik 1998 ). 2.2.1. Statistical learning Learning is defined in this context as estimating dependences from data ( Hastie et al 2001 ). There are two common types of learning: supervised and unsupervised. In this study, we will focus mainly on supervised learning where the endpoints of the treatments such as tumor control or toxicity grade are provided by experienced oncologists following RTOG or NCI criteria. Nevertheless, we will use unsupervised methods such as PCA to aid visualization of multivariate data and selection of kernel type as described later. For discrimination between patients who are at low risk versus patients who are at high risk of radiation therapy, the main idea of the kernel-based technique would be to separate these two classes with ‘hyper-planes’ that maximize the margin between them in the nonlinear feature space defined by implicit kernel mapping. The optimization problem is formulated as minimizing the following cost function, (3) L ( x , ? ) = 1 2 w T w + C ? i = 1 n ? i , subject to the constraint y i ( w T ? ( x i ) + b ) ? 1 ? ? i i = 1 , 2 , … , n , ? i ? 0 for all i where w is a weighting vector, and ?(·) is a nonlinear mapping function. The ? i represents the tolerance error allowed for each sample being on the wrong side of the margin. Note that minimization of the first term in equation (3) increases the separation (improves generalizabilty) between the two classes, whereas, minimization of the second term (penalty term) improves fitting accuracy. The trade-off between complexity and fitting error is controlled by the regularization parameter C . It stands to reason that such nonlinear formulation would suffer from the curse of dimensionality (i.e., the dimension of the problem becomes too large to solve) ( Haykin 1999 , Hastie et al 2001 ). However, computational efficiency is achieved from solving the dual optimization problem instead of equation (3) , which is convex with a complexity that is dependent only on the number of samples ( Vapnik 1998 ). The prediction function in this case is characterized only by a subset of the training data known as support vectors s i : (4) f ( x ) = ? i = 1 n s ? i y i K ( s i , x ) + ? 0 , where n s is the number of support vectors, ? i are the dual coefficients determined by quadratic programming, and K (·,·) is the kernel function as discussed next. 2.2.2. Kernel construction Kernels are the basic ingredient shared by all kernel-based methods. An admissible kernel should satisfy Mercer's positivity conditions since by definition they represent inner product functions ( Schèolkopf and Smola 2002 ): (5) K ( x , x ? ) = ? ( x ) T ? ( x ? ) , where the mapping ? is implicit and need not to be defined.  2.2.1. Statistical learning Learning is defined in this context as estimating dependences from data ( Hastie et al 2001 ). There are two common types of learning: supervised and unsupervised. In this study, we will focus mainly on supervised learning where the endpoints of the treatments such as tumor control or toxicity grade are provided by experienced oncologists following RTOG or NCI criteria. Nevertheless, we will use unsupervised methods such as PCA to aid visualization of multivariate data and selection of kernel type as described later. For discrimination between patients who are at low risk versus patients who are at high risk of radiation therapy, the main idea of the kernel-based technique would be to separate these two classes with ‘hyper-planes’ that maximize the margin between them in the nonlinear feature space defined by implicit kernel mapping. The optimization problem is formulated as minimizing the following cost function, (3) L ( x , ? ) = 1 2 w T w + C ? i = 1 n ? i , subject to the constraint y i ( w T ? ( x i ) + b ) ? 1 ? ? i i = 1 , 2 , … , n , ? i ? 0 for all i where w is a weighting vector, and ?(·) is a nonlinear mapping function. The ? i represents the tolerance error allowed for each sample being on the wrong side of the margin. Note that minimization of the first term in equation (3) increases the separation (improves generalizabilty) between the two classes, whereas, minimization of the second term (penalty term) improves fitting accuracy. The trade-off between complexity and fitting error is controlled by the regularization parameter C . It stands to reason that such nonlinear formulation would suffer from the curse of dimensionality (i.e., the dimension of the problem becomes too large to solve) ( Haykin 1999 , Hastie et al 2001 ). However, computational efficiency is achieved from solving the dual optimization problem instead of equation (3) , which is convex with a complexity that is dependent only on the number of samples ( Vapnik 1998 ). The prediction function in this case is characterized only by a subset of the training data known as support vectors s i : (4) f ( x ) = ? i = 1 n s ? i y i K ( s i , x ) + ? 0 , where n s is the number of support vectors, ? i are the dual coefficients determined by quadratic programming, and K (·,·) is the kernel function as discussed next.  2.2.2. Kernel construction Kernels are the basic ingredient shared by all kernel-based methods. An admissible kernel should satisfy Mercer's positivity conditions since by definition they represent inner product functions ( Schèolkopf and Smola 2002 ): (5) K ( x , x ? ) = ? ( x ) T ? ( x ? ) , where the mapping ? is implicit and need not to be defined.  2.3. Model variable selection Any multivariate analysis often involves a large number of variables or features ( Guyon and Elissee 2003 ). The main features that characterize the observations are usually unknown. Therefore, dimensionality reduction or subset selection aims to find the ‘significant’ set of features. Finding the best subset of features is definitely challenging, especially in the case of nonlinear models. The objective is to reduce the model complexity, decrease the computational burden and improve the generalizability on unseen data. The straightforward approach is to make an educated guess based on experience and domain knowledge; then, apply feature transformation (e.g., principle component analysis (PCA)) ( Dawson et al 2005 , Kennedy et al 1998 ), or sensitivity analysis by using organized search such as sequential forward selection or sequential backward selection or combination of both ( Kennedy et al 1998 ). A recursive feature elimination (RFE) technique that is based on machine learning has also been suggested ( Guyon et al 2002 ), in which the data set is initialized to contain the whole set, train the predictor (e.g., SVM classifier) on the data, rank the features according tocertain criteria (e.g., ? w ?) and keep iterating by eliminating the lowest ranked one. In our previous work ( El Naqa et al 2006 ), we used model order determination based on information theory and resampling techniques to select the significant variables.  3. Methods and materials 3.1. Kernel-based methods for modeling radiotherapy outcomes However, another issue that arises with the generic SVM formulation in (3) is that the cost function treats samples from the two classes equally. However, this may not be the case in many radiotherapy outcome cases, where event and non-event cases maybe imbalanced. One way to account for this issue is by assigning different penalty weights to the samples belonging in the different classes in equation (3) by decomposing C ( El-Naqa et al 2004 ). This way, the penalty term is rewritten as (6) C + ? i ? Z + ? i + C ? ? i ? Z ? ? i , where ± symbols represent the two classes. A higher penalty weight is assigned to the under-represented class. Typically, used nonlinear kernels include (7) Polynomials : K ( x , x ? ) = ( x T x ? + c ) q Radial basis function ( RBF ) : K ( x , x ? ) = exp ( ? 1 2 ? 2 ? x ? x ? ? 2 ) , where c is a constant, q is the order of the polynomial, and ? is the width of the radial basis functions (RBF). Note that the kernel in these cases acts as a similarity function between sample points in the feature space. Moreover, kernels enjoy closure properties, i.e., one can create admissible composite kernels by weighted addition and multiplication of elementary kernels. This flexibility allows for constructing a neural network by using the combination of sigmoidal kernels or one could choose a logistic regression equivalent kernel by replacing the hinge loss with the binomial deviance ( Hastie et al 2001 ). 3.2. Visualization of higher-dimensional data Prior to applying the kernel-based method, it is important to visualize the data distribution, as a screening test. This requires projecting the data into a lower-dimensional space. In this work, we chose the PCA approach due to its simplicity. In PCA analysis, the principal components (PCs) of a data matrix X (with zero mean) are given by ( Härdle and Simar 2003 ) (8) P C = U T X = ? V T , where U ? V T is the singular value decomposition of X . This is equivalent to transformation into a new coordinate system such that the greatest variance by any projection of the data would lie on the first coordinate (first PC), the second greatest variance on the second coordinate (second PC) and so on. For visualization purposes with the PCA, the heterogeneous variables were normalized using z -scoring (zero mean and unity variance). The term ‘Variance Explained,’ used in PCA plots (cf figure 2 ), refers to the variance of the ‘data model’ about the mean prognostic input factor values. The ‘data model’ is formed as a linear combination of its principal components. Thus, if the PC representation of the data ‘explains’ the spread (variance) of the data about the full data mean, it would be expected that the PC representation captures enough information for modeling. We used unsupervised PCA analysis to provide an indication about class separability; however, it should be cautioned that PCA is an indicator and is not necessarily optimized for this purpose as supervised linear discriminant analysis, for instance. 3.3. Evaluation and validation methods To evaluate the performance of our classifiers, we used Matthew's correlation coefficient (MCC) ( Matthews 1975 ) as a performance evaluation metric for classification, which is given by (9) MCC = TP × TN ? FN × FP ( TN + FN ) ( TP + FN ) ( TN + FP ) ( TP + FP ) , where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives. An MCC value of 1 would indicate perfect classification, a value of ?1 would indicate anti-classification, and a value close to zero would mean no correlation. For ranking evaluation, i.e., the ability of the classifier to measure prediction quality trends, we used Spearman's correlation, which provides a robust estimator of trend. This is a desirable property, particularly when ranking the quality of treatment plans for different patients. We used resampling methods (leave-one-out cross-validation (LOO) and bootstrap) for model selection and performance comparison purposes. These methods provide statistically sound results when the available data set is limited ( Good 2006 ). Application of these methods for radiotherapy outcome modeling is reviewed in our previous work ( El Naqa et al 2006 ). Furthermore, we evaluated the generalizabilty of the model (i.e., applicability to unseen data) on an independent RTOG dataset. 3.4. Data sets We used three datasets of patients as representative examples. The first set consisted of 55 head and neck cancer (HNC) patients who were evaluated by quantitative measurements of whole-mouth stimulated and unstimulated saliva flow prior to therapy and at 6 months follow-up post-RT ( Blanco et al 2005 ). Xerostomia (dry mouth) was defined as occurring when the post-RT stimulated flow fell below 25% of the pretreatment value. Several mathematical models were used to fit the saliva data ( Blanco et al 2005 ). The explored variables included relevant clinical variables (age, gender, race, performance status, chemo, stage, histology, tumor subsite, treatment technique, fraction size, total dose, etc) ( Blanco et al 2005 ). ( Blanco et al 2005 , Chao et al 2001 ). In addition, we included a relatively successful model ( Blanco et al 2005 ) that predicts the salivary function from each parotid gland to decrease according to and exponential function with an attenuation parameter equal 0.054 Gy?1. The next two data sets consisted of patients diagnosed with NSCLC treated with three-dimensional conformal radiation therapy (3D-CRT) with median doses around 70 Gy. One set consisted of 52 out of 219 patients diagnosed with post-radiation late pneumonitis (lung inflammation) (RTOG grade ? 3) ( Hope et al 2006 ). In the other set, 45 out of 166 patients were diagnosed with a highest acute esophagitis (esophagus inflammation) score greater than or equal to 3 according to the RTOG scale ( Bradley et al 2004 ). An independent dataset from the RTOG 9311 after removing duplicate patients from our institution was used for evaluating generalizabilty to out-of-sample data. The RTOG 9311 is a radiation dose-escalation study largely based on the V 20 (percent volume receiving more than 20 Gy) value. The endpoint of pneumonitis was tested ( Bradley et al 2005 ). These data sets contain clinical, dosimetric and tumor location parameters. The clinical variables patient related information such as age, last follow-up date, status at follow-up, weight loss, gender, performance status and smoking history. In addition to tumor characteristics such as tumor histology, gross tumor volume (GTV) and tumor stage. The dosimetric variables radiation prescription dose, maximum dose (Gy), treatment time, fraction size (Gy), Vx values (normal lung tissue volume receiving more than x Gy), Dx values (minimum dose to the hottest x % of lung volume) and the use of either sequential or concurrent chemotherapy. The high-dose location effect within the lung by analyzing the center of mass (COM) of the GTV for each patient relative to lateral, anterior-to-posterior and superior-to-inferior dimensions of the lung bounding box ( Bradley et al 2007 ). The results presented here are only for demonstrating the use of our techniques and are not intended as formal clinical findings, which are presented elsewhere ( Blanco et al 2005 , Bradley et al 2004 , 2007 , Hope et al 2006 ). Treatment planning data were de-archived and potential prognostic factors were extracted using CERR ( Deasy et al 2003 ). 3.5. Kernel-based modeling of NTCP As an example, we formulate the treatment outcome modeling as a binary-classification problem of post-treatment risk, i.e., patients who developed complications after treatment belong to class ‘+1’ and patients who did not develop complications belong to class ‘?1’. Consequently, our objectives become to find the best predictor that separates the two classes. Note that designing a kernel-based SVM requires the determination of a regularization parameter ‘ C ’, which provides the best trade-off between machine complexity (defined in terms of the number of support vectors) and the tolerated classification error. Higher values of C indicate more complexity and more penalization of error. The input variables are normalized between 0 and 1 as a pre-processing step. In this work, we used our logistic regression-based modeling technique and contrasted this with SVM–RFE selection as discussed below. In our simulations, we used LOO to select model parameters and measure the generalizability of the different classifiers. In which, all the data are used for training except for one left out for testing; the sample is permuted in a round-robin fashion. Matthew's correlation coefficient, averaged over cross-validation test samples, is used as the performance evaluation metric.  3.1. Kernel-based methods for modeling radiotherapy outcomes However, another issue that arises with the generic SVM formulation in (3) is that the cost function treats samples from the two classes equally. However, this may not be the case in many radiotherapy outcome cases, where event and non-event cases maybe imbalanced. One way to account for this issue is by assigning different penalty weights to the samples belonging in the different classes in equation (3) by decomposing C ( El-Naqa et al 2004 ). This way, the penalty term is rewritten as (6) C + ? i ? Z + ? i + C ? ? i ? Z ? ? i , where ± symbols represent the two classes. A higher penalty weight is assigned to the under-represented class. Typically, used nonlinear kernels include (7) Polynomials : K ( x , x ? ) = ( x T x ? + c ) q Radial basis function ( RBF ) : K ( x , x ? ) = exp ( ? 1 2 ? 2 ? x ? x ? ? 2 ) , where c is a constant, q is the order of the polynomial, and ? is the width of the radial basis functions (RBF). Note that the kernel in these cases acts as a similarity function between sample points in the feature space. Moreover, kernels enjoy closure properties, i.e., one can create admissible composite kernels by weighted addition and multiplication of elementary kernels. This flexibility allows for constructing a neural network by using the combination of sigmoidal kernels or one could choose a logistic regression equivalent kernel by replacing the hinge loss with the binomial deviance ( Hastie et al 2001 ).  3.2. Visualization of higher-dimensional data Prior to applying the kernel-based method, it is important to visualize the data distribution, as a screening test. This requires projecting the data into a lower-dimensional space. In this work, we chose the PCA approach due to its simplicity. In PCA analysis, the principal components (PCs) of a data matrix X (with zero mean) are given by ( Härdle and Simar 2003 ) (8) P C = U T X = ? V T , where U ? V T is the singular value decomposition of X . This is equivalent to transformation into a new coordinate system such that the greatest variance by any projection of the data would lie on the first coordinate (first PC), the second greatest variance on the second coordinate (second PC) and so on. For visualization purposes with the PCA, the heterogeneous variables were normalized using z -scoring (zero mean and unity variance). The term ‘Variance Explained,’ used in PCA plots (cf figure 2 ), refers to the variance of the ‘data model’ about the mean prognostic input factor values. The ‘data model’ is formed as a linear combination of its principal components. Thus, if the PC representation of the data ‘explains’ the spread (variance) of the data about the full data mean, it would be expected that the PC representation captures enough information for modeling. We used unsupervised PCA analysis to provide an indication about class separability; however, it should be cautioned that PCA is an indicator and is not necessarily optimized for this purpose as supervised linear discriminant analysis, for instance.  3.3. Evaluation and validation methods To evaluate the performance of our classifiers, we used Matthew's correlation coefficient (MCC) ( Matthews 1975 ) as a performance evaluation metric for classification, which is given by (9) MCC = TP × TN ? FN × FP ( TN + FN ) ( TP + FN ) ( TN + FP ) ( TP + FP ) , where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives. An MCC value of 1 would indicate perfect classification, a value of ?1 would indicate anti-classification, and a value close to zero would mean no correlation. For ranking evaluation, i.e., the ability of the classifier to measure prediction quality trends, we used Spearman's correlation, which provides a robust estimator of trend. This is a desirable property, particularly when ranking the quality of treatment plans for different patients. We used resampling methods (leave-one-out cross-validation (LOO) and bootstrap) for model selection and performance comparison purposes. These methods provide statistically sound results when the available data set is limited ( Good 2006 ). Application of these methods for radiotherapy outcome modeling is reviewed in our previous work ( El Naqa et al 2006 ). Furthermore, we evaluated the generalizabilty of the model (i.e., applicability to unseen data) on an independent RTOG dataset.  3.4. Data sets We used three datasets of patients as representative examples. The first set consisted of 55 head and neck cancer (HNC) patients who were evaluated by quantitative measurements of whole-mouth stimulated and unstimulated saliva flow prior to therapy and at 6 months follow-up post-RT ( Blanco et al 2005 ). Xerostomia (dry mouth) was defined as occurring when the post-RT stimulated flow fell below 25% of the pretreatment value. Several mathematical models were used to fit the saliva data ( Blanco et al 2005 ). The explored variables included relevant clinical variables (age, gender, race, performance status, chemo, stage, histology, tumor subsite, treatment technique, fraction size, total dose, etc) ( Blanco et al 2005 ). ( Blanco et al 2005 , Chao et al 2001 ). In addition, we included a relatively successful model ( Blanco et al 2005 ) that predicts the salivary function from each parotid gland to decrease according to and exponential function with an attenuation parameter equal 0.054 Gy?1. The next two data sets consisted of patients diagnosed with NSCLC treated with three-dimensional conformal radiation therapy (3D-CRT) with median doses around 70 Gy. One set consisted of 52 out of 219 patients diagnosed with post-radiation late pneumonitis (lung inflammation) (RTOG grade ? 3) ( Hope et al 2006 ). In the other set, 45 out of 166 patients were diagnosed with a highest acute esophagitis (esophagus inflammation) score greater than or equal to 3 according to the RTOG scale ( Bradley et al 2004 ). An independent dataset from the RTOG 9311 after removing duplicate patients from our institution was used for evaluating generalizabilty to out-of-sample data. The RTOG 9311 is a radiation dose-escalation study largely based on the V 20 (percent volume receiving more than 20 Gy) value. The endpoint of pneumonitis was tested ( Bradley et al 2005 ). These data sets contain clinical, dosimetric and tumor location parameters. The clinical variables patient related information such as age, last follow-up date, status at follow-up, weight loss, gender, performance status and smoking history. In addition to tumor characteristics such as tumor histology, gross tumor volume (GTV) and tumor stage. The dosimetric variables radiation prescription dose, maximum dose (Gy), treatment time, fraction size (Gy), Vx values (normal lung tissue volume receiving more than x Gy), Dx values (minimum dose to the hottest x % of lung volume) and the use of either sequential or concurrent chemotherapy. The high-dose location effect within the lung by analyzing the center of mass (COM) of the GTV for each patient relative to lateral, anterior-to-posterior and superior-to-inferior dimensions of the lung bounding box ( Bradley et al 2007 ). The results presented here are only for demonstrating the use of our techniques and are not intended as formal clinical findings, which are presented elsewhere ( Blanco et al 2005 , Bradley et al 2004 , 2007 , Hope et al 2006 ). Treatment planning data were de-archived and potential prognostic factors were extracted using CERR ( Deasy et al 2003 ).  3.5. Kernel-based modeling of NTCP As an example, we formulate the treatment outcome modeling as a binary-classification problem of post-treatment risk, i.e., patients who developed complications after treatment belong to class ‘+1’ and patients who did not develop complications belong to class ‘?1’. Consequently, our objectives become to find the best predictor that separates the two classes. Note that designing a kernel-based SVM requires the determination of a regularization parameter ‘ C ’, which provides the best trade-off between machine complexity (defined in terms of the number of support vectors) and the tolerated classification error. Higher values of C indicate more complexity and more penalization of error. The input variables are normalized between 0 and 1 as a pre-processing step. In this work, we used our logistic regression-based modeling technique and contrasted this with SVM–RFE selection as discussed below. In our simulations, we used LOO to select model parameters and measure the generalizability of the different classifiers. In which, all the data are used for training except for one left out for testing; the sample is permuted in a round-robin fashion. Matthew's correlation coefficient, averaged over cross-validation test samples, is used as the performance evaluation metric.  4. Experimental results 4.1. Data exploration and visualization In the head and neck case, we modeled xerostomia using the ratio of pre- and 6 months post-RT stimulated salivary function, measured by whole mouth collection. The PCA analysis of figures 2(a), (b) shows that these data are almost linearly separable into high-risk and low-risk groups. Moreover, most of the data are explained by the exponential mean dose model alone, given that there is an uncertainty of 30% in the measurement of saliva. Hence, this type of data will not benefit from kernel-based approaches and could be modeled satisfactorily using conventional methods ( El Naqa et al 2005 ). In figures 2(c) and (d) , we show the esophagitis data, where concurrent chemo is used in conjunction with the Vx (percentage esophagus volume that received at least x Gy). From PCA alone, one can notice that the overlap in the esophagitis case is less than that encountered in the pneumonitis case. As a result, a low-order polynomial kernel may be used to separate these two classes. In figures 2(e) and (f) , we analyzed the pneumonitis endpoint, with a pool of dosimetric variables that consisted only of Vx (i.e., the percentage volume of non-GTV lung that got at least x Gy) (Similar to Su et al (2005) ). As Vx values are highly correlated ( Hope et al 2006 ), dose was binned into eight groups ( V 10, V 20, . . . and V 80). Notice that more than 99% of the variation in the input data was explained by the first two components ( figure 2(e) ). Additionally, the overlap ( figure 2(f) ) between patients with and without radiation pneumonitis is very high, suggesting there is no linear classifier that can adequately separate these two classes. Similar overlap was also observed when other clinical and dosimetric variables were added emphasizing the nonlinear nature of this data. 4.2. Kernel-based modeling examples 4.2.1. NTCP modeling of xerostomia in head and neck cancer In figure 3(a) , we notice that the linear classifier with C = 1 (which indicates a preference for the simplest possible model) had better performance than any higher-order polynomial, with MCC = 0.64. Using the more complex RBFs also did not provide further improvement. This is an example where linear methods should be preferred in the modeling. In contrast, using a feed-forward neural network with seven neurons had an MCC = 0.3, while a GRNN yielded an MCC of 0.6 using a width of 1.75 ( El Naqa et al 2005 ). These results are consistent with our multivariate logistic analysis where a single variable (exponential mean dose model) was sufficient to model the data indicating a sigmoidal dose effect only ( Blanco et al 2005 , Chao et al 2001 ). However, we believe that this case is not the norm but rather the exception in radiation therapy as demonstrated next. 4.2.2. NTCP modeling of esophagitis in lung cancer As shown in figure 4 , the best overall performance was obtained using an SVM with a RBF of width 2 and C = 100 resulting in an MCC value of 0.43 ( figure 4(b) ). It is worth mentioning that this result is better by 21% than the generalizability on LOO cross-validation of the multi-metric logistic regression (MCC = 0.36). 4.2.3. NTCP modeling of pneumonitis in lung cancer We will first consider dosimetric variables only to predict pneumonitis. Using the dosimetric variables, Vx , resulted in selecting C for linear and polynomial kernels as shown in figure 5(a) . It is noted that the higher the polynomial order, the better the prediction accuracy. The best performance was obtained with polynomial kernel of order p = 9 and C = 100. In figure 5(b) , we used an SVM with RBFs, where the width of the Gaussian needs to be determined alongside the regularization parameter yielding improved performance when C = 100 and a width of 5 were applied (MCC = 0.21). For comparison purposes ( El Naqa et al 2005 ), an FFNN with three neurons had an MCC = 0.15. The training time of the FFNN is about one day on a 2.0 GHz Intel processor. Using the fast GRNN training (few seconds), we observed two peaks: one is at ? = 3, and the other one is at ? = 5, both having an MCC value of about 0.2. Multiple peaks are usually a sign that variable preference is not strong. However, we show how to improve the performance by including non-dosimetric variables next. 4.3. Variable selection effects in kernel-based methods Variable selection plays a crucial role in the performance of any prediction method. ( Guyon and Elissee 2003 ) In practice, however, the associations between the dosimetric, biological, clinical variables and the observed endpoints are unknown. The aim of the variable selection process is to find the best set of features that can improve the prediction power. In our previous work on multi-metric modeling ( El Naqa et al 2006 ), we investigated methods based on stepwise forward selection and resampling methods to extract significant variables, and to demonstrate whether one variable set is robust versus a cohort of similarly performing models. We now consider the potential effect of having a heterogeneous dataset that includes non-dosimetric variables in the pneumonitis prediction model. In this section, we explore the effect of variable selection over the entire variable pool on the prediction of pneumonitis in the lung using SVM–RBF as a classifier. A total pool of 58 variables were used, including clinical variables, dosimetric variables, such as Vx (lung volume getting at least x Gy), Dx (minimum dose to the hottest x % lung volume) and the relative location of the tumor within the lung. In figure 6 , we show the top 30 selected variables using a recursive-feature-elimination SVM method, which was previously shown to be an excellent method for gene selection in microarray studies ( Guyon et al 2002 ). We use variable pruning to account for multi-colinearity of correlated variables as shown in figure 6(a) . In figure 6(b) , we show the resulting SVM–RBF classifier using the top six variables (using a cutoff of 5% weighting score). The best MCC obtained was 0.22. In figure 7(a) , we show the results of variable selection using our previous multi-metric approach based on model order selection and resampling with logistic regression ( El Naqa et al 2006 , Hope et al 2006 ). The model order was determined to be 3 with variables of D 35, max dose and COM-SI (center of mass of tumor location in the superior–inferior direction) ( Hope et al 2006 ). Figure 7(b) shows the evaluation results of applying the SVM methodology with RBF kernels using these selected variables. The resulting correlation (MCC = 0.34) on LOO testing data significantly improved our previously achieved multi-metric logistic regression by 46% as discussed below using a C +: C ? ratio of 1.0 ( El Naqa et al 2008 ). The basic interpretation of this improvement is that the SVM automatically identified and accounted for interactions between the model variables. Moreover, using the decomposed SVM approach, to account for data imbalance, with a C +: C ? ratio of 1.5 (imposing higher penalty on missing pneumonitis events) yielded an MCC = 0.36 (60% improvement compared to multi-metric logistic regression and 14% compared to C +: C ? ratio of 1.0). The ratio value in this case was selected by searching the C +: C ? neighborhood from the value of 1–4.2, which is the number of samples to the number of pneumonitis events. 4.4. Comparison with previous work and validation on an independent dataset For comparison purposes, previous reports used Spearman rank correlation ( Rs ) as a visible sign for improvement. For instance, when using the SVM classifier to predict radiation pneumonitis risk we achieved an Rs of 0.37 (or MCC = 0.36) compared to V 20 which yields an Rs = 0.18 (or MCC = 0.20) or with the best logistic regression model resulting in an Rs = 0.22 using LOO evaluation. These results thus provide a 60% improvement in prediction power over our current best model. A comparison of risk prediction using proposed kernel-based approach versus conventional V 20 and our previous three-term logistic regression model is shown in figure 8 . In which the patients are sorted in an ascending order and divided into eight equal groups according to their risk prediction using logistic regression and kernel-based prediction as a reference in figures 8(a) and (b) , respectively. The distinctive ability of the proposed approach to fit both low-risk and high-risk groups is demonstrated even in the case of imbalanced representation of events as in this case. Furthermore, this ability to discriminate between risk groups is demonstrated using complication-free survival plots using the Kaplan–Meier method in figure 9 . Another important consideration for clinical applications is the generalizabilty to unseen data as discussed next. For validation on independent dataset, we used the RTOG 93–11. We have previously reported that the best three-parameter multi-metric model from our institute resulted in poor performance when applied to the RTOG dataset ( Rs = 0.06) ( Bradley et al 2007 ). Using the SVM–RBF model trained solely on our institutional WUSTL (Washington University in St. Louis) data with the same three variables and applied blindly to the RTOG data resulted in Rs = 0.16 (or MCC = 0.15), a slightly better correlation nevertheless statistically significant ( p = 0.049). It is noted that the application of variable selection to the RTOG data using logistic regression yielded D 15 as a single variable model, while using SVM–RFE yielded D 95, D 25 and D 15 as the top three variables in descending order of relevance. However, as mentioned earlier, variable selection plays an important role in learning performance. Therefore, we used mean lung dose (MLD) and dose center in the superior–inferior direction (COM-SI) as variables selected by the logistic regression analysis from the combined datasets ( Bradley et al 2007 ). Then, we trained SVM solely on WUSTL data and applied the resulting learning machine to the RTOG data. The result yielded an Rs = 0.31 (or MCC = 0.28). This generalization ability is typically limited when using conventional methods. In figure 10 , we present the resulting kernel-based pneumonitis nonlinear prediction model as a function f of mean dose and dose center in the superior–inferior direction using an SVM–RBF according to equation (4) ( n s = 128, ? RBF = 3). Based on patient's characteristics (MLD and COM-SI), there are four possible regions for prediction based on the risk group and prediction confidence level: (1) region of low-risk patients with high confidence prediction level ( f ? ?1), (2) region of low-risk patients with lower confidence prediction level (?1 ? f ? 0), (3) region of high-risk patients with lower confidence prediction level (0 ? f ? ?1) and (4) region of high-risk patients with higher confidence prediction level ( f ? 1). These are translated into NTCP prediction probabilities using a sigmoidal function for illustration purposes. The lower confidence level group is patients whose characteristics lie within the margins for cases that are considered ‘border-line’ cases. The corresponding logistic regression plot is shown in figure 11 for comparison where the effect of linear versus nonlinear tessellation of the input space could be contrasted.  4.1. Data exploration and visualization In the head and neck case, we modeled xerostomia using the ratio of pre- and 6 months post-RT stimulated salivary function, measured by whole mouth collection. The PCA analysis of figures 2(a), (b) shows that these data are almost linearly separable into high-risk and low-risk groups. Moreover, most of the data are explained by the exponential mean dose model alone, given that there is an uncertainty of 30% in the measurement of saliva. Hence, this type of data will not benefit from kernel-based approaches and could be modeled satisfactorily using conventional methods ( El Naqa et al 2005 ). In figures 2(c) and (d) , we show the esophagitis data, where concurrent chemo is used in conjunction with the Vx (percentage esophagus volume that received at least x Gy). From PCA alone, one can notice that the overlap in the esophagitis case is less than that encountered in the pneumonitis case. As a result, a low-order polynomial kernel may be used to separate these two classes. In figures 2(e) and (f) , we analyzed the pneumonitis endpoint, with a pool of dosimetric variables that consisted only of Vx (i.e., the percentage volume of non-GTV lung that got at least x Gy) (Similar to Su et al (2005) ). As Vx values are highly correlated ( Hope et al 2006 ), dose was binned into eight groups ( V 10, V 20, . . . and V 80). Notice that more than 99% of the variation in the input data was explained by the first two components ( figure 2(e) ). Additionally, the overlap ( figure 2(f) ) between patients with and without radiation pneumonitis is very high, suggesting there is no linear classifier that can adequately separate these two classes. Similar overlap was also observed when other clinical and dosimetric variables were added emphasizing the nonlinear nature of this data.  4.2. Kernel-based modeling examples 4.2.1. NTCP modeling of xerostomia in head and neck cancer In figure 3(a) , we notice that the linear classifier with C = 1 (which indicates a preference for the simplest possible model) had better performance than any higher-order polynomial, with MCC = 0.64. Using the more complex RBFs also did not provide further improvement. This is an example where linear methods should be preferred in the modeling. In contrast, using a feed-forward neural network with seven neurons had an MCC = 0.3, while a GRNN yielded an MCC of 0.6 using a width of 1.75 ( El Naqa et al 2005 ). These results are consistent with our multivariate logistic analysis where a single variable (exponential mean dose model) was sufficient to model the data indicating a sigmoidal dose effect only ( Blanco et al 2005 , Chao et al 2001 ). However, we believe that this case is not the norm but rather the exception in radiation therapy as demonstrated next. 4.2.2. NTCP modeling of esophagitis in lung cancer As shown in figure 4 , the best overall performance was obtained using an SVM with a RBF of width 2 and C = 100 resulting in an MCC value of 0.43 ( figure 4(b) ). It is worth mentioning that this result is better by 21% than the generalizability on LOO cross-validation of the multi-metric logistic regression (MCC = 0.36). 4.2.3. NTCP modeling of pneumonitis in lung cancer We will first consider dosimetric variables only to predict pneumonitis. Using the dosimetric variables, Vx , resulted in selecting C for linear and polynomial kernels as shown in figure 5(a) . It is noted that the higher the polynomial order, the better the prediction accuracy. The best performance was obtained with polynomial kernel of order p = 9 and C = 100. In figure 5(b) , we used an SVM with RBFs, where the width of the Gaussian needs to be determined alongside the regularization parameter yielding improved performance when C = 100 and a width of 5 were applied (MCC = 0.21). For comparison purposes ( El Naqa et al 2005 ), an FFNN with three neurons had an MCC = 0.15. The training time of the FFNN is about one day on a 2.0 GHz Intel processor. Using the fast GRNN training (few seconds), we observed two peaks: one is at ? = 3, and the other one is at ? = 5, both having an MCC value of about 0.2. Multiple peaks are usually a sign that variable preference is not strong. However, we show how to improve the performance by including non-dosimetric variables next.  4.2.1. NTCP modeling of xerostomia in head and neck cancer In figure 3(a) , we notice that the linear classifier with C = 1 (which indicates a preference for the simplest possible model) had better performance than any higher-order polynomial, with MCC = 0.64. Using the more complex RBFs also did not provide further improvement. This is an example where linear methods should be preferred in the modeling. In contrast, using a feed-forward neural network with seven neurons had an MCC = 0.3, while a GRNN yielded an MCC of 0.6 using a width of 1.75 ( El Naqa et al 2005 ). These results are consistent with our multivariate logistic analysis where a single variable (exponential mean dose model) was sufficient to model the data indicating a sigmoidal dose effect only ( Blanco et al 2005 , Chao et al 2001 ). However, we believe that this case is not the norm but rather the exception in radiation therapy as demonstrated next.  4.2.2. NTCP modeling of esophagitis in lung cancer As shown in figure 4 , the best overall performance was obtained using an SVM with a RBF of width 2 and C = 100 resulting in an MCC value of 0.43 ( figure 4(b) ). It is worth mentioning that this result is better by 21% than the generalizability on LOO cross-validation of the multi-metric logistic regression (MCC = 0.36).  4.2.3. NTCP modeling of pneumonitis in lung cancer We will first consider dosimetric variables only to predict pneumonitis. Using the dosimetric variables, Vx , resulted in selecting C for linear and polynomial kernels as shown in figure 5(a) . It is noted that the higher the polynomial order, the better the prediction accuracy. The best performance was obtained with polynomial kernel of order p = 9 and C = 100. In figure 5(b) , we used an SVM with RBFs, where the width of the Gaussian needs to be determined alongside the regularization parameter yielding improved performance when C = 100 and a width of 5 were applied (MCC = 0.21). For comparison purposes ( El Naqa et al 2005 ), an FFNN with three neurons had an MCC = 0.15. The training time of the FFNN is about one day on a 2.0 GHz Intel processor. Using the fast GRNN training (few seconds), we observed two peaks: one is at ? = 3, and the other one is at ? = 5, both having an MCC value of about 0.2. Multiple peaks are usually a sign that variable preference is not strong. However, we show how to improve the performance by including non-dosimetric variables next.  4.3. Variable selection effects in kernel-based methods Variable selection plays a crucial role in the performance of any prediction method. ( Guyon and Elissee 2003 ) In practice, however, the associations between the dosimetric, biological, clinical variables and the observed endpoints are unknown. The aim of the variable selection process is to find the best set of features that can improve the prediction power. In our previous work on multi-metric modeling ( El Naqa et al 2006 ), we investigated methods based on stepwise forward selection and resampling methods to extract significant variables, and to demonstrate whether one variable set is robust versus a cohort of similarly performing models. We now consider the potential effect of having a heterogeneous dataset that includes non-dosimetric variables in the pneumonitis prediction model. In this section, we explore the effect of variable selection over the entire variable pool on the prediction of pneumonitis in the lung using SVM–RBF as a classifier. A total pool of 58 variables were used, including clinical variables, dosimetric variables, such as Vx (lung volume getting at least x Gy), Dx (minimum dose to the hottest x % lung volume) and the relative location of the tumor within the lung. In figure 6 , we show the top 30 selected variables using a recursive-feature-elimination SVM method, which was previously shown to be an excellent method for gene selection in microarray studies ( Guyon et al 2002 ). We use variable pruning to account for multi-colinearity of correlated variables as shown in figure 6(a) . In figure 6(b) , we show the resulting SVM–RBF classifier using the top six variables (using a cutoff of 5% weighting score). The best MCC obtained was 0.22. In figure 7(a) , we show the results of variable selection using our previous multi-metric approach based on model order selection and resampling with logistic regression ( El Naqa et al 2006 , Hope et al 2006 ). The model order was determined to be 3 with variables of D 35, max dose and COM-SI (center of mass of tumor location in the superior–inferior direction) ( Hope et al 2006 ). Figure 7(b) shows the evaluation results of applying the SVM methodology with RBF kernels using these selected variables. The resulting correlation (MCC = 0.34) on LOO testing data significantly improved our previously achieved multi-metric logistic regression by 46% as discussed below using a C +: C ? ratio of 1.0 ( El Naqa et al 2008 ). The basic interpretation of this improvement is that the SVM automatically identified and accounted for interactions between the model variables. Moreover, using the decomposed SVM approach, to account for data imbalance, with a C +: C ? ratio of 1.5 (imposing higher penalty on missing pneumonitis events) yielded an MCC = 0.36 (60% improvement compared to multi-metric logistic regression and 14% compared to C +: C ? ratio of 1.0). The ratio value in this case was selected by searching the C +: C ? neighborhood from the value of 1–4.2, which is the number of samples to the number of pneumonitis events.  4.4. Comparison with previous work and validation on an independent dataset For comparison purposes, previous reports used Spearman rank correlation ( Rs ) as a visible sign for improvement. For instance, when using the SVM classifier to predict radiation pneumonitis risk we achieved an Rs of 0.37 (or MCC = 0.36) compared to V 20 which yields an Rs = 0.18 (or MCC = 0.20) or with the best logistic regression model resulting in an Rs = 0.22 using LOO evaluation. These results thus provide a 60% improvement in prediction power over our current best model. A comparison of risk prediction using proposed kernel-based approach versus conventional V 20 and our previous three-term logistic regression model is shown in figure 8 . In which the patients are sorted in an ascending order and divided into eight equal groups according to their risk prediction using logistic regression and kernel-based prediction as a reference in figures 8(a) and (b) , respectively. The distinctive ability of the proposed approach to fit both low-risk and high-risk groups is demonstrated even in the case of imbalanced representation of events as in this case. Furthermore, this ability to discriminate between risk groups is demonstrated using complication-free survival plots using the Kaplan–Meier method in figure 9 . Another important consideration for clinical applications is the generalizabilty to unseen data as discussed next. For validation on independent dataset, we used the RTOG 93–11. We have previously reported that the best three-parameter multi-metric model from our institute resulted in poor performance when applied to the RTOG dataset ( Rs = 0.06) ( Bradley et al 2007 ). Using the SVM–RBF model trained solely on our institutional WUSTL (Washington University in St. Louis) data with the same three variables and applied blindly to the RTOG data resulted in Rs = 0.16 (or MCC = 0.15), a slightly better correlation nevertheless statistically significant ( p = 0.049). It is noted that the application of variable selection to the RTOG data using logistic regression yielded D 15 as a single variable model, while using SVM–RFE yielded D 95, D 25 and D 15 as the top three variables in descending order of relevance. However, as mentioned earlier, variable selection plays an important role in learning performance. Therefore, we used mean lung dose (MLD) and dose center in the superior–inferior direction (COM-SI) as variables selected by the logistic regression analysis from the combined datasets ( Bradley et al 2007 ). Then, we trained SVM solely on WUSTL data and applied the resulting learning machine to the RTOG data. The result yielded an Rs = 0.31 (or MCC = 0.28). This generalization ability is typically limited when using conventional methods. In figure 10 , we present the resulting kernel-based pneumonitis nonlinear prediction model as a function f of mean dose and dose center in the superior–inferior direction using an SVM–RBF according to equation (4) ( n s = 128, ? RBF = 3). Based on patient's characteristics (MLD and COM-SI), there are four possible regions for prediction based on the risk group and prediction confidence level: (1) region of low-risk patients with high confidence prediction level ( f ? ?1), (2) region of low-risk patients with lower confidence prediction level (?1 ? f ? 0), (3) region of high-risk patients with lower confidence prediction level (0 ? f ? ?1) and (4) region of high-risk patients with higher confidence prediction level ( f ? 1). These are translated into NTCP prediction probabilities using a sigmoidal function for illustration purposes. The lower confidence level group is patients whose characteristics lie within the margins for cases that are considered ‘border-line’ cases. The corresponding logistic regression plot is shown in figure 11 for comparison where the effect of linear versus nonlinear tessellation of the input space could be contrasted.  5. Discussion Attempting to uncover predictive relationships in radiation oncology dose–volume analyses is hampered by many effects, of which we will briefly discuss four. First, effective modeling can only occur if the important features that determine outcomes are indeed included in the feature set (‘ Variable set adequacy ’). Second, many of the variables (especially dosimetric variables) are highly correlated for most types of treatments (‘ Variable correlations ’). Third, limitations in dataset size impose sampling fluctuations (‘ Sample noise ’). A key consequence of Variable correlations and Sample noise together is that there is typically no such thing as ‘the model’: many candidate models may often have similar predictive or correlative power. This issue was partially addressed elsewhere via model building based on bootstrap resampling ( Deasy and El Naqa 2007 , El Naqa et al 2006 ). Last, the variables may interact in a complicated, nonlinear way to determine, or at least correlate with, the endpoint (‘ Variable interactions ’). In our experience, for example, it appears that the risk model for xerostomia is relatively effective, whereas outcome models for pneumonitis are less so (though still useful for high-and low-risk plans). Therefore, by extending the modeling process to a nonlinear framework, including higher-order variable interactions and local variable averaging, we hope to better capture real classification effects. The kernel approach presented here automates the search for higher-order interactions between input variables that may be relevant to risk classification and outcomes in addition to balancing generalizabilty versus fidelity to the data. This is accomplished through an implicit nonlinear mapping. The kernel/SVM approach maximizes the separation between events and non-events in feature space. Moreover, this approach has often been credited to yield better generalization with small datasets compared to the standard maximum likelihood approach ( Schèolkopf and Smola 2002 , Shawe-Taylor and Cristianini 2004 ). Potential benefit from these methods can be predicted on the basis of PCA: if responses may be separated along a linear ridge in a PCA plot, then linear methods probably work well and nonlinear methods are unnecessary (cf figure 2 ). If there is no such linear ridge under PCA analysis, it is more likely that nonlinear features generated via kernel/SVM methods will improve the model prediction. For instance, no clear improvement was noticed in the case of linearly separable data (the xerostomia dataset) while using the kernel model versus traditional techniques. However, this should be evaluated on a case-by-case basis. For instance, if the two classes could be described by Gaussian distribution and have similar covariance matrices, a linear Bayesian classifier could be the optimal classifier. In the current approach, variables were selected according to weights automatically generated by the kernel/SVM algorithm. There is a trade-off between the regularization parameter, C , and bias. When C is large, there are many non-zero variable terms, and the trend is toward overfitting the data, with increased noise in the predictions. When C is small, many of the variables are dropped or reduced but the model may not capture real trends, thus introducing bias. We further introduced the concept of regularization decomposition, where risk groups could be weighted differently based on the available number of events. This approach seemed to improve the prediction power. Another advantage that is demonstrated on the independent multi-institutional dataset is the ability to perform well on an unseen data before, which is something not accounted for by other methods prone to over-fitting. The plot in figure 10 could be used as a guideline in clinical practice for better prediction of pneumonitis risk based on this model. Estimates of the model could be used to stratify patients into different risk groups and therefore modulate treatment regimens accordingly. A favorable feature of this framework is that it highlights areas where the confidence level of prediction power is weak (inside the margin) versus areas of strong confidence level (outside the margin). Therefore, strengths and limitations of the model are provided as well to the analyst. One of the main challenges of this framework is the selection of most relevant variables to include within the model. This is important clinically because it supports increased focus on potentially causative factors. Hence, future work will be dedicated to optimize the selection of the significant variables. As mentioned earlier, off-the-shelf techniques often fail to address the specificity of this application. Our previous selection method based on resampling and information theory seems to have produced better generalization results in comparison to SVM–RFE ( El Naqa et al 2006 ). However, using nonlinear sensitivity analysis of kernel methods ( Rakotomamonjy 2003 ) in conjunction with resampling techniques may provide better opportunities for robust selection of relevant variables. Another issue that might have limited the predictive ability of the presented esophagitis or pneumonitis models ( Rs < 0.5) is missing relevant variables that could be related to the genetics of the patient or underlying biology of the disease that are not currently captured in our existing clinical archives. To explore this issue, we are currently conducting a prospective pilot study in NSCLC patients that aims to build a comprehensive archive of clinical, physical dosimetric variables and relevant biological markers. This could potentially compensate for the observed prediction gap ( Spencer et al 2009 ). In this case, machine-learning algorithms are poised to play an increasingly important role in delineating such complex physical and biological interactions.  5. Discussion Attempting to uncover predictive relationships in radiation oncology dose–volume analyses is hampered by many effects, of which we will briefly discuss four. First, effective modeling can only occur if the important features that determine outcomes are indeed included in the feature set (‘ Variable set adequacy ’). Second, many of the variables (especially dosimetric variables) are highly correlated for most types of treatments (‘ Variable correlations ’). Third, limitations in dataset size impose sampling fluctuations (‘ Sample noise ’). A key consequence of Variable correlations and Sample noise together is that there is typically no such thing as ‘the model’: many candidate models may often have similar predictive or correlative power. This issue was partially addressed elsewhere via model building based on bootstrap resampling ( Deasy and El Naqa 2007 , El Naqa et al 2006 ). Last, the variables may interact in a complicated, nonlinear way to determine, or at least correlate with, the endpoint (‘ Variable interactions ’). In our experience, for example, it appears that the risk model for xerostomia is relatively effective, whereas outcome models for pneumonitis are less so (though still useful for high-and low-risk plans). Therefore, by extending the modeling process to a nonlinear framework, including higher-order variable interactions and local variable averaging, we hope to better capture real classification effects. The kernel approach presented here automates the search for higher-order interactions between input variables that may be relevant to risk classification and outcomes in addition to balancing generalizabilty versus fidelity to the data. This is accomplished through an implicit nonlinear mapping. The kernel/SVM approach maximizes the separation between events and non-events in feature space. Moreover, this approach has often been credited to yield better generalization with small datasets compared to the standard maximum likelihood approach ( Schèolkopf and Smola 2002 , Shawe-Taylor and Cristianini 2004 ). Potential benefit from these methods can be predicted on the basis of PCA: if responses may be separated along a linear ridge in a PCA plot, then linear methods probably work well and nonlinear methods are unnecessary (cf figure 2 ). If there is no such linear ridge under PCA analysis, it is more likely that nonlinear features generated via kernel/SVM methods will improve the model prediction. For instance, no clear improvement was noticed in the case of linearly separable data (the xerostomia dataset) while using the kernel model versus traditional techniques. However, this should be evaluated on a case-by-case basis. For instance, if the two classes could be described by Gaussian distribution and have similar covariance matrices, a linear Bayesian classifier could be the optimal classifier. In the current approach, variables were selected according to weights automatically generated by the kernel/SVM algorithm. There is a trade-off between the regularization parameter, C , and bias. When C is large, there are many non-zero variable terms, and the trend is toward overfitting the data, with increased noise in the predictions. When C is small, many of the variables are dropped or reduced but the model may not capture real trends, thus introducing bias. We further introduced the concept of regularization decomposition, where risk groups could be weighted differently based on the available number of events. This approach seemed to improve the prediction power. Another advantage that is demonstrated on the independent multi-institutional dataset is the ability to perform well on an unseen data before, which is something not accounted for by other methods prone to over-fitting. The plot in figure 10 could be used as a guideline in clinical practice for better prediction of pneumonitis risk based on this model. Estimates of the model could be used to stratify patients into different risk groups and therefore modulate treatment regimens accordingly. A favorable feature of this framework is that it highlights areas where the confidence level of prediction power is weak (inside the margin) versus areas of strong confidence level (outside the margin). Therefore, strengths and limitations of the model are provided as well to the analyst. One of the main challenges of this framework is the selection of most relevant variables to include within the model. This is important clinically because it supports increased focus on potentially causative factors. Hence, future work will be dedicated to optimize the selection of the significant variables. As mentioned earlier, off-the-shelf techniques often fail to address the specificity of this application. Our previous selection method based on resampling and information theory seems to have produced better generalization results in comparison to SVM–RFE ( El Naqa et al 2006 ). However, using nonlinear sensitivity analysis of kernel methods ( Rakotomamonjy 2003 ) in conjunction with resampling techniques may provide better opportunities for robust selection of relevant variables. Another issue that might have limited the predictive ability of the presented esophagitis or pneumonitis models ( Rs < 0.5) is missing relevant variables that could be related to the genetics of the patient or underlying biology of the disease that are not currently captured in our existing clinical archives. To explore this issue, we are currently conducting a prospective pilot study in NSCLC patients that aims to build a comprehensive archive of clinical, physical dosimetric variables and relevant biological markers. This could potentially compensate for the observed prediction gap ( Spencer et al 2009 ). In this case, machine-learning algorithms are poised to play an increasingly important role in delineating such complex physical and biological interactions.  6. Conclusions We have demonstrated an innovative approach for model exploration and building in radiotherapy based on nonlinear kernel-based statistical learning. The method was evaluated using resampling methods and validated on an independent dataset. The method efficiently and effectively handles high-dimensional space of potentially critical features. These methods are known to possess superior statistical power when learning from smaller sample sizes. For cases where nonlinear effects are important, this technique can significantly improve on the best results we achieved from the previous methods, by considering variable interactions and ability to generalize to unseen data. Future work will examine other aspects of nonlinear modeling for outcomes, such as incorporating prior information, adapting the kernel specifically to the expected response structure, this, as well as addressing the variable selection problem more comprehensively. 