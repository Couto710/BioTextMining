Fast iterative image reconstruction methods for fully 3D multispectral bioluminescence tomography We investigate fast iterative image reconstruction methods for fully 3D multispectral bioluminescence tomography for applications in small animal imaging. Our forward model uses a diffusion approximation for optically inhomogeneous tissue, which we solve using a finite element method (FEM). We examine two approaches to incorporating the forward model into the solution of the inverse problem. In a conventional direct calculation approach one computes the full forward model by repeated solution of the FEM problem, once for each potential source location. We describe an alternative on-the-fly approach where one does not explicitly solve for the full forward model. Instead, the solution to the forward problem is included implicitly in the formulation of the inverse problem, and the FEM problem is solved at each iteration for the current image estimate. We evaluate the convergence speeds of several representative iterative algorithms. We compare the computation cost of those two approaches, concluding that the on-the-fly approach can lead to substantial reductions in total cost when combined with a rapidly converging iterative algorithm.  1. Introduction Bioluminescence tomography is an in vivo imaging technique that localizes and quantifies bioluminescent sources in a small animal. It has recently gained a great deal of attention as a promising means for macroscopic in vivo imaging of gene expression and other molecular and cellular-level processes ( Contag and Bachmann 2002 , Ntziachristos et al 2005 ). The objective of bioluminescence tomography is to reconstruct the three-dimensional (3D) spatial distribution of bioluminescent sources inside the animal from images of the light emitted through the animal surface. Collecting measurement data in multiple spectral bands and from more than one view, so that the entire animal surface is imaged, facilitates tomographic reconstruction of the 3D optical source distribution ( Chaudhari et al 2005 ). We have set up a bioluminescence tomographic imaging system using mirrors to collect four independent views ( Chaudhari et al 2005 ). Multispectral data were acquired to improve localization ( Chaudhari et al 2005 , Dehghani et al 2006 , Cong and Wang 2006 , Wang et al 2006b , Kuo et al 2007 , Allard et al 2007 , Lv et al 2007 ). We then used an FEM solution of the diffusion equation to construct realistic forward models. We have successfully localized bioluminescent sources with this system, using the TOAST software to compute the forward model ( Arridge et al 1993 ). However, TOAST is primarily intended to solve the diffuse optical tomography problem and is not well adapted for bioluminescence tomography applications; the forward model computation cost is high. Fully 3D multispectral bioluminescence tomography is a computationally challenging inverse problem because (1) FEM-based forward models, which allow inhomogeneous tissue and realistic geometry, require the inversion of a very large matrix to solve the forward problem, (2) the use of multispectral data, which helps localize deep sources, increases the problem size by the number of spectral bins and (3) the inverse problem is intrinsically ill-posed due to the nature of the photon diffusion process and the limited information contained in data collected only on the animal surface. In this paper we investigate various numerical techniques in order to minimize the cost of computing inverse solutions. The inverse problem is equivalent to a search for the source distribution which best predicts the measured data while also satisfying an a priori assumption regarding the smoothness of the source distribution. In order to iteratively solve the inverse problem, a forward model which maps a source to the data domain must be computed repeatedly. We explore two different approaches to incorporating the forward solutions into iterative algorithms for the inverse problem: (1) a direct calculation approach where the forward problem is solved in advance for every source location and then a forward solution is computed as a linear combination of these precomputed solutions, and (2) an on-the-fly approach where the FEM matrix inversion problem is solved as needed at each iteration using the current estimate of the source configuration. In this way, the full forward model need never be computed. We evaluate those approaches, combined with several different iterative algorithms, to determine the combination of forward and inverse algorithm that will minimize the computational cost. Our goal in this paper is to introduce and evaluate methods that can rapidly reconstruct 3D bioluminescent images while still using a realistic FEM forward model. To the best of our knowledge, the on-the-fly approach is novel to this application and can substantially reduce the total reconstruction time, as shown in section 4. The iterative algorithms we consider in this paper include the class of incremental gradient or ordered subset methods, which have been widely explored for applications in nuclear medicine imaging ( Hudson and Larkin 1994 , Browne and De Pierro 1996 , Ahn and Fessler 2003 ) but have not been investigated in the optical imaging literature. The algebraic reconstruction technique (ART) ( Gordon et al 1970 ), which is a special case of the incremental gradient algorithms, was used for diffuse optical tomography ( Arridge and Schweiger 1998 ). Our comparison of reconstruction times for the different methods described above should help guide the selection of reconstruction and forward modeling algorithms for developers of 3D bioluminescent and fluorescent imaging systems when using FEM-based forward models. In section 2 we formulate the forward problem based on the diffusion equation using FEM models and describe approaches to incorporating the FEM solvers into iterative algorithms. In section 3 we define the inverse problem using regularized least squares (RLS) and describe various iterative algorithms for computing the inverse solution. Finally, in section 4 we evaluate different reconstruction methods in terms of computation times using in vivo bioluminescent imaging data from a mouse with an implanted brain tumor.  2. Forward problem In the forward problem one needs to predict the photon flux from the animal surface for a given bioluminescent source distribution using known optical scatter and absorption properties within the animal. Since iterative solutions to the inverse problem require multiple solutions to the forward problem, a computationally efficient forward solution is important. 2.1. Forward model Photon propagation in turbid media can be described by the radiative transfer or Boltzmann transport equation ( Arridge 1999 ). While this model can be used in bioluminescence tomography ( Klose 2007 ), the high computation cost and detailed knowledge required of the media's optical properties limit its use in practice. In contrast, the diffusion approximation under the assumption of isotropic scattering has been used extensively ( Arridge et al 1993 , Schweiger et al 1995 , Jiang 1998 , Arridge 1999 , Hielscher et al 1999 , Gu et al 2004 , Wang et al 2004 , Cong et al 2005 , Slavine et al 2006 , Dehghani et al 2006 , Comsa et al 2006 , Soloviev 2007 , Kuo et al 2007 ). The diffusion approximation is reasonably accurate in soft tissue in the near-infrared region where scattering dominates absorption ( Arridge 1999 , Shen et al 2007 ). We, therefore, focus on the forward model based on the following steady-state diffusion equation ( Arridge 1999 ), (1) { ? ? ? ? ( r , ? ) ? + ? a ( r , ? ) } ? ( r , ? ) = q ( r , ? ) , for r ? ? , subject to a Robin boundary condition (2) ? ( r , ? ) + 2 ? ( r , ? ) G n ^ ( r ) ? ? ? ( r , ? ) = 0 for r ? ? ? where q represents the bioluminescent source distribution, ? denotes the photon density, r ? R 3 denotes the location vector, ? ? R 3 is the animal volume, ?? is the animal surface, n ^ is a unit vector pointed outwardly normal to the surface ??, ? is the wavelength and ? ( r , ? ) = 1 ? [ 3 { ? a ( r , ? ) + ? s ? ( r , ? ) } ] , with ?a and ? s ? being the absorption and reduced scattering coefficients, respectively. In the boundary condition ( 2 ), G is a parameter modeling internal reflection at the boundary and can be computed as G = (1 + ?)/(1 ? ?), where ? ? ? 1.4399 n int ? 2 + 0.7099 n int ? 1 + 0.6681 + 0.0636 n int with n int being the refractive index of tissue ( Schweiger et al 1995 ). The measurable photon flux at r ? ?? is given by m ( r , ?) = ?( r , ?)/(2G). The mapping from the source distribution q in the volume to the photon flux m through the surface can be obtained by solving ( 1 ) for ? given q . Approaches to solving the diffusion equation (1) include analytical methods that assume simplified geometry and homogeneous tissue ( Rice et al 2001 ) and the more general finite element method (FEM) ( Arridge et al 1993 , 2000 , Schweiger and Arridge 1997 , Jiang 1998 , Arridge 1999 , Roy and Sevick-Muraca 2001 , Gu et al 2004 , Cong et al 2005 , Alexandrakis et al 2005 , Chaudhari et al 2005 , Lv et al 2006 ). Although the analytical method is more computationally efficient, simplifying assumptions about geometry and optical properties can produce inaccurate results ( Chaudhari et al 2005 ). Here we focus on the FEM-based forward model. In the FEM framework, the 3D animal volume ? is discretized into tetrahedral elements connected at v vertex nodes. The source q and the photon density ? are also discretized using finite element basis functions and are represented by q ? R v and ? ? R v , respectively. Then the problem of solving the diffusion equation reduces to (3) F ? = q where F ? R v × v . The FEM matrix F is a function of the optical absorption and reduced scattering coefficients and refractive index. The optical properties can be specified using published measurements applied to a segmented MR or CT volume ( Wang et al 2004 , 2006a , Chaudhari et al 2005 , Alexandrakis et al 2005 , 2006 , Lv et al 2006 , Allard et al 2007 ). A detailed description of how to construct F can be found in Arridge et al (2000) . The FEM matrix F is symmetric ( Arridge et al (2000) , equations (29) –(34)). It is also sparse with on the order of 10 nonzero elements per row. 2.2. Forward solution method There are two standard methods for solving the inversion problem in ( 3 ) for ? : (1) Cholesky factorization and substitution (CFS) and (2) preconditioned conjugate gradient (PCG) ( Arridge et al 1993 , Davies et al 1997 , Schweiger and Arridge 1997 , Arridge 1999 ). See Schweiger and Arridge (1997) for an overview of those methods and an analysis of their computational complexities. We chose CFS for our forward problem solver since CFS was faster than PCG in our experiments using a realistic FEM mesh; a comparison can be found in Chaudhari (2006) and Ahn et al (2007a) . The reason that CFS was more efficient in our case is that if Cholesky factorization is done once, subsequent forward and back-substitutions can be inexpensively performed for multiple sources, as noted in Schweiger and Arridge (1997) . In CFS, F is first decomposed into Cholesky factors as F = U ? U , where U is an upper triangular matrix and ? represents matrix or vector transpose. Next one solves U ? c = q for c ? R v by forward substitution and then solves U ? = c for ? by back-substitution. The Cholesky factorization to calculate U is a one-time computation for a given FEM model F and the forward and back-substitution can be repeatedly computed for different q 's using the precomputed Cholesky factor U . 2.3. System matrix Let w i ? R v be the load vector for a unit source at voxel i and W = [ w 1 w 2 … w p ] ? R v × p be a matrix of the load vectors. Then the source representation x ? R p using the load vectors as a basis, and the corresponding source q ? R v in tessellation nodes have the following relationship: (4) q = Wx . The data y ? R N , which denote the photon flux through the animal surface as measured by a CCD camera, are given by (5) y = D ? where D ? R N × v maps the photon density in the volume to the CCD camera measurements. Typically, p < v and N < v ; therefore, W is a thin matrix and D is wide. Combining ( 3 ), ( 4 ) and ( 5 ), we construct a system matrix A ? R N × p that transforms the source x into the measurement data y such that (6) y = Ax = DF ? 1 Wx . The matrices D and W are sparse so the cost for multiplying a vector by D or W is negligible. Note that the system matrix A is not sparse whereas F is. To compute F ?1 q for some q , one needs to solve the forward problem ( 3 ), as discussed in section 2.2. 2.4. Forward and back-projector implementation approach The two important operations involving the system matrix A are `forward projection' and `back-projection,' referred to jointly below as F/B projection. This terminology is borrowed from computed tomography ( Herman 1980 ) although a mapping from a source to measurement data analogous to the system matrix was called the projection operator and the data were called the projection data in the context of optical tomography in Arridge and Schweiger (1998) . For forward projection one multiplies a vector by the system matrix, that is, one computes y = Ax for some x ? R p . For back-projection one computes x = A ? y for some y ? R N . One needs to compute F/B projection multiple times in iterative algorithms. Since the time required for F/B projection is a major contributor to the total reconstruction time, it is important to implement F/B projection operators efficiently. We investigate two approaches to implementing F/B projectors: a straightforward scheme described below we call the `direct calculation' approach and a new `on-the-fly' method. 2.4.1. Direct calculation approach In the direct calculation approach, one precomputes the full system matrix A and implements the F/B projector by a matrix-vector multiplication. Each column of the system matrix is computed by taking a unit vector corresponding to each source location as follows: (7) compute Cholesky factor U for j = 1 , … , p q ^ = We j solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ store y ^ as the j th column of A end where e j = [ 0 , … , 0 , 1 , 0 , … , 0 ] ? ? R p is the j th unit vector with only the j th entry being 1. The substitution step in ( 7 ) must be repeated for a total of p source locations. However, once A is precomputed, F/B projection can be implemented as direct matrix-vector multiplications, Ax and A ? y . 2.4.2. On-the-fly approach An alternative that avoids precomputing the full system matrix, is to calculate the F/B projection in an on-the-fly manner. The on-the-fly forward projector can be implemented as follows: (8) compute Cholesky factor U if “ Ax ” needs to be computed for some x ? R p q ^ = Wx solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ , which yields “ Ax ” end . Similarly, in view of the symmetry of F , one can compute the back-projection on the fly as follows: (9) compute Cholesky factor U if “ A ? y ” needs to be computed for some y ? R N ? ^ = D ? y solve U ? U q ^ = ? ^ for q ^ by forward and back-substitution x ^ = W ? q ^ , which yield “ A ? y ” end . Every time we compute a forward or back-projection in the on-the-fly approach, we need to perform the substitution step only once. 2.4.3. Comparison In the direct calculation approach, it is very expensive to precompute the full system matrix A ? R N × p because the forward problem must be solved p times where p is the number of source voxels. However, once the matrix has been precomputed, the F/B projection can be computed rapidly as a single matrix vector product. On the other hand, in the on-the-fly approach, whenever one needs to compute the forward or back-projection, one must solve the forward problem in ( 3 ), which is more expensive than computing the product of the system matrix and a vector, partly because the FEM matrix F ? R v × v is larger than the system matrix A ? R N × p (typically, N < v and p < v ) despite the sparsity of F . For example, in the multispectral bioluminescence tomography system we consider in section 4, v 2 was larger than Np by a factor of >150. Which of the direct calculation and on-the-fly approaches should be used? This depends on how many images are to be reconstructed for a given forward model, how many iterations per image are required for practical convergence of an iterative algorithm and how many F/B projections per iteration are needed. If one wishes to investigate many different iterative algorithms and regularization schemes for a given subject or if one assumes that the animal's anatomical structure and position do not change much over time in a longitudinal study, the direct calculation approach will be beneficial because once the expensive precomputation is performed, subsequent forward solutions can be rapidly computed by matrix vector multiplication. In contrast, if one is to compute relatively few images with a small number of iterations, the on-the-fly approach should be chosen. Comparisons of image reconstruction times for different approaches using real mouse bioluminescent data are made in section 4. One of the advantages of the direct calculation approach is that direct access to any column or row in the system matrix is possible when the matrix is precomputed. Column or row access is required by certain iterative algorithms such as coordinate descent and incremental gradient algorithms, as discussed in section 3. Furthermore, the direct calculation approach may be more readily parallelizable than the on-the-fly approach. Interestingly, the on-the-fly approach is closely connected with the adjoint or reverse differentiation method which is widely used for computing the gradient of a cost function in nonlinear diffuse optical tomography ( Davies et al 1997 , Hielscher et al 1999 ), as discussed in the appendix . 2.5. Multispectral system model In multispectral imaging with K spectral bins, a multispectral system matrix A mul is constructed by concatenating monochromatic system matrices A 1(?1), …, A K (? K ) for wavelengths ?1, …, ? K ( Chaudhari et al 2005 ): (10) y mul = A mul x where A mul = [ s 1 A 1 ( ? 1 ) ? … s K A K ( ? K ) ? ] ? ? R N K × p with s 1, …, s K being known emission spectra of the bioluminescent source, and y mul = [ y 1 ( ? 1 ) ? … y K ( ? K ) ? ] ? ? R N K the multispectral data. It is straightforward to apply the F/B projection methods discussed in section 2.4 by combining individual F/B projections for each wavelength. That is, the forward projection is given by y mul = A mul x = [ s 1 ( A 1 ( ? 1 ) x ) ? … s K ( A K ( ? K ) x ) ? ] ? and the back-projection x = A ? mul y mul = ? k = 1 K s k A k ( ? k ) ? y k ( ? k ) . Henceforth, we omit the subscript in A mul and y mul for notational simplicity and consider both monochromatic and multispectral systems in a common framework.  2.1. Forward model Photon propagation in turbid media can be described by the radiative transfer or Boltzmann transport equation ( Arridge 1999 ). While this model can be used in bioluminescence tomography ( Klose 2007 ), the high computation cost and detailed knowledge required of the media's optical properties limit its use in practice. In contrast, the diffusion approximation under the assumption of isotropic scattering has been used extensively ( Arridge et al 1993 , Schweiger et al 1995 , Jiang 1998 , Arridge 1999 , Hielscher et al 1999 , Gu et al 2004 , Wang et al 2004 , Cong et al 2005 , Slavine et al 2006 , Dehghani et al 2006 , Comsa et al 2006 , Soloviev 2007 , Kuo et al 2007 ). The diffusion approximation is reasonably accurate in soft tissue in the near-infrared region where scattering dominates absorption ( Arridge 1999 , Shen et al 2007 ). We, therefore, focus on the forward model based on the following steady-state diffusion equation ( Arridge 1999 ), (1) { ? ? ? ? ( r , ? ) ? + ? a ( r , ? ) } ? ( r , ? ) = q ( r , ? ) , for r ? ? , subject to a Robin boundary condition (2) ? ( r , ? ) + 2 ? ( r , ? ) G n ^ ( r ) ? ? ? ( r , ? ) = 0 for r ? ? ? where q represents the bioluminescent source distribution, ? denotes the photon density, r ? R 3 denotes the location vector, ? ? R 3 is the animal volume, ?? is the animal surface, n ^ is a unit vector pointed outwardly normal to the surface ??, ? is the wavelength and ? ( r , ? ) = 1 ? [ 3 { ? a ( r , ? ) + ? s ? ( r , ? ) } ] , with ?a and ? s ? being the absorption and reduced scattering coefficients, respectively. In the boundary condition ( 2 ), G is a parameter modeling internal reflection at the boundary and can be computed as G = (1 + ?)/(1 ? ?), where ? ? ? 1.4399 n int ? 2 + 0.7099 n int ? 1 + 0.6681 + 0.0636 n int with n int being the refractive index of tissue ( Schweiger et al 1995 ). The measurable photon flux at r ? ?? is given by m ( r , ?) = ?( r , ?)/(2G). The mapping from the source distribution q in the volume to the photon flux m through the surface can be obtained by solving ( 1 ) for ? given q . Approaches to solving the diffusion equation (1) include analytical methods that assume simplified geometry and homogeneous tissue ( Rice et al 2001 ) and the more general finite element method (FEM) ( Arridge et al 1993 , 2000 , Schweiger and Arridge 1997 , Jiang 1998 , Arridge 1999 , Roy and Sevick-Muraca 2001 , Gu et al 2004 , Cong et al 2005 , Alexandrakis et al 2005 , Chaudhari et al 2005 , Lv et al 2006 ). Although the analytical method is more computationally efficient, simplifying assumptions about geometry and optical properties can produce inaccurate results ( Chaudhari et al 2005 ). Here we focus on the FEM-based forward model. In the FEM framework, the 3D animal volume ? is discretized into tetrahedral elements connected at v vertex nodes. The source q and the photon density ? are also discretized using finite element basis functions and are represented by q ? R v and ? ? R v , respectively. Then the problem of solving the diffusion equation reduces to (3) F ? = q where F ? R v × v . The FEM matrix F is a function of the optical absorption and reduced scattering coefficients and refractive index. The optical properties can be specified using published measurements applied to a segmented MR or CT volume ( Wang et al 2004 , 2006a , Chaudhari et al 2005 , Alexandrakis et al 2005 , 2006 , Lv et al 2006 , Allard et al 2007 ). A detailed description of how to construct F can be found in Arridge et al (2000) . The FEM matrix F is symmetric ( Arridge et al (2000) , equations (29) –(34)). It is also sparse with on the order of 10 nonzero elements per row.  2.2. Forward solution method There are two standard methods for solving the inversion problem in ( 3 ) for ? : (1) Cholesky factorization and substitution (CFS) and (2) preconditioned conjugate gradient (PCG) ( Arridge et al 1993 , Davies et al 1997 , Schweiger and Arridge 1997 , Arridge 1999 ). See Schweiger and Arridge (1997) for an overview of those methods and an analysis of their computational complexities. We chose CFS for our forward problem solver since CFS was faster than PCG in our experiments using a realistic FEM mesh; a comparison can be found in Chaudhari (2006) and Ahn et al (2007a) . The reason that CFS was more efficient in our case is that if Cholesky factorization is done once, subsequent forward and back-substitutions can be inexpensively performed for multiple sources, as noted in Schweiger and Arridge (1997) . In CFS, F is first decomposed into Cholesky factors as F = U ? U , where U is an upper triangular matrix and ? represents matrix or vector transpose. Next one solves U ? c = q for c ? R v by forward substitution and then solves U ? = c for ? by back-substitution. The Cholesky factorization to calculate U is a one-time computation for a given FEM model F and the forward and back-substitution can be repeatedly computed for different q 's using the precomputed Cholesky factor U .  2.3. System matrix Let w i ? R v be the load vector for a unit source at voxel i and W = [ w 1 w 2 … w p ] ? R v × p be a matrix of the load vectors. Then the source representation x ? R p using the load vectors as a basis, and the corresponding source q ? R v in tessellation nodes have the following relationship: (4) q = Wx . The data y ? R N , which denote the photon flux through the animal surface as measured by a CCD camera, are given by (5) y = D ? where D ? R N × v maps the photon density in the volume to the CCD camera measurements. Typically, p < v and N < v ; therefore, W is a thin matrix and D is wide. Combining ( 3 ), ( 4 ) and ( 5 ), we construct a system matrix A ? R N × p that transforms the source x into the measurement data y such that (6) y = Ax = DF ? 1 Wx . The matrices D and W are sparse so the cost for multiplying a vector by D or W is negligible. Note that the system matrix A is not sparse whereas F is. To compute F ?1 q for some q , one needs to solve the forward problem ( 3 ), as discussed in section 2.2.  2.4. Forward and back-projector implementation approach The two important operations involving the system matrix A are `forward projection' and `back-projection,' referred to jointly below as F/B projection. This terminology is borrowed from computed tomography ( Herman 1980 ) although a mapping from a source to measurement data analogous to the system matrix was called the projection operator and the data were called the projection data in the context of optical tomography in Arridge and Schweiger (1998) . For forward projection one multiplies a vector by the system matrix, that is, one computes y = Ax for some x ? R p . For back-projection one computes x = A ? y for some y ? R N . One needs to compute F/B projection multiple times in iterative algorithms. Since the time required for F/B projection is a major contributor to the total reconstruction time, it is important to implement F/B projection operators efficiently. We investigate two approaches to implementing F/B projectors: a straightforward scheme described below we call the `direct calculation' approach and a new `on-the-fly' method. 2.4.1. Direct calculation approach In the direct calculation approach, one precomputes the full system matrix A and implements the F/B projector by a matrix-vector multiplication. Each column of the system matrix is computed by taking a unit vector corresponding to each source location as follows: (7) compute Cholesky factor U for j = 1 , … , p q ^ = We j solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ store y ^ as the j th column of A end where e j = [ 0 , … , 0 , 1 , 0 , … , 0 ] ? ? R p is the j th unit vector with only the j th entry being 1. The substitution step in ( 7 ) must be repeated for a total of p source locations. However, once A is precomputed, F/B projection can be implemented as direct matrix-vector multiplications, Ax and A ? y . 2.4.2. On-the-fly approach An alternative that avoids precomputing the full system matrix, is to calculate the F/B projection in an on-the-fly manner. The on-the-fly forward projector can be implemented as follows: (8) compute Cholesky factor U if “ Ax ” needs to be computed for some x ? R p q ^ = Wx solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ , which yields “ Ax ” end . Similarly, in view of the symmetry of F , one can compute the back-projection on the fly as follows: (9) compute Cholesky factor U if “ A ? y ” needs to be computed for some y ? R N ? ^ = D ? y solve U ? U q ^ = ? ^ for q ^ by forward and back-substitution x ^ = W ? q ^ , which yield “ A ? y ” end . Every time we compute a forward or back-projection in the on-the-fly approach, we need to perform the substitution step only once. 2.4.3. Comparison In the direct calculation approach, it is very expensive to precompute the full system matrix A ? R N × p because the forward problem must be solved p times where p is the number of source voxels. However, once the matrix has been precomputed, the F/B projection can be computed rapidly as a single matrix vector product. On the other hand, in the on-the-fly approach, whenever one needs to compute the forward or back-projection, one must solve the forward problem in ( 3 ), which is more expensive than computing the product of the system matrix and a vector, partly because the FEM matrix F ? R v × v is larger than the system matrix A ? R N × p (typically, N < v and p < v ) despite the sparsity of F . For example, in the multispectral bioluminescence tomography system we consider in section 4, v 2 was larger than Np by a factor of >150. Which of the direct calculation and on-the-fly approaches should be used? This depends on how many images are to be reconstructed for a given forward model, how many iterations per image are required for practical convergence of an iterative algorithm and how many F/B projections per iteration are needed. If one wishes to investigate many different iterative algorithms and regularization schemes for a given subject or if one assumes that the animal's anatomical structure and position do not change much over time in a longitudinal study, the direct calculation approach will be beneficial because once the expensive precomputation is performed, subsequent forward solutions can be rapidly computed by matrix vector multiplication. In contrast, if one is to compute relatively few images with a small number of iterations, the on-the-fly approach should be chosen. Comparisons of image reconstruction times for different approaches using real mouse bioluminescent data are made in section 4. One of the advantages of the direct calculation approach is that direct access to any column or row in the system matrix is possible when the matrix is precomputed. Column or row access is required by certain iterative algorithms such as coordinate descent and incremental gradient algorithms, as discussed in section 3. Furthermore, the direct calculation approach may be more readily parallelizable than the on-the-fly approach. Interestingly, the on-the-fly approach is closely connected with the adjoint or reverse differentiation method which is widely used for computing the gradient of a cost function in nonlinear diffuse optical tomography ( Davies et al 1997 , Hielscher et al 1999 ), as discussed in the appendix .  2.4.1. Direct calculation approach In the direct calculation approach, one precomputes the full system matrix A and implements the F/B projector by a matrix-vector multiplication. Each column of the system matrix is computed by taking a unit vector corresponding to each source location as follows: (7) compute Cholesky factor U for j = 1 , … , p q ^ = We j solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ store y ^ as the j th column of A end where e j = [ 0 , … , 0 , 1 , 0 , … , 0 ] ? ? R p is the j th unit vector with only the j th entry being 1. The substitution step in ( 7 ) must be repeated for a total of p source locations. However, once A is precomputed, F/B projection can be implemented as direct matrix-vector multiplications, Ax and A ? y .  2.4.2. On-the-fly approach An alternative that avoids precomputing the full system matrix, is to calculate the F/B projection in an on-the-fly manner. The on-the-fly forward projector can be implemented as follows: (8) compute Cholesky factor U if “ Ax ” needs to be computed for some x ? R p q ^ = Wx solve U ? U ? ^ = q ^ for ? ^ by forward and back-substitution y ^ = D ? ^ , which yields “ Ax ” end . Similarly, in view of the symmetry of F , one can compute the back-projection on the fly as follows: (9) compute Cholesky factor U if “ A ? y ” needs to be computed for some y ? R N ? ^ = D ? y solve U ? U q ^ = ? ^ for q ^ by forward and back-substitution x ^ = W ? q ^ , which yield “ A ? y ” end . Every time we compute a forward or back-projection in the on-the-fly approach, we need to perform the substitution step only once.  2.4.3. Comparison In the direct calculation approach, it is very expensive to precompute the full system matrix A ? R N × p because the forward problem must be solved p times where p is the number of source voxels. However, once the matrix has been precomputed, the F/B projection can be computed rapidly as a single matrix vector product. On the other hand, in the on-the-fly approach, whenever one needs to compute the forward or back-projection, one must solve the forward problem in ( 3 ), which is more expensive than computing the product of the system matrix and a vector, partly because the FEM matrix F ? R v × v is larger than the system matrix A ? R N × p (typically, N < v and p < v ) despite the sparsity of F . For example, in the multispectral bioluminescence tomography system we consider in section 4, v 2 was larger than Np by a factor of >150. Which of the direct calculation and on-the-fly approaches should be used? This depends on how many images are to be reconstructed for a given forward model, how many iterations per image are required for practical convergence of an iterative algorithm and how many F/B projections per iteration are needed. If one wishes to investigate many different iterative algorithms and regularization schemes for a given subject or if one assumes that the animal's anatomical structure and position do not change much over time in a longitudinal study, the direct calculation approach will be beneficial because once the expensive precomputation is performed, subsequent forward solutions can be rapidly computed by matrix vector multiplication. In contrast, if one is to compute relatively few images with a small number of iterations, the on-the-fly approach should be chosen. Comparisons of image reconstruction times for different approaches using real mouse bioluminescent data are made in section 4. One of the advantages of the direct calculation approach is that direct access to any column or row in the system matrix is possible when the matrix is precomputed. Column or row access is required by certain iterative algorithms such as coordinate descent and incremental gradient algorithms, as discussed in section 3. Furthermore, the direct calculation approach may be more readily parallelizable than the on-the-fly approach. Interestingly, the on-the-fly approach is closely connected with the adjoint or reverse differentiation method which is widely used for computing the gradient of a cost function in nonlinear diffuse optical tomography ( Davies et al 1997 , Hielscher et al 1999 ), as discussed in the appendix .  2.5. Multispectral system model In multispectral imaging with K spectral bins, a multispectral system matrix A mul is constructed by concatenating monochromatic system matrices A 1(?1), …, A K (? K ) for wavelengths ?1, …, ? K ( Chaudhari et al 2005 ): (10) y mul = A mul x where A mul = [ s 1 A 1 ( ? 1 ) ? … s K A K ( ? K ) ? ] ? ? R N K × p with s 1, …, s K being known emission spectra of the bioluminescent source, and y mul = [ y 1 ( ? 1 ) ? … y K ( ? K ) ? ] ? ? R N K the multispectral data. It is straightforward to apply the F/B projection methods discussed in section 2.4 by combining individual F/B projections for each wavelength. That is, the forward projection is given by y mul = A mul x = [ s 1 ( A 1 ( ? 1 ) x ) ? … s K ( A K ( ? K ) x ) ? ] ? and the back-projection x = A ? mul y mul = ? k = 1 K s k A k ( ? k ) ? y k ( ? k ) . Henceforth, we omit the subscript in A mul and y mul for notational simplicity and consider both monochromatic and multispectral systems in a common framework.  3. Inverse problem The ultimate goal of bioluminescence tomography is to estimate the source x from the photon flux measurement y based on the system model ( 6 ) or ( 10 ). In this section we formulate the inverse problem and discuss iterative algorithms for the inverse solution. 3.1. Regularized least squares (RLS) We focus on the following regularized least squares (RLS) estimate: (11) x ^ = arg min x ? 0 ? ( x ) , ? ( x ) = L ( x ) + ? R ( x ) . The nonnegativity constraint, x ? 0 , is imposed because the source strength is physically nonnegative ( Jiang and Wang 2004 , Jiang et al 2007 ). Although the problem size can be reduced by allowing sources only in a permissible region ( Cong et al 2004 , 2005 , Jiang and Wang 2004 , Wang et al 2006a , 2006b , Cong and Wang 2006 , Jiang et al 2007 , Lv et al 2007 ), we do not consider such constraints here. In ( 11 ), L is the data-fit function (12) L ( x ) = 1 2 ? y ? A x ? 2 , ? is a regularization parameter, and R is a quadratic regularization function such that (13) R ( x ) = 1 2 x ? R x for a nonnegative definite matrix R ? R p × p . The regularizer R stabilizes the noise in reconstructed images yet usually introduces bias. In a Bayesian framework, if the source and the additive noise are assumed to be zero-mean Gaussian with covariance of K and ?2 I , respectively, then the RLS estimate x ^ obtained with choosing ? = ?2 and R = K ?1 can be viewed as a maximum a posteriori (MAP) estimate, where I is the identity matrix ( Tarantola 2005 , chapter 3). Other noise models such as the shot-noise model ( Ye et al 1999 ) can also be used; other types of regularization functions include generalized Gaussian Markov random field priors ( Bouman and Sauer 1993 ), non-Gaussian Gibbs priors ( Hebert and Leahy 1989 ), which have an edge-preserving property, and L1-norm regularizers, which are known to produce sparse reconstruction ( Cao et al 2007 ). We focus here on the quadratic cost function using ( 12 ) and ( 13 ). However, we note that the optimization methods described below are general, and can, with certain restrictions on differentiability and continuity, be used in conjunction with the nonquadratic regularizers referred to above. We use a weighted L2-norm regularizer with a positive-definite diagonal matrix R ( Lin et al 2006 ). If R is the identity matrix, then R ( x ) becomes a conventional L2-norm regularizer ( Cong and Wang 2006 , Han et al 2006 , Wang et al 2006a , Lv et al 2006 , Han and Wang 2007 , Soloviev 2007 ). In this case, as the regularization parameter ? increases, the reconstructed image becomes smooth and less noisy. But the reconstructed sources also tend to become superficial, consequently increasing localization errors particularly for deep sources. This occurs because deep sources are highly attenuated so need larger amplitudes to match the surface photon flux from a superficial source; these large amplitudes are in turn penalized by the L2-norm. Quadratic weighting schemes have been proposed to reduce the preference for superficial sources in the related problem of cortical current imaging in EEG and MEG ( Lin et al 2006 , Baillet et al 2001 ). Nonquadratic penalties can also have attractive properties in this respect. We use the following sensitivity-dependent weight matrix, which through column normalization removes the increased penalty for deep sources, effectively removing the bias towards superficial ones: (14) R = diag j { ? j 2 } where diag{·} denotes a diagonal matrix, ? j is the sensitivity of the j th source location defined as (15) ? j = ? i a i j , and a ij are the entries in the system matrix A . One can assume ? j > 0 for all j without loss of generality since one can exclude any source x j with ? j = 0 from the parameter space. Note that ? j 's are readily calculated using an on-the-fly back-projection A ? 1 of uniform data 1 = [1, …, 1]?, without precomputing A . The cost function in ( 11 ) with a positive-definite matrix R is strictly convex and so there exists a unique solution x ^ of the nonnegativity-constrained optimization problem in ( 11 ) and a local minimum is also the global minimum ( Bertsekas 1999 , p 685). In the continuous domain, the uniqueness of a solution to the diffusion equation under certain conditions on the source is discussed in Wang et al (2004) for the unregularized case and in Han et al (2006) and Han and Wang (2007) for the regularized one. Most of the iterative algorithms that we study in section 3.2 use the gradient ?? of the cost function ?. The gradient is given by (16) ? ? ( x ) = ( A ? A + ? R ) x ? A ? y , calculation of which requires F/B projections. The Hessian H of the cost function is given by (17) H = ? 2 ? ( x ) = A ? A + ? R . The diagonal elements of the Hessian are used in certain preconditioners in section 3.2.5. The quality of the reconstructed image x ^ in ( 11 ), as reflected in source localization error, quantitation error, resolution and noise characteristics, is determined by the choice of cost function ? as well as the physical properties and limitations of the imaging system. Inaccuracies in the system model A will introduce a systematic bias into the image. An inaccurate noise model, for which we assume a zero-mean white Gaussian distribution, can lead to suboptimal noise characteristics as well as additional bias. Image quality will also be affected by choice of the regularization function R . However, we note that our goal here is to explore methods for solving the inverse problem once the cost function ? is specified, rather than to investigate the relative merits of different cost functions. In section 4 we use a single regularization function which produces reasonably good localization results for the data used in our evaluations. We then compare the convergence speeds of different reconstruction algorithms that all converge to the same optimal solution for the chosen cost. Since all algorithms converge to the same solution we do not present comparisons of image quality in this paper. 3.2. Iterative algorithms We will investigate iterative solutions to the optimization problem ( 11 ) since the nonnegativity constraint precludes an analytical solution. We choose four representative iterative algorithms: a gradient projection method (GPM), preconditioned conjugate gradient (PCG), coordinate descent (CD) and an incremental gradient algorithm, OS-SPS. We compare their convergence speeds using real mouse bioluminescent data in section 4. Those algorithms are described in detail below. A number of different algorithms for bioluminescence tomography have been used in the literature. The expectation maximization (EM) algorithm ( Dempster et al 1977 ) which was originally developed to compute a maximum likelihood solution for linear Poisson inverse problems ( Shepp and Vardi 1982 ) was used in Jiang and Wang (2004) , Alexandrakis et al (2005 , 2006 ) and Jiang et al (2007) . The EM algorithm maximizes an unregularized Poisson likelihood function and its convergence speed is generally very slow ( Meng and van Dyk 1997 , Qi and Leahy 2006 ) so we do not consider it in this paper. A deblurring variant of EM was used in Slavine et al (2006) . A regularized Newton's algorithm ( Gu et al 2004 ) and a modified Newton's method with an active set strategy ( Cong et al 2004 , 2005 , Lv et al 2006 ) have also been used. A major disadvantage of Newton's methods for constrained optimization is that it is computationally costly both to enforce constraints and guarantee convergence to the solution ( Bertsekas 1999 , p 231); for example, naive orthogonal projection onto the nonnegative constraint set does not necessarily guarantee a decrease in the cost function ( Bertsekas 1999 , p 245). A constrained Landweber method was used in Jiang and Wang (2004) , which requires nontrivial stepsize selection for both global convergence and optimal convergence rates. The nonnegativity constraint in ( 11 ) can be more easily enforced by the four algorithms (GPM, PCG, CD and OS-SPS) considered here than by using active set methods ( Cong et al 2004 , 2005 , Lv et al 2006 , 2007 ). In GPM and PCG, the nonnegativity constraint is enforced by orthogonal projection onto the constraint set and a subsequent bent line search ( Mumcuo?lu et al 1994 , Qi et al 1998 ); in CD and OS-SPS, the optimization problem is reduced to a set of one-dimensional constrained problems, which makes it trivial to enforce the constraint. All the algorithms are aimed at minimizing the common RLS cost function in ( 11 ) and converge to the optimal point, with the exception of OS-SPS which approaches an approximate solution as stated below. None of these algorithms has any free parameters, such as step size, so that we can directly compare convergence rates for solving the problem in ( 11 ). 3.2.1. Gradient projection method (GPM) A gradient projection method (GPM) is an iterative algorithm for constrained problems analogous to a gradient descent method for unconstrained problems ( Bertsekas 1999 , p 223). The gradient projection method we focus on here can be summarized as follows: for n = 0, 1, …, (18) g n = ? ? ( x ) n (19) d n = ? P n g n (20) ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n (21) if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (22) x n + 1 = x n + ? n d n where x n ? R p is the n th iterate, x 0 is an initial estimate, P n ? R p × p is a preconditioner, ?? and H are given in ( 16 ) and ( 17 ), respectively, and [·]+ denotesthe orthogonal projection onto the nonnegative orthant, that is, q = [ t ]+ implies q j = min( t j , 0) for all j . We describe preconditioners for this method in section 3.2.5. The above algorithm can be implemented by keeping Ax as a state vector so that GPM requires one F/B projection pair per iteration unless z n leaves the constraint set. Calculating the gradient in ( 18 ) and ( 16 ) requires one back-projection if Ax n is retained as a state vector, and calculating the denominator in ( 20 ) needs one forward projection since d n? Hd n = ? Ad n ?2 + ? d n ? Rd n . If [ z n ]+ ? z n , then an additional forward projection is needed in ( 22 ) because d n is updated in ( 21 ). 3.2.2. Preconditioned conjugate gradient (PCG) A PCG method is a general purpose optimization algorithm using a conjugate direction as a search direction ( Bertsekas 1999 , p 138). To enforce the nonnegativity constraint, we use a constrained version of PCG where the search direction is bent when an iterate leaves the constraint set, as in section 3.2.1 ( Mumcuo?lu et al 1994 , Qi et al 1998 ). The version we use can be summarized as follows: for n = 0, 1, …, g n = ? ? ( x n ) r n = P n g n ? n = { r n ? ( g n ? g n ? 1 ) r n ? 1 ? g n ? 1 , n > 1 0 , n = 0 } d n = ? r n + ? n d n ? 1 if d n ? g n > 0 , then d n = ? r n ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (23) x n + 1 = x n + ? n d n . The PCG method requires one F/B projection pair per iteration with an extra forward projection, depending on whether z n leaves the constraint set. 3.2.3. Coordinate descent (CD) A coordinate descent (CD) method is an iterative algorithm that minimizes the cost function along one coordinate direction for each update ( Bertsekas 1999 , p 267). The method has been successfully applied to image reconstruction applications ( Bouman and Sauer 1996 , Ye et al 1999 ). The CD method can be implemented as follows: (24) r = y ? Ax 0 for initial image x 0 (25) for n = 1 , 2 , … for j = 1 , … , p x j old = x j x j ? [ x j ? ( ? ? ( x ) ) ? e j H j j ] + r ? r + ( x j o l d ? x j ) A ? j end x n = x end where e j is the j th unit vector and A ·j is the j th column of A . In ( 24 ), the j th diagonal element H jj of the Hessian H in ( 17 ) is given by (26) H j j = ? j + ? R j j where (27) ? j = ? i a i j 2 , R j j = ? j 2 from ( 14 ), ( 15 ), and ( 17 ). The CD method requires access to each column A · j of the system matrix; therefore, the on-the-fly approach is not suitable for CD. 3.2.4. Incremental gradient methods Incremental gradient methods ( Bertsekas 1999 , p 108) or ordered subset (OS) algorithms ( Hudson and Larkin 1994 ) are optimization techniques which use only a subset of the data for each update with the aim of reducing the cost for computing gradients. Various incremental gradient methods have been successfully applied to PET / SPECT image reconstruction to accelerate convergence speeds ( Hudson and Larkin 1994 , Qi and Leahy 2006 ). The applicability of an OS algorithm to bioluminescence tomography was mentioned in Jiang and Wang (2004) . In this paper we consider an OS version of separable paraboloidal surrogates (OS-SPS) where a line search is not required ( Erdo?an and Fessler 1999 ). We derive the OS-SPS algorithm for our problem by following the derivation of additive updates in Erdo?an and Fessler (1999) : for n = 0 , 1 , 2 , … , x n , 1 = x n for m = 1 , 2 , … , M x n , m + 1 = [ x n , m ? MP SPS ? ? m ( x n , m ) ] + end x n + 1 = x n , M + 1 end where P SPS = diag j { 1 ? i a i j ? k a i k + ? R j j } and (28) ? m ( x ) = 1 2 ? i ? S m ( y i ? [ A x ] i ) 2 + 1 M R ( x ) for m = 1, …, M , with M being the number of subsets S m or subcost functions ? m . We choose the subsets S m in ( 28 ) by downsampling the data space as S m = { i : i ? m ? 1(mod M ), i =1, …, KN }. Generally, OS-SPS does not converge to an optimal solution of the problem in ( 11 ) but to a limit cycle of points which are only approximate solutions ( Ahn et al 2006 ). The OS type algorithms can be forced to converge to the optimal solution by using relaxation parameters ( Ahn and Fessler 2003 ), switching to a convergent algorithm at some point ( Li et al 2005 ), or rederiving the algorithm in a framework of incremental optimization transfer ( Ahn et al 2006 ). However, we use the original form of OS-SPS as outlined above for simplicity because it works well enough for our purposes, as shown in section 4. Since OS algorithms, including OS-SPS, require access to each row of the system matrix A to compute the gradient ?? m of the subcost function, the on-the-fly approach is not suitable. 3.2.5. Preconditioners For ill-conditioned problems, gradient based methods suffer from slow convergence but can be accelerated using preconditioning. Newton's method using the inverse of the Hessian as a preconditioner converges very quickly ( Bertsekas 1999 , p 26). However, calculating the Hessian inverse is impractical for large-scale problems. Here we consider only simple diagonal preconditioners. A popular choice is a diagonal approximation to Newton's method ( Bertsekas 1999 , p 27), (29) P N = diag j { 1 H j j } = diag j { 1 ? j + ? R j j } , with ? j and R jj defined in ( 27 ). Calculating the term ? j = ? i a i j 2 requires precomputation of the full system matrix A so the on-the-fly approach is not suitable for this preconditioner. However, one can estimate ? j from ? j 2 where ? j ? ? i a ij since ? j and ? j 2 are usually highly correlated and ? j can be readily computed on the fly by back-projecting a vector of ones as discussed in section 3.1. Figure 1 shows an example scatter plot of ? j versus ? j 2 for the multispectral imaging system model from section 4. As shown in the figure, ? j and ? j 2 are strongly correlated (Pearson's correlation coefficient was 0.922). A practical strategy for estimating ? j , which works for the on-the-fly approach, is as follows. First, one computes ? j for j = 1, …, p by back-projection A ? 1 . Next, one randomly chooses T source locations j 1, …, j T , and calculates projections y ( t ) = Ae j t and subsequently ? j t = ? y ( t ) ? 2 = ? i a i j t 2 for t = 1, …, T . Then one estimates the slope ? in the scatter plot by fitting a line ? = ??2 to the T samples { ( ? j t 2 , ? j t ) } t = 1 T . Finally, one approximates ? j as ? ^ j = ? ^ ? j 2 using the slope estimate ? ^ for j = 1, …, p . We refer to the resulting preconditioner as the estimated diagonal approximation to Newton's (EN) method, (30) P EN = diag j { 1 ? ^ j + ? R j j } , which can be applied in both direct calculation and on-the-fly approaches. Calculating this preconditioner requires one back-projection for computing ? j 2 's and T forward projections for estimating ?. In section 4, we choose T = 10 source locations for estimating the slope ?. The coefficient of variation (CV) of ? ^ due to randomly choosing 10 source locations was about 29%, which is stable enough for our purposes (a very accurate estimate of ? is not necessary). For PCG we also consider an EM-type preconditioner as follows: (31) P EM ( x ) = diag j { x j + ? ? j } , where ? is a small positive number, e.g., ? = 10?3 max(1, max l x l ) > 0. This preconditioner originates from the EM algorithm for maximizing a Poisson likelihood ( Shepp and Vardi 1982 ) and has been used successfully to accelerate PCG algorithms for emission tomography ( Kaufman 1987 ). It has also been used with PCG for bioluminescence tomography ( Chaudhari et al 2005 ). 3.2.6. Comparison The on-the-fly approach is not suitable for CD and OS-SPS, as discussed above, since they require access to each row or column of the system matrix; neither is it suitable for the diagonally approximated Newton preconditioner P N in ( 29 ) since calculating ? j needs the precomputed full system matrix. For GPM and PCG, using P EN in ( 30 ) or P EMin ( 31 ), both direct calculation and on-the-fly approaches can be applied. The computation cost for the iterative algorithms is dominated by F/B projections. Therefore, the computational complexity of each algorithm can be represented by the number of F/B projections required per iteration. A forward or back-projection amounts to matrix-vector multiplication for the direct calculation approach, and to one FEM forward solution for the on-the-fly approach. GPM and PCG require one F/B projection pair plus possibly an extra forward projection. The cost per iteration for CD and OS-SPS equals roughly that of one F/B projection pair. Therefore, the computation costs per iteration for those algorithms considered here do not differ substantially once the F/B projection implementation approach is chosen. Other important factors are the cost for precomputation and the convergence speed, that is, how many iterations are required for practical convergence. Memory requirements for the iterative algorithms are modest. During iterations, one needs a memory space O (p + KN ) proportional to the image size p and to the data size KN . Additionally, for the direct calculation approach, one needs to store the KN × p system matrix. On the other hand, for the on-the-fly approach, one must retain the v × v FEM matrix, which is sparse and has about 10 v nonzero elements, for each wavelength if PCG is used for the forward problem solver; and one needs to keep the v × v Cholesky factor, which is a sparse upper triangular matrix, for each wavelength if CFS is chosen for the FEM solver. The cost of all of these methods is dominated by the cost of (possibly partial) forward and back-projections. Since these operations all involve large matrix-vector multiplications for the direct calculation approach, parallelization using multithreading or other methods is relatively straightforward. Among the four methods, the full gradient methods (PCG and GPM) in conjunction with the direct calculation approach benefit most readily since the full forward and back-projection operations can be easily distributed across processors. The CD method which updates one variable at a time will benefit least from parallelization.  3.1. Regularized least squares (RLS) We focus on the following regularized least squares (RLS) estimate: (11) x ^ = arg min x ? 0 ? ( x ) , ? ( x ) = L ( x ) + ? R ( x ) . The nonnegativity constraint, x ? 0 , is imposed because the source strength is physically nonnegative ( Jiang and Wang 2004 , Jiang et al 2007 ). Although the problem size can be reduced by allowing sources only in a permissible region ( Cong et al 2004 , 2005 , Jiang and Wang 2004 , Wang et al 2006a , 2006b , Cong and Wang 2006 , Jiang et al 2007 , Lv et al 2007 ), we do not consider such constraints here. In ( 11 ), L is the data-fit function (12) L ( x ) = 1 2 ? y ? A x ? 2 , ? is a regularization parameter, and R is a quadratic regularization function such that (13) R ( x ) = 1 2 x ? R x for a nonnegative definite matrix R ? R p × p . The regularizer R stabilizes the noise in reconstructed images yet usually introduces bias. In a Bayesian framework, if the source and the additive noise are assumed to be zero-mean Gaussian with covariance of K and ?2 I , respectively, then the RLS estimate x ^ obtained with choosing ? = ?2 and R = K ?1 can be viewed as a maximum a posteriori (MAP) estimate, where I is the identity matrix ( Tarantola 2005 , chapter 3). Other noise models such as the shot-noise model ( Ye et al 1999 ) can also be used; other types of regularization functions include generalized Gaussian Markov random field priors ( Bouman and Sauer 1993 ), non-Gaussian Gibbs priors ( Hebert and Leahy 1989 ), which have an edge-preserving property, and L1-norm regularizers, which are known to produce sparse reconstruction ( Cao et al 2007 ). We focus here on the quadratic cost function using ( 12 ) and ( 13 ). However, we note that the optimization methods described below are general, and can, with certain restrictions on differentiability and continuity, be used in conjunction with the nonquadratic regularizers referred to above. We use a weighted L2-norm regularizer with a positive-definite diagonal matrix R ( Lin et al 2006 ). If R is the identity matrix, then R ( x ) becomes a conventional L2-norm regularizer ( Cong and Wang 2006 , Han et al 2006 , Wang et al 2006a , Lv et al 2006 , Han and Wang 2007 , Soloviev 2007 ). In this case, as the regularization parameter ? increases, the reconstructed image becomes smooth and less noisy. But the reconstructed sources also tend to become superficial, consequently increasing localization errors particularly for deep sources. This occurs because deep sources are highly attenuated so need larger amplitudes to match the surface photon flux from a superficial source; these large amplitudes are in turn penalized by the L2-norm. Quadratic weighting schemes have been proposed to reduce the preference for superficial sources in the related problem of cortical current imaging in EEG and MEG ( Lin et al 2006 , Baillet et al 2001 ). Nonquadratic penalties can also have attractive properties in this respect. We use the following sensitivity-dependent weight matrix, which through column normalization removes the increased penalty for deep sources, effectively removing the bias towards superficial ones: (14) R = diag j { ? j 2 } where diag{·} denotes a diagonal matrix, ? j is the sensitivity of the j th source location defined as (15) ? j = ? i a i j , and a ij are the entries in the system matrix A . One can assume ? j > 0 for all j without loss of generality since one can exclude any source x j with ? j = 0 from the parameter space. Note that ? j 's are readily calculated using an on-the-fly back-projection A ? 1 of uniform data 1 = [1, …, 1]?, without precomputing A . The cost function in ( 11 ) with a positive-definite matrix R is strictly convex and so there exists a unique solution x ^ of the nonnegativity-constrained optimization problem in ( 11 ) and a local minimum is also the global minimum ( Bertsekas 1999 , p 685). In the continuous domain, the uniqueness of a solution to the diffusion equation under certain conditions on the source is discussed in Wang et al (2004) for the unregularized case and in Han et al (2006) and Han and Wang (2007) for the regularized one. Most of the iterative algorithms that we study in section 3.2 use the gradient ?? of the cost function ?. The gradient is given by (16) ? ? ( x ) = ( A ? A + ? R ) x ? A ? y , calculation of which requires F/B projections. The Hessian H of the cost function is given by (17) H = ? 2 ? ( x ) = A ? A + ? R . The diagonal elements of the Hessian are used in certain preconditioners in section 3.2.5. The quality of the reconstructed image x ^ in ( 11 ), as reflected in source localization error, quantitation error, resolution and noise characteristics, is determined by the choice of cost function ? as well as the physical properties and limitations of the imaging system. Inaccuracies in the system model A will introduce a systematic bias into the image. An inaccurate noise model, for which we assume a zero-mean white Gaussian distribution, can lead to suboptimal noise characteristics as well as additional bias. Image quality will also be affected by choice of the regularization function R . However, we note that our goal here is to explore methods for solving the inverse problem once the cost function ? is specified, rather than to investigate the relative merits of different cost functions. In section 4 we use a single regularization function which produces reasonably good localization results for the data used in our evaluations. We then compare the convergence speeds of different reconstruction algorithms that all converge to the same optimal solution for the chosen cost. Since all algorithms converge to the same solution we do not present comparisons of image quality in this paper.  3.2. Iterative algorithms We will investigate iterative solutions to the optimization problem ( 11 ) since the nonnegativity constraint precludes an analytical solution. We choose four representative iterative algorithms: a gradient projection method (GPM), preconditioned conjugate gradient (PCG), coordinate descent (CD) and an incremental gradient algorithm, OS-SPS. We compare their convergence speeds using real mouse bioluminescent data in section 4. Those algorithms are described in detail below. A number of different algorithms for bioluminescence tomography have been used in the literature. The expectation maximization (EM) algorithm ( Dempster et al 1977 ) which was originally developed to compute a maximum likelihood solution for linear Poisson inverse problems ( Shepp and Vardi 1982 ) was used in Jiang and Wang (2004) , Alexandrakis et al (2005 , 2006 ) and Jiang et al (2007) . The EM algorithm maximizes an unregularized Poisson likelihood function and its convergence speed is generally very slow ( Meng and van Dyk 1997 , Qi and Leahy 2006 ) so we do not consider it in this paper. A deblurring variant of EM was used in Slavine et al (2006) . A regularized Newton's algorithm ( Gu et al 2004 ) and a modified Newton's method with an active set strategy ( Cong et al 2004 , 2005 , Lv et al 2006 ) have also been used. A major disadvantage of Newton's methods for constrained optimization is that it is computationally costly both to enforce constraints and guarantee convergence to the solution ( Bertsekas 1999 , p 231); for example, naive orthogonal projection onto the nonnegative constraint set does not necessarily guarantee a decrease in the cost function ( Bertsekas 1999 , p 245). A constrained Landweber method was used in Jiang and Wang (2004) , which requires nontrivial stepsize selection for both global convergence and optimal convergence rates. The nonnegativity constraint in ( 11 ) can be more easily enforced by the four algorithms (GPM, PCG, CD and OS-SPS) considered here than by using active set methods ( Cong et al 2004 , 2005 , Lv et al 2006 , 2007 ). In GPM and PCG, the nonnegativity constraint is enforced by orthogonal projection onto the constraint set and a subsequent bent line search ( Mumcuo?lu et al 1994 , Qi et al 1998 ); in CD and OS-SPS, the optimization problem is reduced to a set of one-dimensional constrained problems, which makes it trivial to enforce the constraint. All the algorithms are aimed at minimizing the common RLS cost function in ( 11 ) and converge to the optimal point, with the exception of OS-SPS which approaches an approximate solution as stated below. None of these algorithms has any free parameters, such as step size, so that we can directly compare convergence rates for solving the problem in ( 11 ). 3.2.1. Gradient projection method (GPM) A gradient projection method (GPM) is an iterative algorithm for constrained problems analogous to a gradient descent method for unconstrained problems ( Bertsekas 1999 , p 223). The gradient projection method we focus on here can be summarized as follows: for n = 0, 1, …, (18) g n = ? ? ( x ) n (19) d n = ? P n g n (20) ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n (21) if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (22) x n + 1 = x n + ? n d n where x n ? R p is the n th iterate, x 0 is an initial estimate, P n ? R p × p is a preconditioner, ?? and H are given in ( 16 ) and ( 17 ), respectively, and [·]+ denotesthe orthogonal projection onto the nonnegative orthant, that is, q = [ t ]+ implies q j = min( t j , 0) for all j . We describe preconditioners for this method in section 3.2.5. The above algorithm can be implemented by keeping Ax as a state vector so that GPM requires one F/B projection pair per iteration unless z n leaves the constraint set. Calculating the gradient in ( 18 ) and ( 16 ) requires one back-projection if Ax n is retained as a state vector, and calculating the denominator in ( 20 ) needs one forward projection since d n? Hd n = ? Ad n ?2 + ? d n ? Rd n . If [ z n ]+ ? z n , then an additional forward projection is needed in ( 22 ) because d n is updated in ( 21 ). 3.2.2. Preconditioned conjugate gradient (PCG) A PCG method is a general purpose optimization algorithm using a conjugate direction as a search direction ( Bertsekas 1999 , p 138). To enforce the nonnegativity constraint, we use a constrained version of PCG where the search direction is bent when an iterate leaves the constraint set, as in section 3.2.1 ( Mumcuo?lu et al 1994 , Qi et al 1998 ). The version we use can be summarized as follows: for n = 0, 1, …, g n = ? ? ( x n ) r n = P n g n ? n = { r n ? ( g n ? g n ? 1 ) r n ? 1 ? g n ? 1 , n > 1 0 , n = 0 } d n = ? r n + ? n d n ? 1 if d n ? g n > 0 , then d n = ? r n ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (23) x n + 1 = x n + ? n d n . The PCG method requires one F/B projection pair per iteration with an extra forward projection, depending on whether z n leaves the constraint set. 3.2.3. Coordinate descent (CD) A coordinate descent (CD) method is an iterative algorithm that minimizes the cost function along one coordinate direction for each update ( Bertsekas 1999 , p 267). The method has been successfully applied to image reconstruction applications ( Bouman and Sauer 1996 , Ye et al 1999 ). The CD method can be implemented as follows: (24) r = y ? Ax 0 for initial image x 0 (25) for n = 1 , 2 , … for j = 1 , … , p x j old = x j x j ? [ x j ? ( ? ? ( x ) ) ? e j H j j ] + r ? r + ( x j o l d ? x j ) A ? j end x n = x end where e j is the j th unit vector and A ·j is the j th column of A . In ( 24 ), the j th diagonal element H jj of the Hessian H in ( 17 ) is given by (26) H j j = ? j + ? R j j where (27) ? j = ? i a i j 2 , R j j = ? j 2 from ( 14 ), ( 15 ), and ( 17 ). The CD method requires access to each column A · j of the system matrix; therefore, the on-the-fly approach is not suitable for CD. 3.2.4. Incremental gradient methods Incremental gradient methods ( Bertsekas 1999 , p 108) or ordered subset (OS) algorithms ( Hudson and Larkin 1994 ) are optimization techniques which use only a subset of the data for each update with the aim of reducing the cost for computing gradients. Various incremental gradient methods have been successfully applied to PET / SPECT image reconstruction to accelerate convergence speeds ( Hudson and Larkin 1994 , Qi and Leahy 2006 ). The applicability of an OS algorithm to bioluminescence tomography was mentioned in Jiang and Wang (2004) . In this paper we consider an OS version of separable paraboloidal surrogates (OS-SPS) where a line search is not required ( Erdo?an and Fessler 1999 ). We derive the OS-SPS algorithm for our problem by following the derivation of additive updates in Erdo?an and Fessler (1999) : for n = 0 , 1 , 2 , … , x n , 1 = x n for m = 1 , 2 , … , M x n , m + 1 = [ x n , m ? MP SPS ? ? m ( x n , m ) ] + end x n + 1 = x n , M + 1 end where P SPS = diag j { 1 ? i a i j ? k a i k + ? R j j } and (28) ? m ( x ) = 1 2 ? i ? S m ( y i ? [ A x ] i ) 2 + 1 M R ( x ) for m = 1, …, M , with M being the number of subsets S m or subcost functions ? m . We choose the subsets S m in ( 28 ) by downsampling the data space as S m = { i : i ? m ? 1(mod M ), i =1, …, KN }. Generally, OS-SPS does not converge to an optimal solution of the problem in ( 11 ) but to a limit cycle of points which are only approximate solutions ( Ahn et al 2006 ). The OS type algorithms can be forced to converge to the optimal solution by using relaxation parameters ( Ahn and Fessler 2003 ), switching to a convergent algorithm at some point ( Li et al 2005 ), or rederiving the algorithm in a framework of incremental optimization transfer ( Ahn et al 2006 ). However, we use the original form of OS-SPS as outlined above for simplicity because it works well enough for our purposes, as shown in section 4. Since OS algorithms, including OS-SPS, require access to each row of the system matrix A to compute the gradient ?? m of the subcost function, the on-the-fly approach is not suitable. 3.2.5. Preconditioners For ill-conditioned problems, gradient based methods suffer from slow convergence but can be accelerated using preconditioning. Newton's method using the inverse of the Hessian as a preconditioner converges very quickly ( Bertsekas 1999 , p 26). However, calculating the Hessian inverse is impractical for large-scale problems. Here we consider only simple diagonal preconditioners. A popular choice is a diagonal approximation to Newton's method ( Bertsekas 1999 , p 27), (29) P N = diag j { 1 H j j } = diag j { 1 ? j + ? R j j } , with ? j and R jj defined in ( 27 ). Calculating the term ? j = ? i a i j 2 requires precomputation of the full system matrix A so the on-the-fly approach is not suitable for this preconditioner. However, one can estimate ? j from ? j 2 where ? j ? ? i a ij since ? j and ? j 2 are usually highly correlated and ? j can be readily computed on the fly by back-projecting a vector of ones as discussed in section 3.1. Figure 1 shows an example scatter plot of ? j versus ? j 2 for the multispectral imaging system model from section 4. As shown in the figure, ? j and ? j 2 are strongly correlated (Pearson's correlation coefficient was 0.922). A practical strategy for estimating ? j , which works for the on-the-fly approach, is as follows. First, one computes ? j for j = 1, …, p by back-projection A ? 1 . Next, one randomly chooses T source locations j 1, …, j T , and calculates projections y ( t ) = Ae j t and subsequently ? j t = ? y ( t ) ? 2 = ? i a i j t 2 for t = 1, …, T . Then one estimates the slope ? in the scatter plot by fitting a line ? = ??2 to the T samples { ( ? j t 2 , ? j t ) } t = 1 T . Finally, one approximates ? j as ? ^ j = ? ^ ? j 2 using the slope estimate ? ^ for j = 1, …, p . We refer to the resulting preconditioner as the estimated diagonal approximation to Newton's (EN) method, (30) P EN = diag j { 1 ? ^ j + ? R j j } , which can be applied in both direct calculation and on-the-fly approaches. Calculating this preconditioner requires one back-projection for computing ? j 2 's and T forward projections for estimating ?. In section 4, we choose T = 10 source locations for estimating the slope ?. The coefficient of variation (CV) of ? ^ due to randomly choosing 10 source locations was about 29%, which is stable enough for our purposes (a very accurate estimate of ? is not necessary). For PCG we also consider an EM-type preconditioner as follows: (31) P EM ( x ) = diag j { x j + ? ? j } , where ? is a small positive number, e.g., ? = 10?3 max(1, max l x l ) > 0. This preconditioner originates from the EM algorithm for maximizing a Poisson likelihood ( Shepp and Vardi 1982 ) and has been used successfully to accelerate PCG algorithms for emission tomography ( Kaufman 1987 ). It has also been used with PCG for bioluminescence tomography ( Chaudhari et al 2005 ). 3.2.6. Comparison The on-the-fly approach is not suitable for CD and OS-SPS, as discussed above, since they require access to each row or column of the system matrix; neither is it suitable for the diagonally approximated Newton preconditioner P N in ( 29 ) since calculating ? j needs the precomputed full system matrix. For GPM and PCG, using P EN in ( 30 ) or P EMin ( 31 ), both direct calculation and on-the-fly approaches can be applied. The computation cost for the iterative algorithms is dominated by F/B projections. Therefore, the computational complexity of each algorithm can be represented by the number of F/B projections required per iteration. A forward or back-projection amounts to matrix-vector multiplication for the direct calculation approach, and to one FEM forward solution for the on-the-fly approach. GPM and PCG require one F/B projection pair plus possibly an extra forward projection. The cost per iteration for CD and OS-SPS equals roughly that of one F/B projection pair. Therefore, the computation costs per iteration for those algorithms considered here do not differ substantially once the F/B projection implementation approach is chosen. Other important factors are the cost for precomputation and the convergence speed, that is, how many iterations are required for practical convergence. Memory requirements for the iterative algorithms are modest. During iterations, one needs a memory space O (p + KN ) proportional to the image size p and to the data size KN . Additionally, for the direct calculation approach, one needs to store the KN × p system matrix. On the other hand, for the on-the-fly approach, one must retain the v × v FEM matrix, which is sparse and has about 10 v nonzero elements, for each wavelength if PCG is used for the forward problem solver; and one needs to keep the v × v Cholesky factor, which is a sparse upper triangular matrix, for each wavelength if CFS is chosen for the FEM solver. The cost of all of these methods is dominated by the cost of (possibly partial) forward and back-projections. Since these operations all involve large matrix-vector multiplications for the direct calculation approach, parallelization using multithreading or other methods is relatively straightforward. Among the four methods, the full gradient methods (PCG and GPM) in conjunction with the direct calculation approach benefit most readily since the full forward and back-projection operations can be easily distributed across processors. The CD method which updates one variable at a time will benefit least from parallelization.  3.2.1. Gradient projection method (GPM) A gradient projection method (GPM) is an iterative algorithm for constrained problems analogous to a gradient descent method for unconstrained problems ( Bertsekas 1999 , p 223). The gradient projection method we focus on here can be summarized as follows: for n = 0, 1, …, (18) g n = ? ? ( x ) n (19) d n = ? P n g n (20) ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n (21) if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (22) x n + 1 = x n + ? n d n where x n ? R p is the n th iterate, x 0 is an initial estimate, P n ? R p × p is a preconditioner, ?? and H are given in ( 16 ) and ( 17 ), respectively, and [·]+ denotesthe orthogonal projection onto the nonnegative orthant, that is, q = [ t ]+ implies q j = min( t j , 0) for all j . We describe preconditioners for this method in section 3.2.5. The above algorithm can be implemented by keeping Ax as a state vector so that GPM requires one F/B projection pair per iteration unless z n leaves the constraint set. Calculating the gradient in ( 18 ) and ( 16 ) requires one back-projection if Ax n is retained as a state vector, and calculating the denominator in ( 20 ) needs one forward projection since d n? Hd n = ? Ad n ?2 + ? d n ? Rd n . If [ z n ]+ ? z n , then an additional forward projection is needed in ( 22 ) because d n is updated in ( 21 ).  3.2.2. Preconditioned conjugate gradient (PCG) A PCG method is a general purpose optimization algorithm using a conjugate direction as a search direction ( Bertsekas 1999 , p 138). To enforce the nonnegativity constraint, we use a constrained version of PCG where the search direction is bent when an iterate leaves the constraint set, as in section 3.2.1 ( Mumcuo?lu et al 1994 , Qi et al 1998 ). The version we use can be summarized as follows: for n = 0, 1, …, g n = ? ? ( x n ) r n = P n g n ? n = { r n ? ( g n ? g n ? 1 ) r n ? 1 ? g n ? 1 , n > 1 0 , n = 0 } d n = ? r n + ? n d n ? 1 if d n ? g n > 0 , then d n = ? r n ? n = arg min ? ? ( x n + ? d n ) = ? d n ? g n d n ? H d n z n = x n + ? n d n if [ z n ] + ? z n d n = [ z n ] + ? x n ? n = arg min ? ? 1 ? ( x n + ? d n ) = min ( ? d n ? g n d n ? H d n , 1 ) end (23) x n + 1 = x n + ? n d n . The PCG method requires one F/B projection pair per iteration with an extra forward projection, depending on whether z n leaves the constraint set.  3.2.3. Coordinate descent (CD) A coordinate descent (CD) method is an iterative algorithm that minimizes the cost function along one coordinate direction for each update ( Bertsekas 1999 , p 267). The method has been successfully applied to image reconstruction applications ( Bouman and Sauer 1996 , Ye et al 1999 ). The CD method can be implemented as follows: (24) r = y ? Ax 0 for initial image x 0 (25) for n = 1 , 2 , … for j = 1 , … , p x j old = x j x j ? [ x j ? ( ? ? ( x ) ) ? e j H j j ] + r ? r + ( x j o l d ? x j ) A ? j end x n = x end where e j is the j th unit vector and A ·j is the j th column of A . In ( 24 ), the j th diagonal element H jj of the Hessian H in ( 17 ) is given by (26) H j j = ? j + ? R j j where (27) ? j = ? i a i j 2 , R j j = ? j 2 from ( 14 ), ( 15 ), and ( 17 ). The CD method requires access to each column A · j of the system matrix; therefore, the on-the-fly approach is not suitable for CD.  3.2.4. Incremental gradient methods Incremental gradient methods ( Bertsekas 1999 , p 108) or ordered subset (OS) algorithms ( Hudson and Larkin 1994 ) are optimization techniques which use only a subset of the data for each update with the aim of reducing the cost for computing gradients. Various incremental gradient methods have been successfully applied to PET / SPECT image reconstruction to accelerate convergence speeds ( Hudson and Larkin 1994 , Qi and Leahy 2006 ). The applicability of an OS algorithm to bioluminescence tomography was mentioned in Jiang and Wang (2004) . In this paper we consider an OS version of separable paraboloidal surrogates (OS-SPS) where a line search is not required ( Erdo?an and Fessler 1999 ). We derive the OS-SPS algorithm for our problem by following the derivation of additive updates in Erdo?an and Fessler (1999) : for n = 0 , 1 , 2 , … , x n , 1 = x n for m = 1 , 2 , … , M x n , m + 1 = [ x n , m ? MP SPS ? ? m ( x n , m ) ] + end x n + 1 = x n , M + 1 end where P SPS = diag j { 1 ? i a i j ? k a i k + ? R j j } and (28) ? m ( x ) = 1 2 ? i ? S m ( y i ? [ A x ] i ) 2 + 1 M R ( x ) for m = 1, …, M , with M being the number of subsets S m or subcost functions ? m . We choose the subsets S m in ( 28 ) by downsampling the data space as S m = { i : i ? m ? 1(mod M ), i =1, …, KN }. Generally, OS-SPS does not converge to an optimal solution of the problem in ( 11 ) but to a limit cycle of points which are only approximate solutions ( Ahn et al 2006 ). The OS type algorithms can be forced to converge to the optimal solution by using relaxation parameters ( Ahn and Fessler 2003 ), switching to a convergent algorithm at some point ( Li et al 2005 ), or rederiving the algorithm in a framework of incremental optimization transfer ( Ahn et al 2006 ). However, we use the original form of OS-SPS as outlined above for simplicity because it works well enough for our purposes, as shown in section 4. Since OS algorithms, including OS-SPS, require access to each row of the system matrix A to compute the gradient ?? m of the subcost function, the on-the-fly approach is not suitable.  3.2.5. Preconditioners For ill-conditioned problems, gradient based methods suffer from slow convergence but can be accelerated using preconditioning. Newton's method using the inverse of the Hessian as a preconditioner converges very quickly ( Bertsekas 1999 , p 26). However, calculating the Hessian inverse is impractical for large-scale problems. Here we consider only simple diagonal preconditioners. A popular choice is a diagonal approximation to Newton's method ( Bertsekas 1999 , p 27), (29) P N = diag j { 1 H j j } = diag j { 1 ? j + ? R j j } , with ? j and R jj defined in ( 27 ). Calculating the term ? j = ? i a i j 2 requires precomputation of the full system matrix A so the on-the-fly approach is not suitable for this preconditioner. However, one can estimate ? j from ? j 2 where ? j ? ? i a ij since ? j and ? j 2 are usually highly correlated and ? j can be readily computed on the fly by back-projecting a vector of ones as discussed in section 3.1. Figure 1 shows an example scatter plot of ? j versus ? j 2 for the multispectral imaging system model from section 4. As shown in the figure, ? j and ? j 2 are strongly correlated (Pearson's correlation coefficient was 0.922). A practical strategy for estimating ? j , which works for the on-the-fly approach, is as follows. First, one computes ? j for j = 1, …, p by back-projection A ? 1 . Next, one randomly chooses T source locations j 1, …, j T , and calculates projections y ( t ) = Ae j t and subsequently ? j t = ? y ( t ) ? 2 = ? i a i j t 2 for t = 1, …, T . Then one estimates the slope ? in the scatter plot by fitting a line ? = ??2 to the T samples { ( ? j t 2 , ? j t ) } t = 1 T . Finally, one approximates ? j as ? ^ j = ? ^ ? j 2 using the slope estimate ? ^ for j = 1, …, p . We refer to the resulting preconditioner as the estimated diagonal approximation to Newton's (EN) method, (30) P EN = diag j { 1 ? ^ j + ? R j j } , which can be applied in both direct calculation and on-the-fly approaches. Calculating this preconditioner requires one back-projection for computing ? j 2 's and T forward projections for estimating ?. In section 4, we choose T = 10 source locations for estimating the slope ?. The coefficient of variation (CV) of ? ^ due to randomly choosing 10 source locations was about 29%, which is stable enough for our purposes (a very accurate estimate of ? is not necessary). For PCG we also consider an EM-type preconditioner as follows: (31) P EM ( x ) = diag j { x j + ? ? j } , where ? is a small positive number, e.g., ? = 10?3 max(1, max l x l ) > 0. This preconditioner originates from the EM algorithm for maximizing a Poisson likelihood ( Shepp and Vardi 1982 ) and has been used successfully to accelerate PCG algorithms for emission tomography ( Kaufman 1987 ). It has also been used with PCG for bioluminescence tomography ( Chaudhari et al 2005 ).  3.2.6. Comparison The on-the-fly approach is not suitable for CD and OS-SPS, as discussed above, since they require access to each row or column of the system matrix; neither is it suitable for the diagonally approximated Newton preconditioner P N in ( 29 ) since calculating ? j needs the precomputed full system matrix. For GPM and PCG, using P EN in ( 30 ) or P EMin ( 31 ), both direct calculation and on-the-fly approaches can be applied. The computation cost for the iterative algorithms is dominated by F/B projections. Therefore, the computational complexity of each algorithm can be represented by the number of F/B projections required per iteration. A forward or back-projection amounts to matrix-vector multiplication for the direct calculation approach, and to one FEM forward solution for the on-the-fly approach. GPM and PCG require one F/B projection pair plus possibly an extra forward projection. The cost per iteration for CD and OS-SPS equals roughly that of one F/B projection pair. Therefore, the computation costs per iteration for those algorithms considered here do not differ substantially once the F/B projection implementation approach is chosen. Other important factors are the cost for precomputation and the convergence speed, that is, how many iterations are required for practical convergence. Memory requirements for the iterative algorithms are modest. During iterations, one needs a memory space O (p + KN ) proportional to the image size p and to the data size KN . Additionally, for the direct calculation approach, one needs to store the KN × p system matrix. On the other hand, for the on-the-fly approach, one must retain the v × v FEM matrix, which is sparse and has about 10 v nonzero elements, for each wavelength if PCG is used for the forward problem solver; and one needs to keep the v × v Cholesky factor, which is a sparse upper triangular matrix, for each wavelength if CFS is chosen for the FEM solver. The cost of all of these methods is dominated by the cost of (possibly partial) forward and back-projections. Since these operations all involve large matrix-vector multiplications for the direct calculation approach, parallelization using multithreading or other methods is relatively straightforward. Among the four methods, the full gradient methods (PCG and GPM) in conjunction with the direct calculation approach benefit most readily since the full forward and back-projection operations can be easily distributed across processors. The CD method which updates one variable at a time will benefit least from parallelization.  4. Results We applied iterative reconstruction methods to real bioluminescent data obtained from a mouse with a brain tumor implanted in the right cerebral hemisphere. Our goal is to compare computation cost for different reconstruction methods. All methods described should converge to the RLS image x ^ in ( 11 ) with the exception of OS-SPS which may enter a limit cycle as described above. Multispectral bioluminescent data were obtained from top and two side views by the IVIS 200 imaging system for wavelengths 580 nm, 600 nm, 620 nm and 640 nm. Since the tumor was near the dorsal surface of the head, no signal was detected from the bottom view and those three views (top and two sides) had sufficient information for localizing the source ( Chaudhari et al 2005 ). 3D CT scan data were used to segment the anatomical volume into skull and soft tissue and to assign standard optical properties ( Cheong et al 1990 ). The specific values of the absorption and reduced scattering coefficients for the skull and soft tissue we used can be found in Chaudhari et al (2005) . A regularization parameter value of ? = 0.05 was determined empirically. All image reconstructions were performed on an AMD Opteron 870 2.0 GHz computer. For all the iterative algorithms, a zero uniform image, 0 = [ 0 , … , 0 ] ? ? R p , was used as an initial estimate x 0. There is a trade-off between the number of FEM mesh nodes and the accuracy of FEM model; 105–106 nodes were recommended for sufficient accuracy in 3D problems ( Schweiger and Arridge 1997 ). Here we used a FEM-based forward model with v = 71256 tessellation nodes, a source space with p = 10814 source locations, and N = 3085 surface measurement nodes for K = 4 spectral bins. We did not attempt to investigate different FEM models in terms of accuracy and computation cost since our purpose is to compare the computation speeds of different reconstruction methods for a given model; however, we achieved reasonably good localization results within a reasonable time, as shown below. See Chaudhari et al (2005) for a study of the effects of the number of surface measurement points on reconstruction results. First, we precomputed the full system matrix A by using the mldivide function in Matlab (v7.3; Mathworks, MA, USA). We computed 500 columns of the system matrix at a time; that is, we prepared a 71256 × 500 matrix by combining 500 load vectors and solved the forward problem in ( 3 ) by putting the matrix on the right-hand side. It took 12 708 s to compute the full system matrix, as shown in table 1 . Then we reconstructed the bioluminescence image with the system matrix using the direct calculation approach. The iterative algorithms were implemented in the C language. Table 1 shows, in the columns named iteration cost, the computation time in seconds and the number of iterations, required for each algorithm to achieve the relative error E = ? x n ? x ^ ? ? ? x ^ ? < 10 % , < 5 % , <5%, and <1%, respectively, where the RLS image x ^ in ( 11 ) was estimated by 2000 iterations of GPM-N. Note that the relative error E is not with respect to the true source distribution which is unknown but with respect to the RLS solution x ^ given in ( 11 ). Therefore, E represents how close an image is to the converged solution and can be used for evaluating the convergence speed of an iterative algorithm. In the table, GPM-U denotes the unpreconditioned GPM; GPM-N and GPM-EN denote GPM with preconditioner P N and P EN given in ( 29 ) and ( 30 ), respectively; CG, PCG-N, PCG-EN, and PCG-EM denote PCG with no preconditioner and with preconditioners P N, P EN, and P EM, respectively; and OS-SPS-2, OS-SPS-5, and OS-SPS-10 denote OS-SPS with two, five and ten subsets, respectively. In table 1 , the total reconstruction time is the sum of the precomputation time and the iteration cost. We did not include the computation cost for constructing the FEM matrix F , which is common in all the methods. Next, we reconstructed images using the on-the-fly approach. For the on-the-fly approach, the precomputation time (110 s) in the table represents the time required for Cholesky factorization. As discussed in section 3, we excluded CD and OS-SPS, and preconditioner P N for this approach. For the on-the-fly approach we implemented the iterative algorithms in Matlab. For the on-the-fly F/B projectors, the one-time computation for Cholesky factorization was performed by the chol function, and repeated substitutions were carried out by the mldivide function in Matlab. As shown in table 1 , for both GPM and PCG, the use of preconditioners P N, P EN and P EM significantly accelerates convergence. In other words, the preconditioned algorithms (GPM-N, GPM-EN and PCG-N, PCG-EN, PCG-EM) require fewer iterations and lower iteration costs in seconds than the unpreconditioned ones (GPM-U and CG). Preconditioners P EN using the estimated diagonal elements of the Hessian and P N using the exact diagonals showed similar convergence speeds. In the direct calculation approach, CD and OS-SPS-10 were much faster than GPM and PCG. In fact, OS-SPS-10 showed the fastest convergence speed; it required only 8 and 15 iterations (11 and 20 s) for E < 10% and E < 5%, respectively. However, OS-SPS-10 (as well as OS-SPS-5) could not reach a point where ? < 1% because of the limit cycle behavior. This is a typical characteristic of incremental gradient or OS algorithms; as the number of subsets increases the initial convergence speed becomes faster but the limit point moves further from the optimal solution. Therefore, when it comes to the number of subsets for OS algorithms, there is a trade-off between convergence speed and accuracy. For ? < 1%, CD was fastest among the iterative algorithms for the direct calculation approach. On the other hand, for the on-the-fly approach, GPM-EN showed the fastest convergence rate for all three relative error cases, and PCG-EN showed a similar yet little slower convergence rate. The precomputation time (12 708 s) for the direct calculation approach is much larger than that (110 s) for the on-the-fly approach whereas the cost per iteration for the direct approach is smaller than that for the on-the-fly approach. If few images are to be reconstructed for a given system, then the on-the-fly approach can substantially reduce the total computation cost. For example, for the relative error <10%, GPM-EN, the fastest among the on-the-fly algorithms in total reconstruction time, is faster than OS-SPS-10, the fastest of the direct calculation algorithms, by a factor of 12719/798 ? 15.9; similarly, for < 5%, by a factor of 12728/1091 ? 11.7. For ? < 1%, GPM-EN in the on-the-fly approach is faster than CD in the direct calculation approach by a factor of 12773/1724 ? 7.4. As the number of required iterations increases, the factor of computational saving of the on-the-fly approach compared to the direct approach decreases. Figure 2 shows the RLS image x ^ and figure 3 shows an image reconstructed by 29 iterations, taking 798 s in total reconstruction time, of GPM-EN, which is the fastest among the on-the-fly algorithms. Although the relative error between the images is about 10%, they look similar and the localization is reasonably accurate.  4. Results We applied iterative reconstruction methods to real bioluminescent data obtained from a mouse with a brain tumor implanted in the right cerebral hemisphere. Our goal is to compare computation cost for different reconstruction methods. All methods described should converge to the RLS image x ^ in ( 11 ) with the exception of OS-SPS which may enter a limit cycle as described above. Multispectral bioluminescent data were obtained from top and two side views by the IVIS 200 imaging system for wavelengths 580 nm, 600 nm, 620 nm and 640 nm. Since the tumor was near the dorsal surface of the head, no signal was detected from the bottom view and those three views (top and two sides) had sufficient information for localizing the source ( Chaudhari et al 2005 ). 3D CT scan data were used to segment the anatomical volume into skull and soft tissue and to assign standard optical properties ( Cheong et al 1990 ). The specific values of the absorption and reduced scattering coefficients for the skull and soft tissue we used can be found in Chaudhari et al (2005) . A regularization parameter value of ? = 0.05 was determined empirically. All image reconstructions were performed on an AMD Opteron 870 2.0 GHz computer. For all the iterative algorithms, a zero uniform image, 0 = [ 0 , … , 0 ] ? ? R p , was used as an initial estimate x 0. There is a trade-off between the number of FEM mesh nodes and the accuracy of FEM model; 105–106 nodes were recommended for sufficient accuracy in 3D problems ( Schweiger and Arridge 1997 ). Here we used a FEM-based forward model with v = 71256 tessellation nodes, a source space with p = 10814 source locations, and N = 3085 surface measurement nodes for K = 4 spectral bins. We did not attempt to investigate different FEM models in terms of accuracy and computation cost since our purpose is to compare the computation speeds of different reconstruction methods for a given model; however, we achieved reasonably good localization results within a reasonable time, as shown below. See Chaudhari et al (2005) for a study of the effects of the number of surface measurement points on reconstruction results. First, we precomputed the full system matrix A by using the mldivide function in Matlab (v7.3; Mathworks, MA, USA). We computed 500 columns of the system matrix at a time; that is, we prepared a 71256 × 500 matrix by combining 500 load vectors and solved the forward problem in ( 3 ) by putting the matrix on the right-hand side. It took 12 708 s to compute the full system matrix, as shown in table 1 . Then we reconstructed the bioluminescence image with the system matrix using the direct calculation approach. The iterative algorithms were implemented in the C language. Table 1 shows, in the columns named iteration cost, the computation time in seconds and the number of iterations, required for each algorithm to achieve the relative error E = ? x n ? x ^ ? ? ? x ^ ? < 10 % , < 5 % , <5%, and <1%, respectively, where the RLS image x ^ in ( 11 ) was estimated by 2000 iterations of GPM-N. Note that the relative error E is not with respect to the true source distribution which is unknown but with respect to the RLS solution x ^ given in ( 11 ). Therefore, E represents how close an image is to the converged solution and can be used for evaluating the convergence speed of an iterative algorithm. In the table, GPM-U denotes the unpreconditioned GPM; GPM-N and GPM-EN denote GPM with preconditioner P N and P EN given in ( 29 ) and ( 30 ), respectively; CG, PCG-N, PCG-EN, and PCG-EM denote PCG with no preconditioner and with preconditioners P N, P EN, and P EM, respectively; and OS-SPS-2, OS-SPS-5, and OS-SPS-10 denote OS-SPS with two, five and ten subsets, respectively. In table 1 , the total reconstruction time is the sum of the precomputation time and the iteration cost. We did not include the computation cost for constructing the FEM matrix F , which is common in all the methods. Next, we reconstructed images using the on-the-fly approach. For the on-the-fly approach, the precomputation time (110 s) in the table represents the time required for Cholesky factorization. As discussed in section 3, we excluded CD and OS-SPS, and preconditioner P N for this approach. For the on-the-fly approach we implemented the iterative algorithms in Matlab. For the on-the-fly F/B projectors, the one-time computation for Cholesky factorization was performed by the chol function, and repeated substitutions were carried out by the mldivide function in Matlab. As shown in table 1 , for both GPM and PCG, the use of preconditioners P N, P EN and P EM significantly accelerates convergence. In other words, the preconditioned algorithms (GPM-N, GPM-EN and PCG-N, PCG-EN, PCG-EM) require fewer iterations and lower iteration costs in seconds than the unpreconditioned ones (GPM-U and CG). Preconditioners P EN using the estimated diagonal elements of the Hessian and P N using the exact diagonals showed similar convergence speeds. In the direct calculation approach, CD and OS-SPS-10 were much faster than GPM and PCG. In fact, OS-SPS-10 showed the fastest convergence speed; it required only 8 and 15 iterations (11 and 20 s) for E < 10% and E < 5%, respectively. However, OS-SPS-10 (as well as OS-SPS-5) could not reach a point where ? < 1% because of the limit cycle behavior. This is a typical characteristic of incremental gradient or OS algorithms; as the number of subsets increases the initial convergence speed becomes faster but the limit point moves further from the optimal solution. Therefore, when it comes to the number of subsets for OS algorithms, there is a trade-off between convergence speed and accuracy. For ? < 1%, CD was fastest among the iterative algorithms for the direct calculation approach. On the other hand, for the on-the-fly approach, GPM-EN showed the fastest convergence rate for all three relative error cases, and PCG-EN showed a similar yet little slower convergence rate. The precomputation time (12 708 s) for the direct calculation approach is much larger than that (110 s) for the on-the-fly approach whereas the cost per iteration for the direct approach is smaller than that for the on-the-fly approach. If few images are to be reconstructed for a given system, then the on-the-fly approach can substantially reduce the total computation cost. For example, for the relative error <10%, GPM-EN, the fastest among the on-the-fly algorithms in total reconstruction time, is faster than OS-SPS-10, the fastest of the direct calculation algorithms, by a factor of 12719/798 ? 15.9; similarly, for < 5%, by a factor of 12728/1091 ? 11.7. For ? < 1%, GPM-EN in the on-the-fly approach is faster than CD in the direct calculation approach by a factor of 12773/1724 ? 7.4. As the number of required iterations increases, the factor of computational saving of the on-the-fly approach compared to the direct approach decreases. Figure 2 shows the RLS image x ^ and figure 3 shows an image reconstructed by 29 iterations, taking 798 s in total reconstruction time, of GPM-EN, which is the fastest among the on-the-fly algorithms. Although the relative error between the images is about 10%, they look similar and the localization is reasonably accurate.  5. Conclusion We have explored computationally efficient methods for fully 3D multispectral bioluminescence image reconstruction. When it comes to incorporating F/B projectors into iterative algorithms, we investigated the straightforward direct calculation approach and an on-the-fly approach where one does not have to precompute the system matrix. We evaluated those approaches combined with various iterative algorithms by using real mouse bioluminescence data. We achieved a substantial speed-up using the proposed on-the-fly approach by a factor of up to 15.9 when a single image reconstruction is required. However, if multiple image reconstructions are to be performed for a given model, the speed-up factor decreases and in this case the direct calculation approach may be more efficient. Using proper preconditioners accelerates convergence speeds significantly; the estimated diagonal approximation to Newton's preconditioner (EN), which applies to both direct calculation and on-the-fly approaches, reduced the iteration cost by a factor of >7 for both GPM and PCG. In the on-the-fly approach, GPM-EN yielded the fastest convergence speed and PCG-EN showed a little slower convergence rates. For the direct calculation approach, OS-SPS, an incremental gradient algorithm, was the fastest one although it converged to an approximated RLS solution; CD was the second fastest, which converges to the RLS solution. We note that the emphasis in this paper is on fast methods for computing inverse solutions. With the exceptions described above, all methods implemented should converge to the same solution. For this reason we have not explored measures of image quality such as bias, resolution and variance. The methods presented here represent the simplest (quadratic) error models and regularization functions. However, most of the ideas discussed can be extended to incorporate more sophisticated noise models and nonquadratic regularization functions. We also restricted our attention here to bioluminescence imaging. However, the methods can be extended to the related problem of reconstructing images in 3D fluorescence optical tomography ( Ahn et al 2007b ). In this case, the models must be extended to also include the propagation of the excitation light through the animal. Since this process can also be approximated using the diffusion equation, similar fast FEM and on-the-fly methods can be developed. 