Utilizing the Molecular Gateway: The Path to Personalized Cancer Management BACKGROUND Personalized medicine is the provision of focused prevention, detection, prognostic, and therapeutic efforts according to an individual’s genetic composition. The actualization of personalized medicine will require combining a patient’s conventional clinical data with bioinformatics-based molecular-assessment profiles. This synergistic approach offers tangible benefits, such as heightened specificity in the molecular classification of cancer subtypes, improved prognostic accuracy, targeted development of new therapies, novel applications for old therapies, and tailored selection and delivery of chemotherapeutics. CONTENT Our ability to personalize cancer management is rapidly expanding through biotechnological advances in the postgenomic era. The platforms of genomics, proteomics, single-nucleotide polymorphism profiling and haplotype mapping, high-throughput genomic sequencing, and pharmacogenomics constitute the mechanisms for the molecular assessment of a patient’s tumor. The complementary data derived during these assessments is processed through bioinformatics analysis to offer unique insights for linking expression profiles to disease detection, tumor response to chemotherapy, and patient survival. Together, these approaches permit improved physician capacity to assess risk, target therapies, and tailor a chemotherapeutic treatment course. SUMMARY Personalized medicine is poised for rapid growth as the insights provided by new bioinformatics models are integrated with current procedures for assessing and treating cancer patients. Integration of these biological platforms will require refinement of tissue-processing and analysis techniques, particularly in clinical pathology, to overcome obstacles in customizing our ability to treat cancer.  Bioinformatics: Techniques and Challenges Bioinformatics translates raw molecular data extracted from patient samples into interpretable, accessible, and distributable information. This step in the personalization process is necessary for interpreting most high-throughput microarray-based biomedical applications and typically consists of the following steps: ( a ) data preprocessing, ( b ) normalization, ( c ) biomarker discovery, ( d ) statistical modeling and validation of prediction, and ( e ) follow-up clinical confirmation. Some details of implementation of these procedures may vary slightly between different molecular platforms, such as cDNA arrays, oligonucleotide arrays, or mass spectrometry ( 46 ). Nevertheless, these approaches have been used across most platforms and therefore provide a systematic method for transcending the challenges encountered during the analysis of most high-throughput biotechnology data. DATA PREPROCESSING High-throughput molecular data possess rich digital information for each molecular target. For example, a single scan of a microarray chip produces hundreds of image pixels for each of >20 000 transcript probes. The initial preprocessing analysis of such high-density data is often tightly combined and encrypted within the manufacturing steps of biotechnology instrumentation. This step, however, is one of the most critical in the analysis for optimizing the molecular information obtainable from such massive amounts of biological data. In fact, preprocessing algorithms generated by third parties sometimes have demonstrated substantially higher reproducibility and sensitivity with certain commercial microarray platforms than those produced by the platform manufacturer ( 47 ). Additionally, identifying and quantifying specific protein species from a background of fragmented-peptide mass-spectrometry spectra are recognized as a challenging statistical and computational problem. The direct involvement of computational researchers in the development of these initial data-preprocessing steps will prove greatly beneficial. NORMALIZATION Normalization is required to standardize multiple data sets produced in independent experiments before they can be combined for analysis. This step is essential for obtaining data with universal applicability despite their having differing institutional origins. Before normalization, data are often log-transformed to make their distributions more appropriate for subsequent analyses. Data are then normalized via correction often with a simple constant factor. More sophisticated normalization methods such as nonparametric regression can be used if different data sets have nonlinear relationships ( 47 ). BIOMARKER DISCOVERY AND PREDICTION MODELING The identification and selection of biomarkers that are clinically relevant are a fundamental step that determines the success of downstream applications that advance personalized medicine. This kind of discovery can be performed in various ways: ( a ) simple comparison of contrasting groups (e.g., disease-free survivors vs relapsed patients, responders vs nonresponders to a therapeutic compound), ( b ) analysis of a statistical association between patients’ outcomes and phenotypes, and ( c ) analysis of correlations with continuous drug-sensitivity data, such as for GI50 (concentration inhibiting growth by 50%) in a cell line panel. An important key to biomarker discovery is the subsequent validation ( 46 ). With a set of preidentified biomarkers, many different statistical approaches have been used in prediction modeling. Such approaches include gene voting, discriminant analysis, Bayesian regression or classification, random forest, Cox regression, and support vector machines, technical details of which are beyond the scope of this review and can be found elsewhere ( 7 , 41 , 42 ). These different prediction-modeling techniques have often been found to provide similar predictive powers if each is finely tuned. An integral aspect of modeling is to maintain extremely tight control of error and bias due to overtraining and multiple comparisons. VALIDATION AND FOLLOW-UP CLINICAL CONFIRMATION To avoid “selection bias” one must validate a trained prediction model with patient data sets that are completely independent of the original discovery and training data set ( 48 ). The follow-up preferably uses data from different places and clinical settings, because a patient cohort from a particular location and clinical setting may produce specific molecular results that may not occur in other patient populations. This step is particularly important to ensure the performance and accuracy of the molecular assay in clinical practice.  DATA PREPROCESSING High-throughput molecular data possess rich digital information for each molecular target. For example, a single scan of a microarray chip produces hundreds of image pixels for each of >20 000 transcript probes. The initial preprocessing analysis of such high-density data is often tightly combined and encrypted within the manufacturing steps of biotechnology instrumentation. This step, however, is one of the most critical in the analysis for optimizing the molecular information obtainable from such massive amounts of biological data. In fact, preprocessing algorithms generated by third parties sometimes have demonstrated substantially higher reproducibility and sensitivity with certain commercial microarray platforms than those produced by the platform manufacturer ( 47 ). Additionally, identifying and quantifying specific protein species from a background of fragmented-peptide mass-spectrometry spectra are recognized as a challenging statistical and computational problem. The direct involvement of computational researchers in the development of these initial data-preprocessing steps will prove greatly beneficial.  NORMALIZATION Normalization is required to standardize multiple data sets produced in independent experiments before they can be combined for analysis. This step is essential for obtaining data with universal applicability despite their having differing institutional origins. Before normalization, data are often log-transformed to make their distributions more appropriate for subsequent analyses. Data are then normalized via correction often with a simple constant factor. More sophisticated normalization methods such as nonparametric regression can be used if different data sets have nonlinear relationships ( 47 ).  BIOMARKER DISCOVERY AND PREDICTION MODELING The identification and selection of biomarkers that are clinically relevant are a fundamental step that determines the success of downstream applications that advance personalized medicine. This kind of discovery can be performed in various ways: ( a ) simple comparison of contrasting groups (e.g., disease-free survivors vs relapsed patients, responders vs nonresponders to a therapeutic compound), ( b ) analysis of a statistical association between patients’ outcomes and phenotypes, and ( c ) analysis of correlations with continuous drug-sensitivity data, such as for GI50 (concentration inhibiting growth by 50%) in a cell line panel. An important key to biomarker discovery is the subsequent validation ( 46 ). With a set of preidentified biomarkers, many different statistical approaches have been used in prediction modeling. Such approaches include gene voting, discriminant analysis, Bayesian regression or classification, random forest, Cox regression, and support vector machines, technical details of which are beyond the scope of this review and can be found elsewhere ( 7 , 41 , 42 ). These different prediction-modeling techniques have often been found to provide similar predictive powers if each is finely tuned. An integral aspect of modeling is to maintain extremely tight control of error and bias due to overtraining and multiple comparisons.  VALIDATION AND FOLLOW-UP CLINICAL CONFIRMATION To avoid “selection bias” one must validate a trained prediction model with patient data sets that are completely independent of the original discovery and training data set ( 48 ). The follow-up preferably uses data from different places and clinical settings, because a patient cohort from a particular location and clinical setting may produce specific molecular results that may not occur in other patient populations. This step is particularly important to ensure the performance and accuracy of the molecular assay in clinical practice.  Considerations for Clinical Pathology BIOSAMPLE QUALITY The privileged access of the clinical pathology laboratory to patient biopsies and samples places it in a unique position to ensure sample integrity through the development and implementation of standardized guidelines for procuring and storing biosamples. The coordination of such efforts is currently facilitated by the US National Cancer Institute’s Office of Biorepositories and Biospecimen Research ( http://biospecimens.cancer.gov/index.asp ) ( 49 ). The establishment of this department in 2005 is a testament to the priority placed on the necessity to preserve reliable biosamples for expediting the development of molecular-based diagnostics and therapeutics. The Biospecimen Research Network, the research branch of the Office of Biorepositories and Biospecimen Research, seeks to conduct and collaborate on projects to help establish best practices for sample storage and tracking, to identify high-quality samples in existing repositories, and to determine the impact of specific variables during sample handling ( 49 ). Whereas the latter 2 aims will provide a short-term solution for the immediate use and interpretation of results produced from currently preserved samples, the implementation of a best-practices policy for bio-sample handling will have a long-standing impact on protocols for the procurement and storage of pathology samples. Variables such as time from sample excision to fixation, optimal fixation solutions and conditions (e.g., ethanol vs formalin fixation), and sample storage, cataloging, and retrieval each require specific attention ( 49 ). EXTRACTION OF MOLECULAR INFORMATION Sustaining the recent momentum in the field of molecular cancer research requires a coalescence of well-annotated clinical information and molecular data derived from patient biosamples. An elegant and abundant cache of patient molecular material is the formalin-fixed, paraffin-embedded (FFPE) samples that have been produced in myriad clinical trials, in which the patient information has been well characterized and the outcomes are known. Retrieval of the full molecular information in archived FFPE samples, however, is hindered by changes in molecular structures that occur during the fixation process. The challenge of extracting useful molecular information despite formalin-induced protein cross-linkages and the addition of monomethylol groups to nucleosides has been partially overcome with the emergence of commercially available kits, such as the RNeasy FFPE Kit (Qiagen), the Paraffin Block Isolation and the RecoverAll Total Nucleic Acid Isolation Kits (Ambion), and the Paradise Plus Reagent System (Arcturus). These kits predominantly rely on proteinase K digestion to facilitate mRNA release from the bonds to cross-linked proteins ( 50 ). Although successful extraction of intact mRNA is tightly tied to the quality of sample fixation, isolation of contiguous miRNA is less dependent on fixation methods, as evinced by the lower cross-sample variation found in a recent study ( 51 ). The superior potential for extracting miRNA from such samples is primarily attributable to the short lengths of miRNAs ( 51 ). Such distinctions underscore the necessity for skilled clinical pathologists to determine which biological platform to implement for accurate tumor assessment given the quality of the available samples. QUANTITATIVE REAL-TIME PCR The power of quantitative real-time PCR (qPCR) to obtain molecular-expression profiles from patient tumor biopsies must be harnessed. Although microarray technology remains the preeminent mode for biomarker discovery, qPCR analysis serves as a useful adjunct for validating microarray results, in addition to functioning as a targeted method for analyzing specific, previously verified biomarker concentrations. As an assessment platform, qPCR offers rapid, single-step amplification and quantification of molecular targets that are useful for clarifying diagnosis, predicting recurrence, and guiding treatment ( 52 ). For example, a qPCR-based ratio of HOXB13 (homeobox B13) expression to IL17RB (interleukin 17 receptor B) expression has been used to predict tumor recurrence in the setting of adjuvant tamoxifen monotherapy ( 53 ). Although conventional qPCR methods are effective for assessing expression signatures for small batches of molecular targets, the advancement of multiplex qPCR permits concentrations of multiple targets to be analyzed in a single reaction ( 52 ). The consolidation of resources and time offered by a multiplexed qPCR model may provide the speed and cost efficiency necessary for individualized molecular-assessment practices to gain widespread clinical integration ( 25 ). FOCUS ON EDUCATION Efforts to emphasize the therapeutic potential of integrating molecular profiling and clinical assessments are necessary to accelerate the actualization of personalized care. The clinical pathology laboratory is well equipped to provide physicians with the knowledge to promote the regular incorporation of molecular tumor analysis into standard workups; however, the overall success of such efforts will reside in striking a cautionary balance of promoting personalized medicine without overselling a concept that lacks proper verification and implementation standards. This sentiment resonates with groups such as the Coriell Personalized Medicine Collaborative ( 54 ). A Coriell study hopes to clarify the impact of genomic assessment on clinical outcomes while shaping the social and legal ramifications associated with openly accessible genomic profiling ( 54 ).  BIOSAMPLE QUALITY The privileged access of the clinical pathology laboratory to patient biopsies and samples places it in a unique position to ensure sample integrity through the development and implementation of standardized guidelines for procuring and storing biosamples. The coordination of such efforts is currently facilitated by the US National Cancer Institute’s Office of Biorepositories and Biospecimen Research ( http://biospecimens.cancer.gov/index.asp ) ( 49 ). The establishment of this department in 2005 is a testament to the priority placed on the necessity to preserve reliable biosamples for expediting the development of molecular-based diagnostics and therapeutics. The Biospecimen Research Network, the research branch of the Office of Biorepositories and Biospecimen Research, seeks to conduct and collaborate on projects to help establish best practices for sample storage and tracking, to identify high-quality samples in existing repositories, and to determine the impact of specific variables during sample handling ( 49 ). Whereas the latter 2 aims will provide a short-term solution for the immediate use and interpretation of results produced from currently preserved samples, the implementation of a best-practices policy for bio-sample handling will have a long-standing impact on protocols for the procurement and storage of pathology samples. Variables such as time from sample excision to fixation, optimal fixation solutions and conditions (e.g., ethanol vs formalin fixation), and sample storage, cataloging, and retrieval each require specific attention ( 49 ).  EXTRACTION OF MOLECULAR INFORMATION Sustaining the recent momentum in the field of molecular cancer research requires a coalescence of well-annotated clinical information and molecular data derived from patient biosamples. An elegant and abundant cache of patient molecular material is the formalin-fixed, paraffin-embedded (FFPE) samples that have been produced in myriad clinical trials, in which the patient information has been well characterized and the outcomes are known. Retrieval of the full molecular information in archived FFPE samples, however, is hindered by changes in molecular structures that occur during the fixation process. The challenge of extracting useful molecular information despite formalin-induced protein cross-linkages and the addition of monomethylol groups to nucleosides has been partially overcome with the emergence of commercially available kits, such as the RNeasy FFPE Kit (Qiagen), the Paraffin Block Isolation and the RecoverAll Total Nucleic Acid Isolation Kits (Ambion), and the Paradise Plus Reagent System (Arcturus). These kits predominantly rely on proteinase K digestion to facilitate mRNA release from the bonds to cross-linked proteins ( 50 ). Although successful extraction of intact mRNA is tightly tied to the quality of sample fixation, isolation of contiguous miRNA is less dependent on fixation methods, as evinced by the lower cross-sample variation found in a recent study ( 51 ). The superior potential for extracting miRNA from such samples is primarily attributable to the short lengths of miRNAs ( 51 ). Such distinctions underscore the necessity for skilled clinical pathologists to determine which biological platform to implement for accurate tumor assessment given the quality of the available samples.  QUANTITATIVE REAL-TIME PCR The power of quantitative real-time PCR (qPCR) to obtain molecular-expression profiles from patient tumor biopsies must be harnessed. Although microarray technology remains the preeminent mode for biomarker discovery, qPCR analysis serves as a useful adjunct for validating microarray results, in addition to functioning as a targeted method for analyzing specific, previously verified biomarker concentrations. As an assessment platform, qPCR offers rapid, single-step amplification and quantification of molecular targets that are useful for clarifying diagnosis, predicting recurrence, and guiding treatment ( 52 ). For example, a qPCR-based ratio of HOXB13 (homeobox B13) expression to IL17RB (interleukin 17 receptor B) expression has been used to predict tumor recurrence in the setting of adjuvant tamoxifen monotherapy ( 53 ). Although conventional qPCR methods are effective for assessing expression signatures for small batches of molecular targets, the advancement of multiplex qPCR permits concentrations of multiple targets to be analyzed in a single reaction ( 52 ). The consolidation of resources and time offered by a multiplexed qPCR model may provide the speed and cost efficiency necessary for individualized molecular-assessment practices to gain widespread clinical integration ( 25 ).  FOCUS ON EDUCATION Efforts to emphasize the therapeutic potential of integrating molecular profiling and clinical assessments are necessary to accelerate the actualization of personalized care. The clinical pathology laboratory is well equipped to provide physicians with the knowledge to promote the regular incorporation of molecular tumor analysis into standard workups; however, the overall success of such efforts will reside in striking a cautionary balance of promoting personalized medicine without overselling a concept that lacks proper verification and implementation standards. This sentiment resonates with groups such as the Coriell Personalized Medicine Collaborative ( 54 ). A Coriell study hopes to clarify the impact of genomic assessment on clinical outcomes while shaping the social and legal ramifications associated with openly accessible genomic profiling ( 54 ).  Conclusions Advancements in molecular-profiling techniques have provided unprecedented insight into the genetic etiologies and basic molecular dysfunctions that lead to tumorigenesis. Moreover, bioinformatics analysis of gene expression data has produced expression signatures that are useful for personalizing many aspects of cancer management, including genetic screening, early detection, tumor classification, and prediction of prognosis and therapeutic response. Successful incorporation of bioinformatics-based assessments alongside current methods of tissue and clinical evaluation will leverage the complementary natures of these biological platforms. Orchestrating this collaboration is one of the steepest challenges facing clinical pathologists in their quest to integrate bioinformatics with frontline clinical care. The likely payoff for such efforts appears enormous. Through mediation of conventional and molecular-assessment methods, the personalization of cancer management stands poised for success. 